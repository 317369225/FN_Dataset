★How much would it cost to crawl 1 billion sites using rented AWS servers/bandwidth?
I assume you mean 1 billion pages as opposed to sites since the latter would imply an index as big as major search engines. Check the very end of the article if you did mean sites. The average page size has been growing quickly: http://www.websiteoptimization.c... It used to be closer to 100KB, now it is more than 500KB. Still, the average will vary depending on what kind of pages you are interested in, e.g. largely text of lots of multimedia. Let us assume 250KB as a mid-estimate, you can adjust the end result if you prefer. 250KB * 1B = 250,000 GB = 250 TB, so you will need to pay for 250 TB of data transfer. AWS currently charges data transfer $0.08/GB of outbound data for large transfers (> 150 TB/month) (http://aws.amazon.com/ec2/#pricing) So your total for data transfer would be 250,000 * $0.08 = $20K Note that AWS currently does not charge for inbound data transfer, but that is going to change on Nov 1 2010. They will start charging $0.10/GB, adding $25K to your bill. In addition to data transfer, you will need to pay for machine instances doing the crawl, but those charges will be much smaller. Assuming you use 10 small instances, it would take  a month or so (33M pages/day) and you would pay 30 * 10 * $2.4 = $720 for those machines i.e much less than for data transfer. Even if you used large instances, your bill would be 4x larger for a total of $2880, still dominated by transfer. Of course, you can reduce the bill by doing the processing on AWS but then you will need to pay for extra CPU time. These numbers are only rough ballpark estimates but I would say you should assume that it would take tens of thousands dollars. There are other alternatives e.g. 80legs.com  They charge only $2 per million pages, resulting in a bill of only $2K. But note that there is a charge  of $0.084/GB for pages > 25KB (http://wiki.80legs.com/Pricing) which would be largely the case in your crawl. So the final bill would still be comparable, perhaps a bit smaller. The best way IMHO to do such a crawl would be to recruit a group of say 100-1000 of your friends, and their friends, and write a simple distributed app running in background on their machines, when they sit idle or are lightly used. This way you will be amortizing their monthly broadband bills, with their monthly quotas (e.g. Comcast 250GB) largely unused anyway. I would think that you can get dozens of Mbps of cross bandwidth in such a network, which could do the job in a matter of months. BTW, if you really meant 1 billion sites, as opposed to pages, multiply the above bills by 100x (average number of pages per site).