â˜…What is the scale of a real-life DDoS attack?
I have seen DDoS attacks over 700 gigabits/sec -- among the largest ever seen. It was conducted for research purposes. The largest criminal DDoS attacks seem to be several hundred gigabits in size. Size is not the only thing that matters in DDoS -- a great many servers and technologies are vulnerable to very small DDoS's as well. It is possible to attack a multi-million dollar defense network with less than 10 servers and disable the applications -- you just have to know what specific resource you wish to target and consume. There are many more resources to target than just bandwidth and PPS rates. Many devices have connection table limits, SSL/TLS limits, working thread limits, etc... This is why slow-attacks can be very effective DDoS's too. Relying on spoofed IP's is really only useful for SYN floods, bandwidth flooding and DNS attacks. You can't use them for application attacks on HTTP and HTTPS for example as they use TCP and the HTTP request can only be made once the TCP connection is established. You can't just block based on IP address these days without a lot of false positives. There are many network proxies, NAT addresses (even countries which roll up to single IP addresses). Most people who block using big blacklists tend not to care about false positives during a DDoS attack. But remember: if someone is randomizing (spoofing) source address, you can't really rely on IP blacklists. Also your blacklisting system has to have a lot of capacity in terms of PPS. Ultimately, for volumetric attacks, if the inbound packet rate and/or bit rate exceeds the input/output capacity of system the DOS is successful. So only highly distributed, highly scalable, cloud-based technologies can stand up to really huge DDoS attacks that focus on overflowing bandwidth. Ultimately, for application attacks, the key is detecting abuse patterns over time. The shorter the time it takes and the more detailed the abuse pattern (multi-variate (like rate + port + entropy + protocol aware decoding) vs. single variate (like rate) tend to be more effective.