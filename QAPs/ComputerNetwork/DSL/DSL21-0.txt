â˜…What is the consideration in selecting a language to build DSL?How different is the choice if I just need to run the DSL in JVM only compared to needing to also run in C++ runtime?
For building an embedded DSL, one of the important criteria is that the host language must have the power of composition. That is, abstractions that you design as part of your model must be composable to form larger abstractions. With languages like Haskell that supports pure functional programming, you can compose using the power of higher order functions. Additionally since Haskell has a powerful typesystem and great type inferencing facilities, you can make your DSL very succinct by encoding some part of your domain logic within the types that you create. On the JVM Scala is a good option to create typed embedded DSLs. Scala supports OO as well as functional programming - so you get the modularity of object oriented programming as well as the power of higher order functions. However type inferencing is not as powerful as Haskell - so the DSL may turn out to be a little more verbose in some cases. Scala 2.10 also has support for compile time macros which will help you create domain specific syntactic structures at the compiler level without any run time overhead. Dynamic languages like Clojure, Ruby and Groovy also provide great support for writing DSLs. Without type annotations, the DSL turns out to be quite compact, but you lose the ability to do static type checking and may end up with a few of the errors during runtime which could have been detected during compile time. But all of these languages support higher order functions and meta-programming and hence offers composability of functional abstractions. While Ruby and Groovy has support primarily for runtime meta-programming, Clojure offers prime support for compile time meta-programming because of the power of macros. External DSLs are stand alone DSLs where you need to build the custom DSL ground up. Earlier we used to do this using parser generators like lex and yacc that let you write your own lexical analyzers and parsers. However today we have tools like ANTLR (http://antlr.org) that are easier to use and can recognize more powerful languages than lex and yacc. lex and yacc help you design an LALR parser, which despite being easier to construct than LL(*) ones (which ANTLR generates) are much difficult to debug and fix because of some conflicts (shift/reduce, reduce/reduce) that the parser generates. With ANTLR you can write grammar specifications and generate parser code for a host of target languages. So if you would like to use C/C++ then ANTLR may be a good choice. For external DSLs, language workbenches are also another option these days. Take a look at XText (http://www.eclipse.org/Xtext/) which offers a complete language development and management environment with great integration to IDEs. Jetbrains also offers a meta programming system (http://www.jetbrains.com/mps/) that offers a complete DSL development workbench. Finally another great tool for developing DSLs is a parser combinator abstraction. Parser combinators are higher order functions with each combinator being an individual parser. You can compose them incrementally and build up your own language. The best part is that you don't have any external dependencies - select a language that has a parser combinator library and just use it to design your DSL. Haskell has parsec, a monadic parser combinator library (http://www.haskell.org/haskellwi...). Scala also has a parser combinator library which is used pretty often for designing external DSLs. I wrote a blog post on this quite some time back (http://debasishg.blogspot.in/200...).