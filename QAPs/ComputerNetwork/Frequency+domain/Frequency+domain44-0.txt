★Why isn't speech recognition software more accurate?
This is an excellent question to start off an automatic speech recognition (ASR) interview. I would slightly rephrase the question as "Why is speech recognition hard?". The reasons are plenty and here is my take on the topic: An ASR is just like any other machine learning (ML) problem, where the objective is to classify a sound wave into one of the basic units of speech (also called a "class" in ML terminology), such as a word. The problem with human speech is the huge amount of variations which occur while pronouncing a word. For example, below are two recordings of the word "Yes" spoken by the same person (wave source : AN4 dataset [1]). It can easily be seen that the signals differ and the same can be verified by analysing it in frequency or time-frequency domain. ​ ​ Comparison of two different recording of the word "Yes" in the time domain. There are several reasons for this variation, namely stress on the vocal chords, environmental conditions and microphone conditions to mention a few. To capture this variation,  ML algorithms such as  the hidden Markov model (HMM)[2] along with Gaussian mixture models are used. More recently, deep neural networks (DNN) have been shown perform better. One way to do ASR is to train ML models for each word. During the training phase, the speech signal is broken down into a set of features (such as Mel frequency cepstral coefficients, or MFCC for short) which are then used to build the model. These models are called acoustic models (AM). When a speech signal has to be "recognized" (testing phase),  features  are again extracted, and are compared against each word model. The signal is assigned to represent the word, which has the highest probability value. This way of doing ASR works pretty well for small vocabularies. When the number of words increases, we end up comparing with a very large set of models which is computationally not feasible. There is another problem of finding enough data to train these models. The word model fails for large vocabulary continuous speech recognition task, due to high complexity involved in decoding as well the need for high amount of training data. To overcome this problem, we divide words into smaller units called phones. In  English language (and many Indian Languages), there are approximately 50 phones which can be combined to make up any word. For example the word "Hello" can be broken in to "HH, AH, L, OW".  You can look up the CMU pronouncing dictionary [6] for phonetic expansion of  English words. Now the problem of ASR boils down to recognizing the phone sequence, instead of a word. This requires  building ML models for every phone. These models are called Monophone models. If you can do a good job of recognizing the phones, you have  solved a big part of the ASR problem. Unfortunately, recognizing phones is not an easy task. If we plot the Fourier spectrum of a phone utterance, distinct peaks are visible as can be seen in the plot below: ​                    Formant Frequencies. Image source [5] The peak frequencies F1 and F2 are key indicators of a phone. Scatter plot of the vowels with respect to F1 and F2 is shown below. As can be seen, the spread  is large and very often overlaps with one another. ​ Variation of dominant frequencies between vowels.  No clear boundaries can be drawn to differentiate the vowels. Image credit [3] This overlap makes it hard for a ML algorithm to distinguish between phones. An other problem with monophones is that, they are often influenced by the neighboring phones. The figure below, shows the time domain as well as the time-frequency domain (STFT) representation of a speech utterance "Heel". Time domain and STFT representation of the word "Heel". Image source [Page on upenn.edu ] The word heel can phonetically be expanded as "HH IY L". The influence of the phone "HH" on "IY" can clearly ... (more)Loading...