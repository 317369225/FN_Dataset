★What is the difference between throughput and bit rate?
Hmm. trying to think of a way to explain this correctly without nerding out completely. Throughput is the transfer rate between two applications.  Bit rate is speed at which two devices can transmit a change in state.The first is pretty clear.    For example, Google Chrome running on your computer is copying a file from a webserver.  The throughput is how much actual data is getting transmitted from the webserver to Google Chrome. However, in that data path, there are many things happening. Google Chrome is talking to the TCP stack inside your computer, which is talking to the IP stack, which is talking to the ethernet card, which is talking to the ethernet card in your local router, which is talking to another router, and another and another, until it gets to the server which then reverses the process inside itself, ethernet -> IP -> TCP -> webserver. It's even more complicated than that, but let's keep it there for now.   The Chrome->TCP->IP->Ethernet process takes the data, splits it into pieces and then at each layer, a header is attached to that data, so by the time your data hits the ethernet, it looks like this: ethernet header - IP header - TCP header - data Bit rate is the rate at which the two like devices can talk to each other on the network.    Ie, two ethernet cards, an ethernet card and a switch, two modems, two cable modems, etc. So there will be many bit rate for a connection through the Internet, and unfortunately, the slowest one dictates the throughput. That's why even though you have 1Gbps ethernet cards in your computer, you can only download that picture of furry cats at 30kbps. I hope that is clear.