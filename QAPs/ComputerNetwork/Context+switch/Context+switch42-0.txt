★Linux Kernel: How much processor time does a process switching cost to the process scheduler?Assume a linux scheduler.
Profiling the switching time is very difficult, but the in-kernel  latency profiling tools, as well as oprofile (which can profile the  kernel itself) will help you there. Here is an experiment: The lineupApril 21, 2011: update: I added an "extreme" Nehalem and a low-voltage Westmere. I've put 4 different generations of CPUs to test: A dual Intel 5150 (Woodcrest, based on the old "Core" architecture, 2.67GHz).  The 5150 is a dual-core, and so in total the machine has 4 cores available.  Kernel: 2.6.28-19-server x86_64. A dual Intel E5440  (Harpertown, based on the Penrynn architecture, 2.83GHz).  The E5440 is a quad-core so the machine has a total of 8 cores.  Kernel: 2.6.24-26-server x86_64. A dual Intel E5520 (Gainestown, based on the Nehalem architecture,  aka i7, 2.27GHz).  The E5520 is a quad-core, and has HyperThreading  enabled, so the machine has a total of 8 cores or 16 "hardware threads".   Kernel: 2.6.28-18-generic x86_64. A dual Intel X5550 (Gainestown, based on the Nehalem architecture,  aka i7, 2.67GHz).  The X5550 is a quad-core, and has HyperThreading  enabled, so the machine has a total of 8 cores or 16 "hardware threads".   Note: the X5550 is in the "server" product line.  This CPU is 3x more  expensive than the previous one.  Kernel: 2.6.28-15-server x86_64. A dual Intel L5630 (Gulftown, based on the Westmere architecture,  aka i7, 2.13GHz).  The L5630 is a quad-core, and has HyperThreading  enabled, so the machine has a total of 8 cores or 16 "hardware threads".   Note: the L5630 is a "low-voltage" CPU.  At equal price, this CPU is  in theory 16% less powerful than a non-low-voltage CPU.  Kernel:  2.6.32-29-server x86_64.As far as I can say, all CPUs are set  to a constant clock rate (no Turbo Boost or anything fancy).  All the  Linux kernels are those built and distributed by Ubuntu.First idea: with syscalls (fail)My  first idea was to make a cheap system call many times in a row, time  how long it took, and compute the average time spent per syscall.  The  cheapest system call on Linux these days seems to be gettid.   Turns out, this was a naive approach since system calls don't actually  cause a full context switch anymore nowadays, the kernel can get away  with a "mode switch" (go from user mode to kernel mode, then back to  user mode).  That's why when I ran my first test program, vmstat  wouldn't show a noticeable increase in number of context switches.  But  this test is interesting too, although it's not what I wanted  originally. Source code: timesyscall.c Results: Intel 5150: 105ns/syscall Intel E5440: 87ns/syscall Intel E5520: 58ns/syscall Intel X5550: 52ns/syscall Intel L5630: 58ns/syscallNow  that's nice, more expensive CPUs perform noticeably better.  But that's  not really what we wanted to know.  So to test the cost of a context  switch, we need to force the kernel to de-schedule the current process  and schedule another one instead.  And to benchmark the CPU, we need to  get the kernel to do nothing but this in a tight loop.  How would you do  this?Second idea: with futexThe way I did it was to abuse futex (RTFM).  futex  is the low level Linux-specific primitive used by most threading  libraries to implement blocking operations such as waiting on a  contended mutexes, semaphores that run out of permits, condition  variables and friends.  If you would like to know more, go read Futexes Are Tricky by Ulrich Drepper.  Anyways, with a futex, it's easy to suspend and  resume processes.  What my test does is that it forks off a child  process, and the parent and the child take turn waiting on the futex.   When the parent waits, the child wakes it up and goes on to wait on the  futex, until the parent wakes it and goes on to wait again.  Some kind  of a ping-pong "I wake you up, you wake me up...". Source code: timectxsw.c Results: Intel 5150: ~4300ns/context switch Intel E5440: ~3600ns/context switch ... (more)Loading...