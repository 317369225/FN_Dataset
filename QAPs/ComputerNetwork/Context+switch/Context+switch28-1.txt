★How does thread switching differ from process switching?In most systems it’s possible to switch between threads much faster than it is possible to switch between processes. How does thread versus process switching differ? What is the performance difference?
Robert already gave a great answer, I will just try to add some numbers on it.  Assuming a not too old Linux 2.6 kernel on a modern x86_64 CPU, here are some rules of thumbs: A simple context switch costs about 3µs of CPU time, on average. A simple thread switch costs about the same, unless you manually manage CPU affinity. With proper CPU affinity management, thread context switching is about 3x cheaper, a tad bit more than 1µs per context switch. If your application uses any non-trivial amount of data, assume that each context switch incurs a penalty of 30µs of CPU time wasted. Some important fine prints. Cost of context switching is highly dependent on the size of your working set.  The more memory pages your program typically accesses, the more expensive each context switch becomes, because of caches that get flushed. Past a certain working set size, the cost of context switching is negligible compared to the cost of accessing memory due to cache misses.  This depends mostly on the size of the caches in the CPU you're using. You cannot rely on Linux's scheduler to magically make thread context switching significantly cheaper than process context switching.  In my experience, without manual tuning the scheduler doesn't do a good job at achieving CPU affinity, so threads tend to bounce around a lot.  A typical server has lots of background processes taking small amounts of CPU time, and unless you pin processes to specific cores, you'll see that the costs of context switching and thread switching don't significantly differ in practice, unfortunately. Here's a graph for a pair of old Intel CPUs (circa 2006): The cost of context switching (red line, left axis) increases linearly with the size of the working set.  The amount of time needed to write a page (green line, right axis) has a big step function as soon as the working set no longer fits in the 32KB L1d cache. On a pair of more recent Intel CPUs (circa 2009), with the Nehalem microarchitecture: Here the big step function appears as soon as the number of pages in the working set no longer fits in the TLB.  Note that this graph's x axis is on a log scale to cover a wider range of working sets. All of the above comes from some investigation I did in 2010, you can find more details as well as source code to measure those numbers yourself at http://blog.tsunanet.net/2010/11... Also finally note that the cost of context switching can be reduced by using huge pages, so that fewer entries need to be repopulated in the TLB after each switch.