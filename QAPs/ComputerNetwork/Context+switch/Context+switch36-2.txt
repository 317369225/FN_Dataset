★What is the difference between concurrency and parallelism?
Not the by-the-book definition, but helps distinguish them: Parallelism is when multiple code flows are running in parallel. Your email client is running in parallel to your browser. You don't need to close one to open the other one. In more detail, parallelism is when your mail is updating multiple accounts at the same time, or multiple browser tabs. In more detail, is when an algorithm can break down a task in separate parts, and execute each part in its own code branch. Parallelism should mean code running at the same time, but in practice it may not happen. If there is only one CPU, the code will run interleaved. In multiple CPUs, it will still run interleaved, but multiple at the same time. Parallelism is also when code is launched remotely into other machines. Parallelism's challenge is identify the data or task to execute and understand how to break it into smaller parts. Until....  they hit concurrency! Concurrency is the when at least two code flows are really running at the same time, and trying to access a shared resource - an internal memory position (thread variable, shared memory, etc.), a file, a database, etc. The running at the same time may even happen on interleaved one-at-the-time single CPU cases, because the access to the resource is not atomic, and if we consider the actions a+b+c, the CPU may be interrupted before c is finished, and other task may try to access it. Concurrency is about ensuring actions into shared resources are atomic, usually by wrapping into locks so only one code can access it at the same time. The challenge is to rearchitecture the whole code to avoid these locks at maximum, because each lock means there will be tasks doing nothing until the first one finishes.