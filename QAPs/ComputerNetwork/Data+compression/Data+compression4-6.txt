★Is data compression a dual of learning? If not, why?Just like the reverse version of MDL
In some respects compression and the classification task in learning are very close.  In a discrete variable setting, in order to compress a source X1 X2 X3... you ideally would like to know pn(X1 X2 X3 ... Xn), i.e. the law governing the X process. If you did know pn(X), then you could derive from it an optimal compressor which assigns a code length close to log pn  (the mechanics of the compressor are very practical - for example arithmetic coding will do the trick). If Xi represented a feature vector concatenated with its label then similarly you could derive a classifier using the underlying probability law, by guessing the label that is more likely given the features. So both compression and classification can benefit from very good modeling of data. However classification need not have a perfect model of data to do very well in terms of accuracy. Still, if you insisted on having a classifier with very good log loss then you would be placing very similar demands on your estimation of pn as compression would. In practice though the techniques used are quite divergent. It will take more time to see these fields unified.