â˜…What is the next big thing in computer networks?
** updated to include multiplexing advances** Applied networking is one of those fields that advances slowly. Very slowly. This isn't necessarily due to a lack of innovation, but more a lack of momentum in enterprise environments; when your entire business is hinged upon something working well and reliably, it's not a place where you want to introduce change quickly or haphazardly. That being said, there are two major places where I see networking changing from a technical point of view. First are the data center protocols being developed by engineering bodies such as the IETF and the IEEE, and then there are advances in multiplexing enabling incredible bandwidth increases. Data center protocol advances Things are coming to a head in the world of data centers, and this change is largely due to the confluence of cloud/virtualization, tiered applications and networked storage. Specifically there are three major disruptive changes coming to corporate data centers near you: 1) Software Defined Networking (SDN) Software Defined Networking is such a game changer, such a potentially disruptive technology that's very difficult for even someone who understands it to explain completely or apply it to use cases. But here goes. SDN is a layer of abstraction and interaction between networking hardware and the infrastructure/applications that rely on it. Think of it as an API that allows changes to physical or virtual network hardware/software via a centralized controller. SDN is a return from distributed control planes (where each router or switch is configured independently of one another) to a centralized control plane. This means that applications reliant on a network can speak to a centralized controller that can make infrastructure wide-changes on behalf of the application to make the network work better for the application. The simplest use case that I've heard people talking about in the data center realm is an example involving virtualization: a virtual machine is moved to a hypervisor somewhere in a data center. The virtual machine requires certain network resources (such as access to a iSCSI storage vlan and at least one data network. If the host hypervisor doesn't have these resources available, the process issues a request to a SDN controller and the controller makes the necessary adjustments to the networking infrastructure to bridge the vlans to the upstream switch. 2) Layer 2 Multipathing technologies (L2MP) L2MP is a hot topic in the IEEE and IETF right now as working groups are struggling to define and approve standards that allow layer 2 switches to be directly connected to each other in a mesh environment without the use of a bandwidth-killing protocol such as spanning-tree protocol. The exciting thing about L2MP technologies is the ability to create larger layer 2 domains that can better support east/west traffic patterns created by storage and multi-tiered applications without having to run backup links that sit idle while the active link is operating at peak capacity. The two emerging standards fighting for the L2MP crown are Shortest Path Bridging (SPB- IEEE) and Transparent Redundant Interconnection of Lots of Links (TRILL- IETF). SPB is a set of protocol enhancements to the existing IEEE 802.1 suite of Ethernet protocols that are known as Data Center Bridging (DCB). DCB includes a lot more than just SPB, and I'll get around to that later. SPB early adopters include companies such as Avaya and Alacatel-Lucent. As you might be able to deduce from those companies, these protocols are more geared towards the service provider/telco environments, but could work just as well for enterprises. I've heard it said that SPB is better for service providers because it doesn't require a massive swap of gear to get it running. TRILL on the other hand works quite similarly to DCB, but is being adopted by heavy hitters such as Cisco, Brocade, Oracle, & Force10 Networks. TRILL does require a forklift of data center gear to operate. I haven't heard of anyone running these protocols in production environments quite yet, but it's probably happening somewhere. 3) Data Center Ethernet (DCE) DCE has emerged largely from the IEE SPB projects as well as efforts to get the fibre channel over ethernet protocol (FCoE/T11 FC-BB-5) to run reliably. Fibre channel requires a lossless transmission medium, and Ethernet, by design, is a lossy medium. To bridge this gap, DCE/SPB standards have emerged to allow more sophisticated control signals to be passed between interfaces, bridges and routers. These signals allow for various devices to more efficiently throttle and queue packets to allow for the most time and loss sensitive data to be transmitted over a network without dropping packets, delivering them out of order or introducing significant queuing/processing delays. While the designs may have originated for FCoE, the overall data center fabric benefits, as other protocols may not be loss sensitive, but can still benefit from the technology upgrades. Advances in data rates Fudging around with timing and spectrum frequencies have been the traditional method for increasing the data rates capable of sending and receiving data over unchanged medium for the last 40 years, and the scientists and engineers behind this technology only get more and more innovative. The latest multiplexing technology involves rotating lasers at high speed while applying an orthogonal formula (that I couldn't even begin to explain or really understand) to encode the data. The result? 100 Tb/s of bandwidth. So, while it's not coming to a cable modem near you any time soon, it definitely is illustrative of where things will end up in the next 10-20 years. If you want to get really geeked out, read the Nature Photonics article here: http://www.nature.com/nphoton/jo... Or, if you're a mere mortal, read the BBC write-up of the same article: Laser smashes data rate records