How does file compression work?
Most compression programs use a variation of the LZ adaptive dictionary-based algorithm to shrink files. "LZ" refers to Lempel and Ziv, the algorithm's creators, and "dictionary" refers to the method of cataloging pieces of data. In most languages of the world, certain letters and words often  appear together in the same pattern. Because of this high rate of  redundancy, text files compress very well. A reduction of 50 percent or more is typical for a good-sized text file. Most programming languages are also very redundant because they use a relatively small collection  of commands, which frequently go together in a set pattern. Files that  include a lot of unique information, such as graphics or MP3 files, cannot be compressed much with this system because they don't repeat many patterns (more on this in the next section). If  a file has a lot of repeated patterns, the rate of reduction typically  increases with file size. Also, more pervasive patterns might emerge in  the longer work, allowing us to create a more efficient dictionary. This efficiency also depends on the specific algorithm used by the compression program. Some programs are particularly suited  to picking up patterns in certain types of files, and so may compress  them more succinctly. Others have dictionaries within dictionaries,  which might compress efficiently for larger files but not for smaller  ones. While all compression programs of this sort work with the same  basic idea, there is actually a good deal of variation in the manner of  execution. Programmers are always trying to build a better system.    Embed Quote 