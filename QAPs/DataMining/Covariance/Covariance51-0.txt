In multiple regression, why do correlated covariates (independent variables) lead to larger errors in the estimated regression parameters?
Jay Verkuilen's answer based on   is entirely correct mathematically, but (at least for me) not much immediate help intuitively. My intuitive understanding runs as follows. First, let's think for a minute about simple regression (one independent variable). A good regression looks something like this: The red line is the conditional mean of the true data-generation process; the blue line is the estimated regression line; and the grey area is the confidence interval for the conditional mean (not for the predictive interval; which is why a lot of the points are outside the grey). Now, what happens if we have the same amount of data, but over a narrower range of x values? This: You can think of each of the data points as being a spring-loaded "support", and the blue line as trying to "balance" on these supports. A narrower range of x's makes it harder to "balance". The confidence width at the center will tend to be the same; but it will quickly deteriorate to large values as you move away from the center. Now let's move to a multiple regression with two independent variables. If they are uncorrelated with a marginal variance of 1, the x values will look something like this: Imagine the conditional means for the Y values are in a plane that is floating out of the screen above those measured X points. That plane rests on a spring-loaded "table leg" at each of those points. Because the legs are reasonably widely spaced in all directions, that "tabletop" is reasonably well-balanced. Now, consider the case when the covariates are correlated. That gives you a picture like this: Now, if you rest your estimated conditional-expected-Y surface on spring-loaded legs at each of these X points, it is no longer well balanced in all dimensions. Along the line it is stable; but across the line, it can easily tip. You can push this intuitive understanding further, if you think about reparameterizing the X's "along" the values above. For instance, in this case you could use Xᵢ₁*=Xᵢ₁+Xᵢ₂ and Xᵢ₂*=Xᵢ₁-Xᵢ₂. This gives two uncorrelated covariates; Xᵢ₁*, with low variance, and Xᵢ₂*, with high variance. Now, because these reparameterized covariates are uncorrelated, Jay Verkuilen's answer becomes intuitive; . That is to say, the low variance of one of the uncorrelated covariates leads directly to high errors in the Y slope along that dimension. If you then return to the original coordinates, the high errors from the low-variance reparameterized dimension now impact both of the original dimensions. (Note that this kind of reparameterization can be statistically useful, but in practice the low-error dimension might be scientifically meaningless.) Yet another way to think about this is using "variance inflation factors". That is, , where S² is the sample variance of that covariate, R² is the proportion of that variance which is explained by the other covariates, and 1/(1-R²) is called the variance inflation factor. (Thus, the product of the denominators is the unexplained sum of squares for this covariate.) If you look at the pictures above with this equation in mind, the variance inflation factors are directly related to the skinniness of the support along the each of the original dimensions, and thus the "tipsiness" along those dimensions.    Embed Quote 