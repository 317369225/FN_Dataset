★What is an eigenvector of a covariance matrix?
One of the most intuitive explanations of eigenvectors of a covariance matrix is that they are the directions in which the data varies the most. (More precisely, the first eigenvector is the direction in which the data varies the most, the second eigenvector is the direction of greatest variance among those that are orthogonal (perpendicular) to the first eigenvector, the third eigenvector is the direction of greatest variance among those orthogonal to the first two, and so on.) Here is an example in 2 dimensions [1]: Each data sample is a 2 dimensional point with coordinates x, y. The eigenvectors of the covariance matrix of these data samples are the vectors u and v; u, longer arrow, is the first eigenvector and v, the shorter arrow, is the second. (The eigenvalues are the length of the arrows.) As you can see, the first eigenvector points (from the mean of the data) in the direction in which the data varies the most in Euclidean space, and the second eigenvector is orthogonal (perpendicular) to the first. It's a little trickier to visualize in 3 dimensions, but here's an attempt [2]: In this case, imagine that all of the data points lie within the ellipsoid. v1, the direction in which the data varies the most, is the first eigenvector (lambda1 is the corresponding eigenvalue). v2 is the direction in which the data varies the most among those directions that are orthogonal to v1. And v3 is the direction of greatest variance among those directions that are orthogonal to v1 and v2 (though there is only one such orthogonal direction). [1] Image taken from Duncan Gillies's lecture on Principal Component Analysis [2] Image taken from Fiber Crossing in Human Brain Depicted with Diffusion Tensor MR Imaging    Embed Quote 