What is the relation between zero covariance and independence? What are examples in science of variables which are not independent but have zero covariance?
In general terms, correlation and covariance measure whether two random variables have a linear relationship. Statistical independence is about whether the variables have any relationship at all; i.e. whether knowing something about one tells you anything about the other. Here's a simple example. Random variable A takes on the values {1, -1} with equal probability. Random variable B depends on A as follows: If A = 1, then B takes on the values {1000, -1000} with equal probability. If A = -1, then B = 0. A and B are uncorrelated, that is, they have zero covariance: both variables have zero mean, and the expected value of A B = 1000 - 1000 + 0 - 0 = 0. But the two variables are clearly dependent; without knowing B, A could be +1 or -1 with equal probability. But if you know B, then you also know A exactly! And if you know A, you know B's absolute value exactly. (http://en.wikipedia.org/wiki/Unc... discusses this a bit as well.)    Embed Quote 