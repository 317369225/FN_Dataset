★Has anyone tried cyclic wiring of the neural network?What I mean by "cyclic wiring": Connecting some output neurons directly to the input ones. This could provide term memory or perhaps simulate kind of "consciousness" :). I just can't find any experiments with this.
You could freely interprete this figure in several ways, since it does not specify details of the underlying algorithm, like the sequence of transmissions or weight updating rules.   One interpretation, which you mention, is that the neural network works just like an ordinary feedforward net but that the feedback provides a memory of the past example. This is useful if the examples are samples from a time series and feedback loops are indeed how recurrent networks like the echo state networks retain a memory of past events. However, for a feedforward net, such information will usually be encoded directly in the input, by e.g. specifying the time derivative or a moving average as an input, if needed.   Another interpretation is that the network is predicting the value of a missing input. Work in chaotic synchronization has shown that the dynamics of a system can be accurately reconstructed with only partial information, say the two first inputs in the above figure. However, reconstructing the missing input can have a stabilizing effect, even if it's the prediciton system itself doing the reconstruction.    Embed Quote