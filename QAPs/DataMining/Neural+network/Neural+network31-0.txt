★Has anyone tried cyclic wiring of the neural network?What I mean by "cyclic wiring": Connecting some output neurons directly to the input ones. This could provide term memory or perhaps simulate kind of "consciousness" :). I just can't find any experiments with this.
This is called a recurrent neural network, as several people have pointed out already. There exists a large number of permutations on this theme with regard to which layer(s) are feeding back to the system. A more common situation is where the hidden layer feeds back upon itself (hidden-to-hidden recurrence). The power of a recurrent network, compared to a simple feed-forward network, lies in its ability to capture patterns that would not have been possible otherwise. George Hinton gives a great example of this in his Coursera course on neural networks by illustrating how a hidden-to-hidden setup can be used to perform binary addition. The addition of two numbers may seem very simple to us (simple calculators have been doing this for decades), but the point of this simple example is that the network is capturing a pattern of adding binary numbers (how the digits in each column are carried) and is not simply performing mapping of inputs to outputs. Here's a link to the video: Coursera. Recurrent networks are especially well-suited for two types of situations where simple feed-forward neural networks may struggle: 1. Characterizing sequences 2. Characterizing temporal patterns While they may seem at first to be different phenomena, the terms "backpropagation through time" and "a layer feeding back to itself (or another layer" are really one and the same mathematically. An interesting and perhaps less widely implemented model (thus far) is the Boltzmann machine. A deep Boltzmann machine (with multiple hidden layers) can capture in theory many of the same types of sequential/temporal patterns that a recurrent network can capture. Where the Boltzmann machine really becomes interesting is in situations in which the input data vectors are incomplete. This was the case in the Netflix competition that challenged solvers to develop a model to predict viewer's movie ratings, and it's also the case in many real-life problems. The winning team used an average of many Boltzmann machine models to solve this problem. This is a nice paper that illustrates how Boltzmann machines can shine: Restricted Boltzmann machine for collaborative filtering by Salakhutdinov, Mnih and Hinton. Neural networks falter where data is missing. Recurrent neural networks, unlike binary systems like Hopfield nets and Boltzmann machines, are not energy-based and therefore have the potential to behave chaotically. This is their biggest draw back, in my opinion. The ability of many different Boltzmann machines to be used for each unique input vector, and then to be able to combine these models, is a remarkable property that makes these models attractive for solving a gamut of real-life problems.    Embed Quote