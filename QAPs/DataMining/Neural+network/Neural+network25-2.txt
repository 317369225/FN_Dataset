★What is back propagation in neural networks?
Lets say your Neural network NN has random  wieghts( or all 1),  Now u want to update your weights to get the required output. You are getting (EXpected - actual) output difference. Now this differennce in output called ERROR will act as a feedback to update the weights, This ERROR is function of input(s), output and all weights One can calculate the (partial differenciation or GRADIENT or variation ) of Error with respect to a particalar weight on a layer i.e d(Error)/ weight(i,j) . This GRADIENT can tell us  whether ERROR is increasing or decreasing with increasing that particular weight. So we change that particular weight's value according to that GRADIENT. We repeat this weight updation strategy called gradient descent for all weights This is called backward propagation because,  the GRADIENT for a particular weight is  a function of all network nodes and weights ahead of it i.e In other words, while calculating output(s), one weight's [weight(i,j)] signal are forwarded to some nodes and weights ahead of it [called forward propagation], now that same nodes and weights are deciding the GRADIENT of that weight [weight(i,j)] by propagating  ERROR back called backward propagation.    Embed Quote