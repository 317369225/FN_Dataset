★What is back propagation in neural networks?
When you use a neural network, the inputs are processed by the (ahem) neurons using certain weights to yield the output. This is like a signal propagating through the network. When training the network, you generate an error signal when the inputs are `propagated' through to the outputs (usually the difference between outputs and the expected known values). Now the errors are used to change the weights, that is, the errors are processed to generate a change in the weights, also called an update.  It's like the errors are propagating backwards through the network to yield a better set of weights that would match the inputs to the outputs, at least on the training data. That's a crude way to understand `back propagation'. The back propagation step yields new weights for the neurons, through a process of optimization via gradient descent (this is the most basic method). You could also call the step a feedback step.    Embed Quote