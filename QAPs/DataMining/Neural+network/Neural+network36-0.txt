★Can you train neural networks in parallel?For instance, could I split my data set into two halves, train two networks with the same architecture, and then combine them somehow?
Yes you can! There are many papers about that. The approach you describe is called data parallelization  and one example is described in [1]. The general idea is that there is a single master model which dispatches multiple copies of itself, training in parallel on different subsets of training data. When all copies have finished on their share of training data, the parameters of different models are averaged and the master model is updated with the difference. Then, again, the master dispatches new copies of itselt and the whole process is repeated until convergence. There are, of course, other ways to parallelize. Some different strategies are described in [2] References [1] Huang, Zhiheng, et al. "Accelerating recurrent neural network training via two stage classes and parallelization." Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013. [2] Chen, Xie, et al. "Pipelined Back-Propagation for Context-Dependent Deep Neural Networks." INTERSPEECH. 2012.    Embed Quote