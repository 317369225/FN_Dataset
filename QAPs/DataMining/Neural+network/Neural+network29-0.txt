★Artificial Neural Networks: What would a neural network with Alzheimer's look like?
I think there are two neural network that work a bit like my grandpa with Alzheimer's. So my grandpa got Alzheimer's, but he is still able to have a conversation with someone. When I go visit him, I can speak with him and it will make sense but at some point he'll start talking about my mother like she is a total stranger to me. So that means he got no clue of who I am, but from the context he manage to understand I'm a guest he should entertain, and he asks me always the same question when I arrive: "Did you have a nice trip ?", "Are you a bit tired ?", "Would you enjoy a drink", and so on... Back to the question. It happens that Recurrent Neural Network works just like that. The simplest RNN I know was trained to produce the next character in a sequence of characters. So basically each time you give him a character, it use an internal state and this character to update it's internal state and predict the next character. So if you first gave him a sequence of characters, and then each time he tells you a character you give him back to him, he is able to write sentences. It's somewhat impressing that just by predicting character one after another (including spaces)  is able to form something that looks like English. Here is the output of a RNN traineb by Sutskever, Martens and Hinton (http://machinelearning.wustl.edu...), which was first fed by "The meaning of life" The meaning of life is the tradition of the ancient human reproduction:  it is less favorable to the good boy for when to remove her bigger. So it doesn't make sense but it looks like English. As for my grandpa the conversation isn't what it should be but it's a conversation. The common point is that they both remember long term information - what English looks like and how you should behave with guests. In the RNN the weights of the network are playing that role. But as they have limited memory they can't remember everything (like what the words are meaning, or who the people are). But they somehow manage to have a coherent behavior (at their own scale) using a short term memory (the internal state for the RNN). Now point 2: Dropout Training So I lied a bit, it's not exactly a Neural Network but rather a way of training them. The idea is that when you train the NN, each time you ask him to predict something, you hide half of his neurons randomly. That forces the NN in not relying too much on an individual neurons to make up is decision. It makes the NN more robust and to generalize better. In a way my grandpa is doing something like that. Because he knows that he can't trust his brain on who people are, he has to use context information, like how is wife is behaving with them, to treat them properly. In a general manner Deep Learning still have a lot to learn from how the brain works and how it grows.    Embed Quote