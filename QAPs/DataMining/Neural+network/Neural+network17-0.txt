★How big is the largest feedforward neural network ever trained, and what for?
The most computationally intensive feed-forward network to date that does learning may be the visual object recognition network built by Le et al (2011) as a collaboration between Google and Andrew Ng's lab at Stanford. [1] Their network comprised millions of neurons and 1 billion connection weights. They trained it on a dataset of 10 million 200x200 pixel RGB images to learn 20,000 object categories. The training simulation ran for three days on a cluster of 1,000 servers totaling 16,000 CPU cores. Each instantiation of the network spanned 170 servers. What is interesting about their model is that they used unsupervised learning, so the algorithm was not told in advance what the object categories were or where objects were located in the image. The main result they report is that the network spontaneously learned a reasonably performing face detector from YouTube video frames. Eugene Izhikevich ran a simulation on the scale of the human brain that used 100 billion neurons and 10^15 synapses. It ran for 50 days on a cluster of 27 servers to generate a 1 minute simulation. This network did no learning -- it only simulated neural dynamics. In fact the network was so large that it could not all fit in memory, so the neurons were deleted and regenerated at every simulation time step. He describes it on his webpage dedicated to large scale neural network simulations: http://www.izhikevich.org/human_.... The problem with trying to simulate a model of the brain is that no one knows how the brain works yet. So these simulations can "go through the motions", doing things that neurons are known to do. But getting intelligent behavior out of these models is a big challenge. The magic of the brain is not the number of neurons, but how the circuits are wired and how they function dynamically. If you put 1 billion transistors together, you don't get a functioning CPU. And if you put 100 billion neurons together, you don't get an intelligent brain. Related: How many neurons are needed to create a conscious entity? What animals are computers now smarter than? How much computing power would be needed to simulate a virtual world that is indistinguishable from the real world? ----- [1] Le QV et al. (2011). Building High-level Features Using Large Scale Unsupervised Learning. ArXiv. (http://arxiv.org/abs/1112.6209) [2] Izhikevich E (2008). Large-scale model of mammalian thalamocortical systems. (http://scholar.google.com/schola...)    Embed Quote