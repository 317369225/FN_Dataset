★Why are feed forward neural networks currently more powerful than recurrent neural networks?
Recurrent neural networks (RNNs) are much harder to train, even with Google-scale computation and data resources. There are two primary problems: The vanishing gradient problem is worse for algorithms like backpropagation. With a feedforward network, you backpropagate until you reach the input layer, but with RNNs, you can backpropagate as long as you like by "unrolling" layer cycles in the RNN. This makes RNNs capable of deeper and richer representations (most notably, it retains a memory of previous inputs), but deeper networks require exponentially more data. This can be helped somewhat with modern methods like RELU nonlinearities, but from what I hear, it's not good enough. RNNs have a hard time producing periodic sequences. Many problems involve some sort of spatially or temporally correlated information, and RNNs have trouble learning multiple "octaves" of repetition. For example, one might be able to bend a network to learn short term patterns, but then it ends up forgetting long-term patterns.Some recent approaches like Memory networks and LSTM have displayed promising results at tackling these issues, but by and large, feedforward networks are more powerful than RNNs simply because they are the only ones that work right now for practical industrial applications.    Embed Quote