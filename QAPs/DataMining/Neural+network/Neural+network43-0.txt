★What is a tiled convolutional neural network?What is it? And what is the difference between it and a regular CNN?
Hidden layers within Convolutional Neural Networks reduce the number of parameters by "tying" together the adjacent NxN weights surrounding each input neuron. Each neuron in the hidden (convolutional) layer is only connected to an NxN grid of its surrounding neighbors (centered on a given neuron in the input layer), and the corresponding weights in each NxN grid connecting each hidden layer neuron to the input layer are the same (shared) across all hidden layer neurons. This weighted "local receptive field" is mathematically equivalent to a convolution operation (and a convolution is a special case of the more general "matrix multiplication" operation, expressed in fully-connected neural networks where the weights are "untied"). Why tie weights together like this? Three reasons: 1.) The number of learned parameters is drastically decreased, making models more computationally tractable given the limited compute power of even top-of-the-line GPUs (to say nothing of CPUs).  2.) Training can be accomplished with fewer examples than in a fully-connected neural network. 3.) Convolutions are translation-invariant (as in LTI system theory, the convolution operation itself commutes with translations in the input). Tiled Convolutional Neural Networks are an extension to Convolutional Neural Networks that learn k separate convolution kernels within the same layer. These convolution operations are applied over every k'th unit (hence the "tiling"). Even k=2 has been shown to give good results. The advantage of this is that through the pooling operation (where layers are "downsampled" by taking the max, average, or even stochastic combination of each pxp window in the output of a convolutional layer, across many tiles -- where k=p has been shown to give good performance), the tiled layers can provide rotational and scale invariance as well as the translational invariance that comes from having convolutional layers in the first place. Moreover, each convolution operation is effectively learning an additional feature (or map), which is a learned representation of the training data, and the tiled layers, like convolutional layers, also still have a relatively small number of learned parameters. In essence, it is the pooling operation over these multiple "tiled" maps that allows the network to learn invariances over scaling and rotation. For more information, feel free to read the original Stanford paper at: Page on stanford.edu    Embed Quote