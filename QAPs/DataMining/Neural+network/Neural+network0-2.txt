★How do artificial neural networks learn?
Artificial neural networks are trained by being given a large number of inputs, together with the correct output for each.  Kind of like the way you train a dog.  You issue a command then show them what you want them to do in response. But that is basically rote recall, and an artificial neural network does something even more difficult, which is learn is a sort of "average", so that you can later give the network an input it hasn't seen before, and it will give an output close to what you expect. How they generate an output, given an input, is by having a built-in internal model that has a lot of variables in it.  If the variables are set to random values, and the model is given an input, then it will generate a random output.  What the training does is search for a set of values for the variables that causes the outputs from the model to match to the correct outputs given in the training set. In fact, training starts by giving random values to all the variables.  Then all of the inputs + correct outputs are given to the model.  It measures the error between what it generates versus the correct output.  Then it changes the values of the variables in the direction that will reduce the sum of the errors.  Then it repeats, until changes in the variables no longer reduce the summed up errors (summed over the whole training set of inputs plus correct outputs). Now, how it decides the direction to change each variable, and the amount, is by knowing something about the model.  The model built in to most artificial neural networks makes an input consist of a large number of "digits", and provides one variable for each digit that is multiplied by that digit.  Then it adds together the results of all the digit times variable multiplies.  That final sum is then passed through a limiter, so there is a maximum output value (both max positive and max negative).  That is how the model takes input digits and produces an output from them.  It multiples the digits by the variables then sums the results. It turns out that with a model of this form, you can always tell whether increasing an individual variable's value will increase the error or decrease it.  That is, if you give the model a set of input digits, and measure the error versus the correct output, then you can tell how much each variable contributed to the error.  So, for a given input in the training set,  the network makes a recording for each variable.  It measures how much that variable contributed to the error generated by the network, for that input.  It also records the direction to change the variable to reduce the error for that input. It keeps a running sum of the size of contribution and the direction to change it. At the end, for each variable, it has a sum of the individual changes that would have reduced that variable's contribution to the errors.  Then it just adds that sum to the variable value. To be cautious, though, it only adds a fraction of the sum.  This gives a new variable value.  Then it repeats the process, until the summed error stays nearly the same two times in a row.  (There are variations: some neural networks save up all the corrections and apply the sum at the end, while others, like "back propagation" change the variables after each input) It's pretty simple, conceptually, and quite amazing that it works as well as it does. The magic is in being able to look at a single variable and see whether increasing that variable's value will increase the error or decrease it, and by how much.  A major breakthrough that started artificial neural networks was in discovering a model that has this property.  This form of model is called a "linear combination", and it enables the process described above, which is called "gradient descent".  The way to calculate the contribution of a variable to the error is by "taking the derivative".  The derivative points in the direction of steepest change of the error, with respect to the variable differentiated upon.  (Small correction: in order to make this all work, the square of the error is actually used everywhere). Recently many variations have been discovered to this basic approach.  They all have the same underlying idea of starting with a model that has many variables, and measuring the contribution of each variable to the error, then adjusting the variables and repeating.  However, for some variations, this gets hidden deep down inside the math. Sean    Embed Quote