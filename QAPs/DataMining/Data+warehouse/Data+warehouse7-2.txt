★Do you prefer data warehouse or Hadoop for big data process?I find traditional data warehouse users would not accept Hadoop due to its hard coding and debugging, but without it, how do you deal with big data problems now? Do you have to scaling up the database/DW continuously due to the growing data size and high concurrency?
It all depends on the problem space.  Hadoop can be really painful for someone coming from a traditional RDBMS background and trying to do the same things they previously did on a traditional database. Efforts like Impala, Stinger, and Drill are trying to make that easier, but they are still in their infancy.  Where I prefer Hadoop and related tools is for problems where the relational data model isn't a good fit.  Usually this is when I need to do anything that involves non-trivial transformations and computations on the source data.  As an example, I might use Sqoop to extract data from the data warehouse and dump it to HDFS.  Then I would use Spark, Pig, Cascading, Scalding, or something similar to prepare the data for input into a machine learning algorithm.  Then maybe I could pass that data into a command line tool like Vowpal Wabbit using Hadoop streaming. Or maybe I would use a native machine learning library like Mahout or  MLlib to do it in the original data processing language. Basically, I prefer Hadoop and related tools whenever I need to create a non-trivial data processing pipeline.  SQL and traditional data warehouses are really good for querying and selecting existing, structured data sets, but it isn't as good when you need to do non-trivial data processing.    Embed Quote