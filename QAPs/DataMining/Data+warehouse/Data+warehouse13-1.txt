★Is data warehouse normalized or denormalized? Why?
Denormalized. Let's first start with a regular normalized database. The aim of Normalization is to reduce data redundancy. To achieve this, your one complex table (for example, a table which has information regarding 'Movies', 'Viewers', 'Actors') 'is broken down into numerous smaller tables (one for 'Movies', one of 'Viewers', one for 'Actors' and some relationship tables used to link these three tables). As a result, if you use a database with normalized architecture, you need to join these n number of smaller tables when you need to pull/retrieve data (for example, if you had to list/count the movies with female directors which were viewed in the last decade). Every join is costly!- which means your 'analysis' (OLAP) query would run for a long time. And, if every analyst is running such queries (which they will because a data warehouse is an enterprise database), your query will run forever. To understand why data-warehouse is a time-critical for reading data, you need to remember three things. The purpose of a data warehouse - data aggregated across various departments of an enterprise to make effective centralized decisions. It contains historical data which you would most often scan to compute statistics like counts, sums, averages etc. Data Warehouse (OLAP) is a read-only database unlike OLTP (T stands for transactional -  meaning read & write database) which is the exact opposite. The first two points (which cause the data warehouse to store huge amounts of data) coupled with the third point demand the need of a denormalized structure. Also note that most of the techniques, concepts of query performance for a OLTP database is the exact opposite for a OLAP (warehouse) database.    Embed Quote