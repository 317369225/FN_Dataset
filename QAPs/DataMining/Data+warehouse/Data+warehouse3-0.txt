★Why are data warehouse appliances so popular?If you have a good team of sysadmins and DBAs why wouldn't a normal set of hardware and software work just as well as an appliance? Also, with an appliance, aren't you limiting your expansion options?
Whether you are using a data warehouse appliance, or are building a home grown system using general purpose DBMS software and hardware, simply having a good team of DBAs and Sysadmins is not enough to build a proper data warehouse. First of all, I see no mention of Data Architects and/or Data Modelers.  Regardless of the platform, performance of any data centric system is dependent upon a data model that is logically and functionally complete and designed to handle the types of queries that users of the data warehouse need with the volume of data that is expected to be stored. Now assuming that you have the right technical people to design and build your data warehouse, whether or not you can work with commodity hardware and general purpose DBMS software is largely a function of volume.  There are key architectural differences in the design of hardware and software for data warehouse machines that have to be taken into consideration. First of all, it's helpful to understand how Massively Parallel Processing architectures help in the wrestling of "big data".  In the data warehouse world, an MPP "shared nothing" architecture typically uses a strategy of "divide and conquer" to handle large volumes of data.   Essentially you have one node of the data warehouse cluster act as a supervisor node - its job is to take queries, distribute them to independent nodes that access relevant data, then combine result sets returned from these independent nodes to the user as a consolidated result set.  The strategy of "divide and conquer" depends on the fact that all else being equal, any database can work faster with smaller sets of data than it can with larger sets of data.  Distribution of data is key to the "divide and conquer" approach to performance.  You want all nodes in the cluster to be working together.  If you were to distribute data across nodes based on something like a transaction date, and if end users typically are interested in data from a specific segment of time, you will find that only a fraction of the nodes participate in acquiring data for users. There are two keys to the divide and conquer strategy - Know your data and Know your users. In addition to the concept of a supervisor spreading work to other nodes, another factor in the software design of a DBMS that is found in many (but not all) data warehouse appliances is the use of columnar data stores.  Traditional OLTP databases use row based data stores.   What this means is that if you were to look at the physical layout of data in a row oriented DBMS, you would find that data is physically stored similar to the definition of the table itself.  For example, an Employee record may contain First Name, Last Name, Job Title, Hire Date, and Salary. (Obviously this is a simple example)  In a row-oriented database, the physical layout of data on disk would be exactly as described in the previous sentence.  But in a columnar data store, all the First Names would be stored in contiguous storage, followed by all the Last Names, then the Job titles, then the Hire Dates and finally the Salaries would be grouped together.  (Obviously pointers exist to tie records together) If you think about typical access patterns for an OLTP system as opposed to a Data Warehouse, you can see the benefits of this approach.  It's common in an OLTP system to use indexes to access a single record or a handful of records that match a certain criteria.  Whereas in a large data warehouse, it's common to minimize indexes and do large table scans based on columns that meet a certain critieria.  It's also very common to only be interested in a small subset of columns in a given table, but to want to scan all of the contents of that table in a data warehouse environment. Vertica and Sybase IQ are probably the most well known vendors utilizing columnar data stores.  But Oracle Exadata utilizes a special version of Oracle 11R2 that is a hybrid columnar store.  Aster Data, ParAccel and Ingress are also columnar oriented. In addition to differences in capabilities of underlying DBMS software, there are also tricks that are done with hardware to maximize performance.  One constant on all MPP data warehouse appliances is a high speed interconnection between nodes in the cluster and the supervisor node.  Obviously the "divide and conquer" approach to performance can't be made viable if data just sits on a node after it has been extracted from the local data store. In addition to the high speed interconnections, use of Solid State Disk Drives is becoming increasingly common.  Oracle Exadata utilizes SDDs, and Teradata and Netezza (among others) are rapidly moving in that direction. There are other factors as well, but the simple answer to your question is that if your data sets are small, you can get away with commodity hardware and general purpose DBMS software.  But as size increases, you need to do things to get beyond the wall you are bound to hit regardless of how talented your team is.    Embed Quote