★In regression analysis, does it make sense to continue adding independent variables to predict a dependent variable so long as your adjusted R^2 improves and your standard error falls?Say you don't have a large enough sample to run meaningful cross-validation techniques.  Is your best bet for prediction to feed enough independent variables to maximize R^2 and minimize standard error?  Why or why not?
As you add one more independent variable, your new model subsumes the old one and is always going to better fit the data. Once you have several models, you need some sort of model selection mechanism since as you keep increasing complexity of the models they will start over-fitting. You can use a validation set, perform cross-validation or use any of the other techniques such as BIC/AIC to achieve this. Other alternative is to formulate your model such that it implicitly involves model selection. Typically such models involves a penalty term proportional to the complexity of the model. Examples of such models for regression are LASSO (L1 regularization) or Ridge regression (L2 regularization).    Embed Quote