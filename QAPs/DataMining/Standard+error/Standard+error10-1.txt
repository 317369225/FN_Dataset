★How does one build a stronger predictive regression model?  Optimize for highest adj. R^2?  Lowest standard error?Why not keep adding variables that bumps up your adjusted R^2?  Or only add insofar as it reduces your standard error?  If your goal is maximum prediction power, how do you build the strongest regression model?  If you have a weak R^2, can adding other relatively weak R^2 independent variables/relationships contribute to an overall stronger R^2?  Any resources to learn more on this topic?
If your goal is prediction, you should use a hold-out sample (and ideally many samples, through some sort of cross-validation scheme).  Even better, do a 3-way split, use one set to build the initial model (training set), a second set to calibrate the model (valiadation set), and a third set to assess the model (test set).  Your assessment of the model should be based solely on the test set - what statistic you decide to use shouldn't matter too much (R^2 works just fine, it just needs to be the R^2 of the test set). Note that cross-validation has a stronger history from the data mining/machine learning schools of analysis than traditional statistics.  Intuitively, you should recognize its advantages - it's also been shown theoretically and empirically that cross-validated statistics are generally superior to more traditional model selection techniques like using R^2 or information criterion, especially for prediction.  The biggest problems: it requires more 1. computing power and 2. programming 3. tradition. EDIT: I forgot to mention another major reason why you don't want to do cross-validation.  Small sample sizes.  However, even in this case, 'leave-one-out' cross-validation might provide improvements over using the whole set. Any data mining literature will have coverage of cross-validation, and plenty of modern statistics will cover it as well. Cross-validation (statistics) There are some (free and paid) online courses available that you might enjoy.    Embed Quote