★How does one interpret standard error in time series regression models?Inquiring about the standard error outputted by regression models.  Is this the 'average' error?  Can I expect with some probability that the actual result will be in the range of standard error (or multiple thereof) for future predictions?  What is most commonly misunderstood about this metric?
You don't specify what kind of regression model you're talking about, so let's look at the simplest, which is linear regression with time on the X-axis.  Linear regression fits your data to a model of the form where y is the time series, a is a coefficient, and is a Gaussian random variable with mean zero and the specified variance.  (In other words, we assume that the data comes from such a process, and given this assumption we calculate which parameters give the "best fit" for our data.) If your data really comes from such a process -- if there's a fundamentally linear process relating the inputs to the outputs, with some (homoscedastic!) white noise thrown in at some step(s) along the way -- then linear regression will tell you the coefficients of that process and the volume of the noise.  If it approximately comes from such a process, then you might get an approximate model.  If your data doesn't come from this kind of model at all then you'll get numbers, too, but they won't mean anything. Okay, so let's assume that you're dealing with a process which is legitimately linear -- that your data is really generated by a process of the form , where .  When you actually run the regression, you're only giving it a finite sample, so it can't reliably determine the model parameters.  Instead you'll get approximations and -- for a model like the one above, given any data set whatsoever there's a (likely minuscule, but strictly nonzero) probability that it came from such a model, so strictly speaking you can't really rule anything out. But let's assume that somehow the regression analysis magically came up with exactly the right coefficients.  In this event (which literally has probability zero, even under the previous unrealistically optimistic assumptions), then we can regard the standard deviation output by the regression analysis as telling us something concrete about the problem, namely the distribution of the "error" in the model.  (If you don't know how a Gaussian distribution works and what the standard deviation tells you, go learn that first.) In the real world, all of these assumptions are violated to some extent or other, and the technique works as some kind of approximation at best.  (But of course that's just the nature of statistics, not something specific to regression analysis.)    Embed Quote