★In regression analysis, does it make sense to continue adding independent variables to predict a dependent variable so long as your adjusted R^2 improves and your standard error falls?Say you don't have a large enough sample to run meaningful cross-validation techniques.  Is your best bet for prediction to feed enough independent variables to maximize R^2 and minimize standard error?  Why or why not?
If you're not cross-validating and you keep adding more variables, you're definitely going to run into overfitting due to random noise at some point. For example, let's say I'm predicting house prices. I add "house color" to my variables and my sample set tells me that black houses tend to sell for $20,000 more. Odds are that this is just random noise, and including house color in my model will actually result in worse predictions on other data.    Embed Quote