★How does one interpret standard error in time series regression models?Inquiring about the standard error outputted by regression models.  Is this the 'average' error?  Can I expect with some probability that the actual result will be in the range of standard error (or multiple thereof) for future predictions?  What is most commonly misunderstood about this metric?
Your question is ill-posed, since 'standard error' can have many different meanings, especially in regression. One standard error of the regression, as made less than clear by the ordinary least squares Wikipedia article: Ordinary least squares is really the sample mean-squared error between your estimator and the data. If we assume the model for our data with and , and we estimate the regression function as , then the standard error is just You can show that, under certain reasonable assumptions on , , and , that the mean squared error of our estimator is Squared Bias Variance. In words, this says that our errors using the estimator are decomposed into system noise, systematic errors (bias), and stochastic errors (variance). If we use instead of , the bias and variance terms will drop, and we're just left with the system noise level. So, if we're very careful about choosing , our error, on average, will be , and we won't be able to do any better than this due to noise inherent in the data-generating process. An alternative meaning for standard error is this one, Standard error in which case you are talking about the standard error for something like an estimator of the parameter in a linear regression model In this case, our estimator is itself a random variable (it's a function of our training data), and has a mean and variance associated with it. The standard error of is just the square root of the variance of the estimator. As far as common mistakes with the (first type of) standard error, you want to make sure you compute the standard error on a held out data set (a data set not used in fitting the model), or use something like cross-validation: Cross-validation Computing the standard error on your training set (the data set you used to fit the model) is a bad idea, since it will be overly optimistic as to your future error rates using the model. Intuitively, you may have fit the 'noise' in your training data that will hurt you in the long run, but you won't know it.    Embed Quote