Is there a way to prevent your website (data) from being scraped?
Fundamentally, No. You're Publishing. By the very act of putting data on a web server with a public IP Address, anyone and everyone can copy (and thus process) your bits. You could force "scraping" into an Optical Character Recognition (OCR) problem, by making your entire web site a series of images (GIF, JPEG, PNG (Portable Network Graphics), whatever) with image map links for navigation. However, you would likely incur rather greater web service bandwidth costs with this approach (quite aside from how annoying it would be). Also, your web site would effectively be invisible to the web search engines (e.g. Google, Bing, Baidu). Fundamentally, if you have Intellectual Property (IP) rights to your data, this becomes a patent or copyright enforcement problem, just as in book publishing: anyone can photocopy a book, bind it, and sell it. That's a violation of the publisher's copyright, but it's up to the publisher to enforce his rights. For IP rights holders, the problem with The Internet and digital media in general is that it's much easier and cheaper to copy bits than paper, hence the copyright violations problem is worse.    Embed Quote Updated 14 Apr. 990 views.