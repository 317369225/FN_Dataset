Is there a way to prevent your website (data) from being scraped?
I agree with Clifton, but would add that I believe CAPTCHAs and rate limitations are the most effective tools to prevent scraping.  Rate limiting tools will have limited efficacy where the user frequently changes his IP address, so the most effective way to implement rate limiting procedures is to require users to login before they can access the data.  If those options seem distasteful, the next best option is to render all of your content dynamically (under forms instead of links), and to use cookies to track usage.  A user should not be able to get to deep content unless he has a cookie that provides him access (which will prevent a user from clearing cookies as frequently).  While a would be scraper can still get your content, it is harder, because the the scraper must use a traditional browser instead of a faster headless program.  Additionally, because of the cookies, it is easier to track user usage, and determine whether they are scraping or using the site normally.    Embed Quote 