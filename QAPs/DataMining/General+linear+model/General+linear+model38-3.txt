★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
Newton's Method uses information about the second derivative, or Hessian, of the objective function.  Basic forms of gradient descent do not.  To use a physical analogy in 1 dimension, gradient descent will simply compute the tangent line of an objective function, and take a fixed step size in that direction.  Newton's Method will also account for the "velocity" of the gradient, and take the appropriate step size in that direction, thus Newton's Method will tend to converge in fewer steps.  This does not necessarily mean the algorithm will always be faster, since the inverse Hessian matrix may be difficult to compute.  Wikipedia also has an excellent summary: http://en.wikipedia.org/wiki/New...    Embed Quote