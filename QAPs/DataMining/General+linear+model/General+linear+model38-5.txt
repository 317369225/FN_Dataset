★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
First of all, Newton's method does not always work. It may diverge or the Hessian may be singular. Secondly, for each iteration of Newton's method, it is much more costly because it needs n^2 function evaluations of the Hessian. So, Newton's method can be slower in time than steepest descent if say it takes Newton's 10 iterations and steepest 5 iterations. Thus, we say Newton's method is much faster than steepest descent in a sense that when it converges, it takes much less iterations to get to the local minimizer. Rosenbrock function is a great example on showing this in Brian's answer. Given Newton's method is not as "fast" as you think, here is the state-of-the-art method if you want to know: Broyden–Fletcher–Goldfarb–Shanno algorithm, which can be seen as an improvement to Newton's method. However, you still have to keep in mind that, given real problems, no method is guaranteed to be faster than the others in every case. Advanced methods usually have larger overheads and computation costs in each iteration.    Embed Quote