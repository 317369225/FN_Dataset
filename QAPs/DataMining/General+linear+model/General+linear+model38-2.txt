★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
Newton's method corrects the direction of the search such that it always points to the minimum, while the gradient descent points to the maximum direction of change.That makes NM faster , the higher the Eigen value spread in input correlation data [1] , the more is the difference in the two directions (vanishes for circular contours : eccentricity =1) [1]Gradient information is weighted by the inverse of the correlation matrix of the input (calculation which is one of the flip sides of NM , among others e.g. it may diverge,consumes global information{update for one weight influences all the other inputs in the system} etc)    Embed Quote