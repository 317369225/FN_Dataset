★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
Considering this logarithmic plot of the rosenbrock function: Source: http://www2.imm.dtu.dk/courses/0... http://en.wikipedia.org/wiki/Ros... Without second order knowledge (ie gradient descent), you can miss the narrow valley entirely with fixed sized steps in the direction of the gradient (ie fixed steps are too large). Even if you go into the valley, you would spend a great deal of time zig zagging back and forth the steep walls because the gradient at those walls would simply direct the descent to each side of the valley. Second order information (ie Hessian) allows you to take into account the curvature and take steps sized inverse to the 'steepness' (very steep -> small steps, very flat -> large steps).    Embed Quote