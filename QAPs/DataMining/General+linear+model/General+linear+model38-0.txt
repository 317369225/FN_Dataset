★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
Carlin Eng made a very good point that Newton methods are not necessarily *faster* than steepest descent (in Newton methods, the cost per iteration is usually higher due to the need to compute derivatives); the mathematical notion you want here is not "speed", but "rate of convergence". The steepest descent method has linear convergence, while Newton's method has *local* quadratic convergence (which means if you start your initial guess far enough away, you may not actually get quadratic convergence -- you can find proofs for this). Also, the steepest descent algorithm tends to do badly near the optimum. It's easy to understand this geometrically. As one gets closer to a minimum, the contours become flatter, which causes the steepest descent algorithm to zigzag a long time until it hits the bottom (because steepest descent moves in the negative direction of the gradient). This problem is circumvented in the conjugate gradient method, but that's another story.    Embed Quote