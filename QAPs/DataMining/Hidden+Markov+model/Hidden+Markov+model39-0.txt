★Hidden Markov Models: What's an intuitive HMM example with Continuous Emission Probability?
There is a special type of probability density function called a mixture distribution.  Mixture distributions (or "mixtures") are simply weighted sums of other density functions which are called the mixture "components".  The weights sum to one and each weight correspond to the probability of a sample being generated by the corresponding component density function. A Gaussian mixture model (or GMM) is a particular type of mixture distribution where each component density function is a Gaussian.  So an HMM using GMM's for he emission probabilities is simply computing the likelihood of data given a state using a GMM. Mixture models and HMM's have something in common: they each have a hidden state.  In the case of the mixture model, the state corresponds to the particular component density function that generated a given sample.  The total likelihood of a sample for a mixture is the weighted sum of the likelihoods of the sample for each component density.  The total likelihood of a sequence of samples for an HMM is the sum of the likelihoods of all possible state sequences to the data.  Maximum likelihood training for both types of models can be efficiently performed using the expectation-maximization (EM) algorithm.  In the case of the HMM, the Baum-Welsh algorithm is used for the E step because sequence data is involved.    Embed Quote