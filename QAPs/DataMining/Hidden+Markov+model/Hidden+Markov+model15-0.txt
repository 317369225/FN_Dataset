★What are differences between recurrent neural network language model, hidden markov model and n-gram language model?
RNN do not make the Markov assumption and so can, in theory, take into account long-term dependencies when modeling natural language. In practice however, Learning Long-Term Dependencies with Gradient Descent is Difficult as described by Bengio & al. and, to my knowledge, Mikolov's work is not addressing this problem. His work focuses more on democratizing the use of RNN for language modeling tasks by making them blazingly fast to train in comparison to previous implementations. The fact that long term dependencies are still difficult to learn would argue in favor of using n-gram sequences as an input to the recurrent network and help it have a better "long-term short-term" memory. The main advantages of using a recurrent neural network over Markov chains and hidden Markov model would be the greater representational power of neural networks and their ability to perform intelligent smoothing by taking into account syntactic and semantic features (see for example Turian et al.). By comparison n-grams have a number of parameters exploding with the size of the vocabulary and n and rely on simple smoothing techniques like Kneser–Ney or Good–Turing. I would add (for what it's worth) that I kind of think of the Hidden Markov model vs Recurrent Neural Network "battle" as being similar to a mixture model vs Feed-Forward Neural Network "battle". Even accounting for intelligent smoothing leaves unanswered the question of why RNN seem to perform better than NN on language modeling tasks.  It suggests that indeed RNN are able to take into account some kind of long-term dependencies. However, I am not aware of any recent work on the kind of dependencies that RNN account for. Thus answering your question on what kind of linguistic phenomena RNN can or cannot handle seems difficult. Finally, you may be interested in looking at James Martens and Ilya Sutskever's work on training a RNN with very long short-term memory using Hessian-Free optimization and the demo on a letter level language model here. No linguistic explanation there either but the network is very good at long term dependencies.    Embed Quote