★Why does deep learning/architectures use only non-linear activation function in the hidden layers?I think for feature representation they only use non-linearity functions  in the hidden layers, such as sigmoid. Only the output is a linear  classifier. Is this why they solve high complex problems and they transfer their features via several non-linear functions to better features that are easily divided by a linear classifier?
in terms of what I know, if you use linear activation function in the hidden layers, these hidden layers can be  superposed via superposition principle, and they  can be superposed to just one larger hidden layer , resulting in the same effect in feature representations, however, if we use non-linear activation function in hidden layers, we can transform feature in each hidden layer (oh, luckily, here superposition principle can not be adopted ) ,so we can get a good higher layer feature representation .    Embed Quote