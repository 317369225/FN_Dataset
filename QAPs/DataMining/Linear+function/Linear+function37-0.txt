★With the massive success of piecewise linear activation functions such as rectified linear units, maxout units, and learned variants in deep learning, is there still a use for traditional sigmoidal units? In what regimes would you want to use one over the other?
Sigmoid and Tanh activations are still in use especially for Autoencoders and Generative Network models since they do not have diminishing gradient problem by layer-wise training. Especially for Sparse AEs keeping the encoded values in the binary range is very useful for many problems so ReLU is not suitable for this.    Embed Quote