★Why does deep learning/architectures use only non-linear activation function in the hidden layers?I think for feature representation they only use non-linearity functions  in the hidden layers, such as sigmoid. Only the output is a linear  classifier. Is this why they solve high complex problems and they transfer their features via several non-linear functions to better features that are easily divided by a linear classifier?
In a single-layer neural network, we can think of the outputs as being the result of a given activation function applied to the product between the inputs and the weights of the layer as follows: What we can observe in this case is that if we are using linear activation (thus, ), then a two-layer network described by: can be replaced by a single-layer network with parameters , so stacking more than one linear layer is only going to make your system slower and more complicated, not more expressive. This is not true when using a non-linear activation function, such as the sigmoid , or the hyperbolic tangent .    Embed Quote