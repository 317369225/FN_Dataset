★Why does deep learning/architectures use only non-linear activation function in the hidden layers?I think for feature representation they only use non-linearity functions  in the hidden layers, such as sigmoid. Only the output is a linear  classifier. Is this why they solve high complex problems and they transfer their features via several non-linear functions to better features that are easily divided by a linear classifier?
real world problems are non linear . most problems we work are non linear ie the space is -n dimensional non linear space where they are discriminate and disentanled . may be it takes so much linearity to map a simple non linearity (may be exponential at times) or we may even fail to map non-linearity with linearity . thus linear activation functions are not used in hidden layers    Embed Quote