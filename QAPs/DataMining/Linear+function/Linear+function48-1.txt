★Support Vector Machines: How does going to higher dimension help data get linearly separable which was non linearly separable in actual dimension?
SVMs simply map input features to a space where the number of dimensions is the number of support vectors. The infinite-dimensional mapping refers to expressing the possible features available for use in learning as a transformation of raw input features. You never really have infinite features to deal with; only the implicit RKHS of the kernel can be infinite-dimensional. Finding the ideal mapping when nothing is known about the complexity of the problem requires conceding infinite-dimensional spaces with max-margin regularization to prevent over-fitting. Some answers have mentioned ideal mapping or raising points to a particular level based on the label. This is equivalent to having the label as one of the features. Obviously, the dataset is linearly separable on this feature alone. While this is intuitive and trivial to imagine, the intuition is not sufficient to be carried to the learning and prediction setting where it is assumed that the label will not be available on the test set. Consider that your non-linear separator is defined by a polynomial function of the original features. If you construct an Euclidean space with the monomials of the separating polynomial function (treating the monomials as the new features), the data points will be linearly separable in this new vector space, because your separator is now linear in these new features by construction. It turns out that polynomial functions with no restrictions on the powers in their monomials can uniformly approximate any smooth continuous classifier function (see Stone–Weierstrass theorem). So this condition is not very restricting. For example, the effective max-margin separator shown below is not a very complicated polynomial function of the original features. Also have a look at the following video that visualizes a linear separator on 2D data projected into 3D space.    Embed Quote