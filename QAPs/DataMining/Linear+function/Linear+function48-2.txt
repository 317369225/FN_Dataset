★Support Vector Machines: How does going to higher dimension help data get linearly separable which was non linearly separable in actual dimension?
It's a common misconception of SVMs that the feature-space map has to map onto higher-dimensional spaces for it to work well. In reality, the strength of SVMs lies on the simple re-location of the points. This is what happens: the points are re-located onto another space such that they become linearly separable. This can be achieved by mapping not only onto bigger space, but also into smaller ones. This is also why linear maps do not provide any improvement at all in the classification, no matter how high-dimensional the space is: Because they maintain the same structure of the data. The misconception is, however, quite understandable: Infinitely many more hyper-planes can be determined in higher-dimensional spaces, and thus infinitely more potential separations exist. However, since you are seeking just the one "best" separation hyper-plane, quantity does not always convert into quality. In fact, it's demonstrable that mapping into smaller space is better: Mapping the points onto too high-dimensional spaces enhances over-fitting and weakens generalization properties. Also, try to visualize that the "ideal" mapping is always 1-dimensional, and is simply the mapping which maps the data-point directly to its own label. As an example, take the data-set in Jeffrey M Girard's answer: The ideal mapping is one which maps onto 1 when the point is blue, and -1 when the point is red. Ok, this is the ideal, unrealistic example. But take this other realistic mapping: First assume that the data-set is centered, then just map each point onto the distance between the point itself and the origin. What you'll find is a very good 1-dimensional mapping in which red-points are mapped to low values, and blue points to high values, and thus linear separation is trivially achieved. I hope this answer allowed you to understand a bit more what is the real strength of SVMs: the different structure    Embed Quote