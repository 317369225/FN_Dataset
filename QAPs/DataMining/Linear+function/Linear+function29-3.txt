★Why does deep learning/architectures use only non-linear activation function in the hidden layers?I think for feature representation they only use non-linearity functions  in the hidden layers, such as sigmoid. Only the output is a linear  classifier. Is this why they solve high complex problems and they transfer their features via several non-linear functions to better features that are easily divided by a linear classifier?
This this no longer true. If rectifier is not considered linear, maxout or channel-out can use linear activations. Although a pool is still nonlinear, I think it is meaningful to remove the constraint of nonlinearity from our mind.    Embed Quote