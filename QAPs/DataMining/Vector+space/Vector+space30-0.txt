★What is naive Bayes, vector space classification and support vector machines in information retrieval?And I need ppt, pdf and simple solved example.
The algorithms / model remain the same its all on the data set that you use. Its not like there's a new definition for Naive bayes or SVM. So lets take a classic example of document classification, so you have a set of documents and lets assume for a subset of these you have some tags allocated to them  (for simplicity lets assume that one document can be mapped to exactly one tag), we will treat each tag as a class. One simple way to represent the documents into a vector space is by building a vector for each of the d0cuments. First start with creating a vocabulary, create a set of all the words that are present in at least one of the documents. Use this word-set as distinct dimensions of a hyperspace. Now for each document calculate the frequencies of words in the vocabulary. This would form the vector for the document, this is known as Term Frequency vector. Example : Document 1 : "you are stupid because you ask stupid question" Document 2 : "you are wrong because no question is stupid" your dictionary/vocabulary will be  - {you,are,stupid,because,ask,question,wrong,no,is} so your term frequency vectors are : d1 = [2,1,2,1,1,1,0,0,0] d2 = [1,1,1,1,0,1,1,1,1] You may directly want to start applying models but a better way would be to remove some words which dont convey much info about the document, words like "is","the","you","are", but again it depends on what results you expect from all this. You could also you TFIDF tf–idf instead of just the term frequencies. Even after removal of stopwords your word set would be large and the data set may be sparse, a better way would be to reduce dimensions, for that you can go with PCA(Principal component analysis or ) Latent semantic indexing - read the part which talks about handling sparsity. Once you are through this, start with applying models. Introduction to Information Retrieval    Embed Quote