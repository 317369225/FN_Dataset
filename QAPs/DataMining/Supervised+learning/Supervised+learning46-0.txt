★What are some standard ways to evaluate structured prediction models (e.g. HMM's, CRF's, etc.) in supervised learning?I'm familiar with metrics such as precision, recall, and f-measure for linear classifiers such as logistic regression, but was wondering what metrics are used for structured models. Are they the same? Are they calculated in the same way? Any help is appreciated.
Thanks for A2A. This depends a lot on the specific domain you're dealing with. I'm most familiar with the speech and language domain in which both the input and output are sequences of labels (words, phones, etc...). For this kind of input/output the 2 sequences are typically compared using some variant of the edit distance algorithm, where the specific penalties for insertion, substitution, deletion can vary depending on the task at hand: http://en.m.wikipedia.org/wiki/E... Even within speech recognition there are many ways to evaluate a system's performance: http://www1.icsi.berkeley.edu/Sp... One important point to consider is that even if some metrics are widely used in the research community, do they accurately capture the perceived performance of the system? For example, machine translation does not use the same metric as in speech recognition but something else that better correlates with human judgment: http://en.m.wikipedia.org/wiki/BLEU    Embed Quote