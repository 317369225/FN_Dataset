★In a supervised learning problem, what are some effective techniques that can deal with highly imbalanced datasets?Suppose you have 5 classes. Class A and B have 200 records each, while C, D and E have 20 each. Is there a straightforward way to train ML classifiers on such a dataset?
First, it's important to understand *why* having an unbalanced dataset often causes problems. The reason is because most Machine Learning algorithms are centered around minimizing a cost. This cost is usually based on the number of examples that are misclassified and the complexity of the resulting model. So if you have a dataset where 9,900 records are from class A and 100 are from class B, the lowest cost model is probably to classify everything as "A". That's because the error from misclassifying the B examples is very small-most likely smaller than the cost of the complexity. So there are a few ways around this: 1. Take 20 records from each class. This is the simplest method. 2. Change your error function to give a higher penalty for misclassifying C, D, or E. This method is a little more prone to overfitting and a bit more work, but will generally give better results. 3. Combine your smaller classes C, D, and E into one larger class F. Effectively you'd run two models. The first would predict whether a new record is in A, B, or F. If the first model predicts F, the second model predicts whether the example is in C, D, or E.    Embed Quote