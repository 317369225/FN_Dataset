★In a supervised learning problem, what are some effective techniques that can deal with highly imbalanced datasets?Suppose you have 5 classes. Class A and B have 200 records each, while C, D and E have 20 each. Is there a straightforward way to train ML classifiers on such a dataset?
Suppose you have two classes, one of which is highly imbalanced and you are trying to classify them, let's say a SVM. You could do the following: 1. Remove extra samples from the majority class so that you balance the number of samples in both the classes. In this case, you are compromising by removing some knowledge from the system. 2. Introduce samples in the minority class by just doubling the number of samples, or higher, by simply making copies of them. Here, you are just repeating the samples, and if any of those happen to be the support vectors, they will have a higher influence. 3. An efficient technique otherwise would be to create (I mean artificially create) samples for the minority class using a technique called SMOTING. Here is the paper - Page on Arxiv    Embed Quote