★Supervised Learning: How do current NN approaches deal with the vanishing gradients of Backpropagation?I guess one of the solutions is the unsupervised pretraining of the NN before the supervised phase. What are the other possible solutions?
As far as I know, a common solution is to stack the auto-encoder (a kind of one-hidden layer NN) or restricted boltzmann machine (RBM), both are unsupervised learning and are used for feature extraction. For example, if you want to train a NN with 3 hidden layer. You need to first feed the input to a auto-encoder, the auto-encoder will try to find a non-linear transformation to make the output equals to the input. Train the model and the weights between the input layer and the hidden layer will have the weights for non-linear transformation. After that, remove the output layer and append a new auto-encoder (without input layer). This time use the output of the first hidden layer as input and train the appended auto-encoder. This procedure repeats until the whole NN is trained (3 hidden layer in this example). This is the pre-training stage of a NN with multiple hidden layer. After the pre-training, the model can be trained in normal way (back-propagation).    Embed Quote