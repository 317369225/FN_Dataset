★What are the best supervised learning algorithms for sentiment analysis in text?I want to do text analysis to determine whether a news item is positive or negative for a given subject. What are the best supervised learning algorithms for this?
I am currently interning in Deutsche Bank and my project is to build NLP Tools for News Analytics. Apart from that, I am also doing B.Tech Project under Pushpak Bhattacharya, Centre for Indian Language Technology, IIT Bombay. The solution to your answer depends on the resources you have and the source, domain and reliability of data you are going to use it on. Sentiment Analysis will require the following pre-processing: 1. Noise Removal - Cleaning the data from irrelevant news as well as advertisements/bio(if you have collected data by web crawling) 2. Classification - Categorizing the news data to different domains - "Markets",   "Economy", "Industry", "Technology" and so on. It is as necessary as the algorithm because you will have different set of features for different domains and thus, each domain should have different classifier. For example, A positive news in Technology sector for Microsoft may be a negative news for Apple stocks. 3. Named Entity Recognition - This is the most important part of sentiment analysis as the objective of sentiment analysis is (In words of Bing Liu): "Given an opinion document, discover all the opinion quintuples - entity,aspect,sentiment on aspect of the entity, opinion holder and the time/context of opinion." For example, Sentiment analysis on political news to predict elections will obviously have to extract political entities - Narendra Modi/Rahul Gandhi and the aspects of their campaign - secularism/minority upliftment from the news and then, tag them as positive or negative. 4. Subjectivity Classification - Classifying sentences as subjective or objective since subjective sentences hold sentiments while objective sentences are facts and figures. 5. Feature Selection - The features can be unigrams and/or bigrams or higher ngrams with/without punctuation and with/without stopwords with presence(boolean)/count(int)/tfidf(float) as accompanying feature scorer for each sentence/paragraph/file. Filtering Stopwords reduces accuracy. Adverbs and determiners that start with "wh" can be valuable features, and removing them as English Stopwords causes dip in accuracy. Similarly, punctuation helps in detecting sarcasm and exclamation. I used unigrams and bigrams without removing stopwords but removing proper nouns as my features. Presence, Count or TfIdf score were used depending on classifier as Presence gave better results with Naive Bayes while Tf Idf gave better results with Linear SVM. I then, did an association measure test based on chi square/poisson stirling ratio/likelihood ratio to find the most informative features and used them to train the model.  6. Sentiment Extraction - It can be done using unsupervised learning, supervised learning, sentiment lexicon based approach or a mix of these. Coming to your question, there is no such thing as best algorithm. I have tried and tested the following algorithms :   ExtraTreesClassifier GradientBoostingClassifier RandomForestClassifier LogisticRegression BernoulliNB GaussianNB MultinomialNB KNeighborsClassifier LinearSVC NuSVC SVC DecisionTreeClassifier Naive Bayes Maximum Entropy Model For Naive Bayes, I had used unigrams (68% accuracy) and bigrams (79% accuracy) as features. I don't exactly remember the PRF value for each classifier on my data set but I got maximum accuracy for Bernoulli Naive Bayes, Maximum Entropy Model (IIS), Linear SVM using five fold validation. I would suggest you not to go for Tree Classifiers as they are not space optimal for training data creating over-complex trees leading to over-fitting and also, learning an optimal decision tree is an NP complete problem. I will thus suggest you to try these : Naive bayes - BernoulliNB, GaussianNB, MultinomialNB Support Vector Classifiers - LinearSVC, PolynomialSVC, RbfSVC, NuSVC Maximum Entropy Model - GIS, IIS, MEGAM, TADM I am currently using a voted model between Naive Bayes and Lexicon based approach.    Embed Quote