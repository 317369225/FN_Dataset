★How are linear regression and gradient descent related?My understanding is that gradient descent is an optimization function used for linear regression. 1/ Does that mean that gradient descent is a type of linear regression? 2/ Is gradient descent similar to ordinary least squares (OLS) and generalized least squares(GLS)?
Linear regression is basically curve fitting (in this case, the curve is a line)-- You have some data points and you want to find the best line that fits the data points, that is, the line that minimizes the squared distance. A line has the equation . We have data points, say . For every our line will give us a corresponding . We want to minimize the error between our predicted and the actual . The vertical lines denote the distance between the data points and the line, which is what we want to minimize. Mathematically, for the data points, we want to minimize We need to pick a certain value of and a certain value for that will minimize the above expression. That is where gradient descent comes into the picture. Gradient descent is an algorithm that minimizes a convex function. It turns out that if we denote the above expression as a sort of a cost, then for every choice of , we will get a certain cost. We want this to be minimum. Mathematically, for every choice of and we get another value, the cost. We want to minimize the value of . If we plot the cost that we get for every value of and we get a surface that looks like this: The x-axis has all values of the parameter , the y-axis has the values of and the z-axis gives us the cost. The term convex function informally means that the local and global minima are the same. So gradient descent takes as an input some seed values of and and iteratively improves it, until we reach the minimum cost, thereby giving us the optimal parameters and , and then we can get the desired line, .    Embed Quote