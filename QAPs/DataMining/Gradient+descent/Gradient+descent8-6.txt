★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
I suppose the question is implicitly assuming: 1 stochastic gradient descent may solve the global solution when the objective is not convex. 2 why not using the original newton-raphson method? for 1: note that the plain-vanilla version of the gradient descent does assume convexity for 2: nr requires matrix inversion at each step, which is unstable and slow, despite the fact that the outer iteration converges quadratically. In this respect, gradient descent is a greedy adaptation of the basic newton-raphson idea to eliminate the inversion at the cost of sub-optimal step sizes.    Embed Quote