★Is full gradient descent and stochastic gradient descent inherently different?I know how they differ in the style of updating parameters. My question is, presumably we have a complete set of training data, which ought to be infinite. Then the actual training data can be viewed as a subset of the complete set. When I run FGD on the training data, can I say that I ran a mini-batch SGD w.r.t. the complete data? Or, is FGD a SGD with a huge mini-batch size?
The difference, like in the names, in the *stochastic* part. Having noise in the gradients (due to choosing a subset of data) changes the convergence rates. The larger the batches, the smaller the variance, but there is a real difference between "just a little" and zero. In statistical terms, if you consider the performance of the algorithm on the future data (that is unavailable to you), both algorithms generalize well as long as you use each data point just once. For SGD this might allow you some real progress (depending on how much data you have), for FGD this allows just one iteration, so not very useful.    Embed Quote