★What is the relationship between the Hessian of a matrix and Gradient Descent algorithm?
Convergence of gradient descent depends on the strong smoothness parameter, which is a bound on the maximum eigenvalue of the Hessian. Equivalently it gives you a quadratic which upper bounds the function.  When you have a full matrix specification of this quadratic, minimizing the upper bounding quadratic every time is called Newton's method, a 2nd-order method. When you just have a smoothness parameter, you do gradient descent which consists of repeatedly minimizing the upper bound and converges quicker for smoother functions because you can take bigger steps.  The best thing to do is to check out this convex optimization course by Sebastien Bubeck, specifically here: ORF523: Oracle complexity of smooth convex functions    Embed Quote