★Is the L-bfgs always better than (stochastic) gradient descent?As far as I know, they both are scalable and only require gradient, while L-bfgs benefit from newton-method.
SGD is fast especially with large data set as you do not need to make many passes over the data (unlike LBFGS, which requires 100s of psases over the data). In my personal experience, it is much simpler to implement and tend to be more numerically stable. Also, for many problems (especially in NLP), SGD is just as good if not superior to L-BFGS for models where the surfaces are pretty flat. For its ease of use and speed, it is also a good first thing to try. That said, tuning the learning rate for SGD can be difficult, though there are alternative techniques like Adagrad which helps you determine a learning rate to use.    Embed Quote