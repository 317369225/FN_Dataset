★What is the relationship between Metropolis-Hastings algorithm for Markov Chain Monte Carlo and Gradient Descent?
They are two different methods used for different purposes in different settings. Gradient descent is used for optimization in frequentist setting. For example you have a parameterized model that you assume generates a data, you can use gradient descent to find the maximum likelihood estimates of the parameters. MCMC is often used to approximate a complex probability distribution from which it is not easy/possible to sample. Suppose you want to calculate expectation of a quantity (say mean) under a distribution but do not know how to sample then you construct an MCMC. MCMC is often used in Bayesian setting to approximate posterior distribution of the parameters (remember there are no point estimates in the Bayesian setting, hence no gradient descent). Now in certain cases, you can use MCMC within gradient descent. For example, in each iteration of gradient descent, suppose parameter update required you to calculate an expectation then you can use MCMC to compute that. For e.g. in CRF (Conditional Random Field) training, each parameter update requires us to calculate expectation of the model under current parameters.    Embed Quote