★What are some fast gradient descent algorithms?Follow-up on: http://hunch.net/?p=119
One method that I believe has been applied to large-scale data sets at Google is parallel boosting with momentum (called BOOM for "BOOsting with Momentum"). The technical paper is available here: Parallel Boosting with Momentum Yoram Singer, one of the paper's co-authors, gave a technical but broadly accessible talk on it here, where he also discussed some big data experiments: BOOM is a combination of two methods: Nesterov's accelerated gradient method (due to Yurii Nesterov). This method dates back to the 1980s and allows us to speed up gradient descent-type methods using purely first-order information to achieve substantially faster initial convergence. This is covered in Singer's video, but you can get a good overview of the method here: Strongly Convex A version of gradient descent called "parallel boosting" or "parallel coordinate descent" where, instead of using the same learning rate across all coordinates, we use different learning rates in different coordinates, taking into account that some features occur more frequently than others. Singer's video offers an overview of parallel boosting. PS: In the interests of full disclosure, one of Singer's co-authors on the BOOM paper, Indraneel Mukherjee, is the CEO at LiftIgniter, the company where I currently work. BOOM is one of the algorithms that we're using at the company, and I was first introduced to it by Indraneel. My own work at the company has involved adding some checks and tweaks to BOOM, and developing related algorithms.    Embed Quote