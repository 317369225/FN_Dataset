★Is gradient descent (which is used for optimizing the cost function of any learning algorithm) a greedy approach? If yes how do we make sure that we approach the cost which is the global minimum and not the local minimum using gradient descent?
Try to formulate the cost function as a convex function. The nice part about convex function is that if you have the local minimum, that is indeed the global minimum. In short(contrary to way the answers you normally expect): Don't change the optimization method, change the cost function that approximates the actual cost. Replace the discrete loss with hinge loss or whatever you find can be formulated as convex optimization problem. It is however an art to pose crazy looking optimization problems as convex optimization formulation. See the details of SVM optimization formulation formulated as QP(Quadratic program)    Embed Quote