★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
Gradient descent is using the derivative of the function at an initial point, stepping along it, then finding the derivative again, etc. What is the "traditional way" supposed to be? I see you mean solving d/dx = 0 Unfortunately, you can't always do this directly. consider least squares: the normal formula for solving this is based on d/dx = 0, but unfortunately it grows cubically (not quadratically; whoops) in the number of variables. This is very bad sometimes. Gradient descent methods, especially stochastic variants (which take data one-at-a-time instead of all at once, treating them as samples from the real function) can be implemented linearly in the dimensionality of the data.    Embed Quote