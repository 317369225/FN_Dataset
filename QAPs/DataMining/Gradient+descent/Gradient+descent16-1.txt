★Mathematical Optimization: Is Newton's method always superior to gradient descent?
Let me add a bit to Carl's point about computing the Hessian being a pain.  In many interesting machine learning problems, the parameter vector over which you're searching is multidimensional and the dimension is in fact very large (say, billions).  If k is the size of your parameter vector, computing a gradient requires computing O(k) things, but computing a Hessian requires computing O(k^2) things, and inverting it is even more expensive.  For k=10^9, you won't even have enough memory in a cluster to hold such a large matrix!  Even if you could, or if your problem is smaller, the improvement in convergence rate you get from Newton's method may still not be worth this extra cost. There are first-order methods like L-BFGS that mostly avoid the extra storage and computation of Newton's method but often converge much faster than gradient descent for common problems and are more robust to the choice of starting point than Newton.  If gradient descent isn't working well enough, Newton's method is infeasible, and you'd like to use a single machine to solve your problem, I'd suggest trying L-BFGS.    Embed Quote