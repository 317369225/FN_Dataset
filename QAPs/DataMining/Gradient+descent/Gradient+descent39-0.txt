★What are the potential downsides of the gradient descent method?
Gradient descent is a fairly well studied algorithm, so for the most part the problems with GD have solutions (or at least patches). But as with most problems, there is not a silver bullet. Big Data- Sometimes calculating the gradient can be very expensive or intractable if the size of your data is large. As Rahul mentioned, stochastic gradient descent is a good option which theoretically will converge to a local max given the Robbins-Monroe conditions. I think batch stochastic gradient is pretty popular these days. Step Size- Knowing how far to travel along the gradient can be a big problem, especially when you have a very non-convex objective function. This can be addressed through line-search methods (see Armijo-Wolfe conditions) which pick appropriate step sizes. Poor Problem Geometry- Sometimes based off the geoemetry of your problem, gradient descent with line search will cause you to zig zag back and forth. Hessian based methods (conjugate gradient, BFGS, etc) which select step sizes based off 2nd order approximations can also help by making your "elliptical" space more "spherical". This generally results in fewer steps, but causes each step to be more expensive.    Embed Quote