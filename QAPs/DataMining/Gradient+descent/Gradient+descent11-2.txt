★Convex Optimization: Why use gradient descent when the normal equation exists?
Generally speaking, from a numerical point of view, the normal equations are not stable, and can  give numerical roundoff errors that propagate into the true solution.  this is precisely why most solutions need to be regularized additionally, gradient descent is not the easiest method to use, since it is difficult to include an L1 regularizer.  there are many alternatives, the simplest being coordinate descent (used in LibLinear and Graphlab) (S)GD is only the preferred method for online learning, when the training data is changing rapidly    Embed Quote