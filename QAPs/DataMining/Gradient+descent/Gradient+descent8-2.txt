★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
If it is a convex function, and the number of optimization parameters is large (say n) i.e. the cost function is n-variate; using the grad(f) = 0 method you would need to solve a system of n equations corresponding to each partial derivative, in order to get the solution. This may incur a much higher computational cost (especially if the system of equations is non linear) than using gradient descent, where you compute the n partial derivatives at every step and iterate till you reach the solution, without needing to solve any system of equations.    Embed Quote