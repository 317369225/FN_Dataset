★What are some tips for debugging a gradient descent algorithm that isn't converging?I'm interested in graphical and numerical methods for tuning hyperparameters, batch size, initial weights, etc.
Ben Packer gives some good high-level tips for understanding why gradient descent is not working on an underlying level. But if I have reason to believe that gradient descent will work on my problem, I would use these practical tips for debugging first: First of all, the minibatch size shouldn't matter much in general (modulo scaling the learning rate). Just use 100 for now, while debugging. It's probably the least important hyperparameter. For the initial weights, try using a Gaussian with zero mean and unit variance. If everything is diverging, then perhaps the weights are too large. You will primarily want to play with the learning rate. Try varying it by factors of 10 while debugging (or sqrt(10) once debugged). Use a fixed learning rate while debugging. When you run your gradient descent algorithm, you have the value of the criterion before you apply the update, as well as the gradient you compute. You should also calculate the value of the criterion after you apply to update. (Remove that value after you're done debugging.) Play with some different learning rates, and make a plot of the moving average of the pre-update criterion value: Is it decreasing and then converging? Good, you have a good learning rate. Is it decreasing but not converging? The learning rate is too low. It is not moving or jumping around? The learning rate is wrong or something's messed up. Read further. Is it diverging? The learning rate is much too high. If you still are confused what's going on, look at the delta between pre-update criterion and post-update criterion. The criterion is moving the direction you want for most mini-batches, right? If not, then pretend a single mini-batch is your entire data set. Optimize just this mini-batch and don't switch the data. If you still can't optimize this sole mini-batch with a variety of fixed learning-rates. If you still can't get optimization to work, even after plotting the curves and looking at the deltas for the sole mini-batch, then something is messed up, and you should refer to Benjamin Packer's answer. If that fixed-learning rate gradient descent works, you can get more tricky. Consider the learning-rate decay schedule described by Leon Bottou. http://leon.bottou.org/projects/sgd    Embed Quote