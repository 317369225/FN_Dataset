★In optimization, why is Newton's method much faster than gradient descent?Please give some more details, for example Think of it in terms of  geometry theory
You can think of Newton's method as a gradient descent that looks at second-order information to change the step size and direction. Newton's method is not always faster than gradient descent. A step requires solving a system of equations involving the Hessian. In generalized linear models, this is O(F^3) where F is the number of features. A gradient descent step, on the other hand, is O(F). Newton's method also has much larger memory requirements, so people often use quasi-Newton methods like L-BFGS.    Embed Quote