★What are some fast gradient descent algorithms?Follow-up on: http://hunch.net/?p=119
The NIPS 2010 tutorial on optimization for machine learning by Stephen Wright is an excellent overview of the state of the art:   http://videolectures.net/nips201... For instance the Nesterov and FISTA accelerated methods seem very interesting (but they are batch methods and might not scale as Stochastic approximation do on datasets with a very large number of samples). Adapting accelerated methods to an online setting might be possible though (see Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization by Lin Xiao). Edit: recent developments with stochastic gradient descent variants that benefit from accelerated rates of convergence, for instance:   SAG: Minimizing Finite Sums with the Stochastic Average Gradient   Accelerating Stochastic Gradient Descent using Predictive Variance Reduction For very wide L1 regularized linear problems (with n_features >> n_samples), carefully implemented Coordinate Descent seems to be faster than accelerated gradient methods:   http://www-stat.stanford.edu/~ti... Watch out though: fast / advanced optimization methods are not always necessary for machine learning in a large scale setting as the generalization error might make the optimization error irrelevant in practice:   http://leon.bottou.org/talks/lar...    Embed Quote