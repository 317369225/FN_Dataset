★What are some tips for debugging a gradient descent algorithm that isn't converging?I'm interested in graphical and numerical methods for tuning hyperparameters, batch size, initial weights, etc.
Erik Goldman had a few of these covered, but here are some of the main problems you might be having: Problems with your function No minimum. Make sure your function actually has a minimum, otherwise you will continue to descend forever. Not differentiable. Make sure the gradient can be calculated at all places. Some functions are differentiable only in certain regions and not in others. If this is the case, you can use a subgradient method: http://www.stanford.edu/class/ee.... Violating constraints. This doesn't relate as much to convergence problems, but if your function is constrained, it may be that your gradient is taking you to a place that violates the constraints. In that case, you can project back to the constrained space, or use several other techniques for constrained optimization: http://www.stanford.edu/class/ee... or http://www.stanford.edu/class/ee.... Problems with your gradient Incorrect gradient. Make sure your gradient is correct. One way to check it is by using finite differencing: http://en.wikipedia.org/wiki/Fin.... You can use finite differencing to compute the approximate gradient at each step, and if the difference between your analytically calculated gradient and the approximate gradient doesn't approach 0 as the epsilon you use for finite differencing approaches 0, then your gradient is probably incorrect. Problems with your procedure Step size. If your step size is too small, your function will decrease very slowly and take a long time to converge, and if your step size is too large, you may jump around the function and never converge on a local minimum. You can simply try changing your step size or you could adaptively change your step size (typically high step size at first, lowering your step size as you go). A better thing to do is probably to simply use a line search: each time you compute the gradient, find the optimal step size to take, which is the step size that minimizes your function (you can use a binary search to find it). For example, see here: http://www.stanford.edu/class/ee.... Convergence criteria. It's possible you've set your convergence criteria to be too strict, and that by setting it to be less strict, you will converge faster (by definition, if the rest of your procedure is sound). Other methods. Consider using other methods for optimizing your function, like conjugate gradient descent (http://en.wikipedia.org/wiki/Con...) or Newton's method (http://en.wikipedia.org/wiki/New...), which usually converge much faster.    Embed Quote