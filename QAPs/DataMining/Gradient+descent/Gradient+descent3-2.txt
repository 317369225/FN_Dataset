★What are the weaknesses of gradient descent?
If the learning rate for gradient descent is too fast, you are going to skip the true local minimum to optimize for time. If it is too slow, the gradient descent may never converge because it is trying really hard to exactly find a local minimum. The learning rate can affect which minimum you reach and how quickly you reach it, as shown below. A good practice is to have a changing learning rate, that slows down as your error starts to decrease. From: Tuning the learning rate in Gradient Descent (Note: You might get reach the best local minimum by chance using a fast learning rate, or reach it quickly even by using a slow rate. It depends on what the function looks like for your specific model and where you are starting).    Embed Quote