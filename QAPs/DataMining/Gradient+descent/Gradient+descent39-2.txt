★What are the potential downsides of the gradient descent method?
Plain gradient descent can be rather slow on large datasets as it requires summing over all the training examples before computing the gradient. This can be overcome with stochastic gradient descent but that usually is a quite a bit slower to converge.  Another potential downside is getting stuck in local optimum especially in nonconvex cost functions (in modern deep learning methods, the cost function is extremely non-convex). But in general, gradient descent learning is the best we can currently do in practical machine learning and it works pretty well for sure. As I mentioned, getting stuck in local optimum is perhaps the biggest downside but there are some clever methods to get past this such as momentum. Another way is to use more advanced optimization methods such as L-BFGS or the conjugate gradient  method but these are usually a lot slower on larger problems as well.    Embed Quote