★Are there deep learning methods that do not rely on gradient descent?Gradient descent requires the building blocks of a (deep) neural net to be differentiable or be able to pass the gradient through during backpropagation. Are there deep learners using digital/non-differentiable components?
So my answer to this question is Generally the objective function used are mainly 1) Squared loss (differentiable)- Autoencoders 2) Cross entropy - Contractive autoencoders 3)Exponential functions for Boltzmann Machines. The differentiable criteria is implicitly checked. So in cases of non-gradient using a threshold value and updating the hidden layers may be used due to the "magic of Deep learning". -Refer dropout approach in Deep learning    Embed Quote