★Mathematical Optimization: Is Newton's method always superior to gradient descent?
No. It depends on the problem you're trying to solve. In order for Newton's method to apply, the function you are trying to solve should be approximately quadratic in the neighborhood of the solution. This happens to be true for very large classes of functions, but certainly not all of them. For instance, functions that are are not twice differentiable at or near the solution cannot be approximated by a quadratic. The performance of Newton's method on such problems cannot be guaranteed; you may find the answer, but you may not. Also, for Newton's method to converge, you must have an estimate for the answer that falls within that local neighborhood. If your best estimate falls outside the local neighborhood of the true solution, Newton's method may or may not converge to it. Furthermore, verifying whether you do in fact have a good enough initial estimate can be very difficult. For some problems, the convergence region of Newton's method can actually be fractal. Gradient descent is almost never as fast as Newton's method - it is almost always much, much slower, in fact - but it is much more robust. It applies to a larger class of functions. It does not require that the function be twice differentiable; it only has to be once differentiable, and in many cases is so robust that even that is not a hard requirement. In addition, gradient descent typically has a much larger region of convergence than Newton's method. One very common optimization algorithm, Levenburg-Marquardt, makes use of gradient descent's robustness by conducting a line search that interpolates between the gradient descent solution and the Newton's method solution at every step. Gradient descent also requires less computation per step (although it usually takes many more steps). This is because whereas Newton's method requires both the gradient and the Hessian (the second partial) of the objective function, gradient descent requires only the gradient. Computing Hessians can be a tremendous pain in the rear. If the Hessian is very very expensive or difficult to compute, gradient descent might be preferable. This is especially true if you are only trying to find a solution to a particular problem once instead of developing a general-purpose algorithm. If you only have to solve the problem once, the time it takes for you to code up the Hessian may not be worth it - the classic programmer time versus CPU time tradeoff.    Embed Quote