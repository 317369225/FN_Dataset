★What's the difference between gradient descent and stochastic gradient descent?
To add on to Abhishek's excellent response: Gradient descent is deterministic,which means that every time you run GD for a given training set, you will get the same optimum in the same number of iterations.  Stochastic gradient descent is, well, stochastic.  Because you are no longer using your entire training set a once, and instead picking one or more examples at a time in some likely random fashion, each time you tun SGD you will obtain a different optimum and a unique cost vs. iteration history.    Embed Quote