★If you had to use batch gradient descent for the training of an artificial neural network instead of stochastic gradient descent, which is better at avoiding local minima, could you improve resilience to getting stuck in local minima by adding random noise to the error function?Alternatively how else could better local minima resilience for batch gradient descent be achieved?
Yes. This idea has been explored pretty extensively so far, and is an active area of research. I'll briefly present 3 examples. Hopefully this also helps answer your followup question. Regularization It turns out that that certain forms of random noise are equivalent to regularizing the objective. Ultimately, regularization helps avoid overfitting; in a way, this helps the ANN dodge bad local optima corresponding to regions that overfit to the training data. For example, adding an L2 regularizer to the weights of a neural network are equivalent to assuming that the weights were drawn from i.i.d. Gaussians. An L1 regularizer is equivalent to assuming that the weights were drawn from i.i.d. double exponentials. More generally, you can actually show that this is true for any estimator; for example, adding an L2 regularizer to logistic regression is equivalent to a Gaussian prior on the weights. Denoising Autoencoders Denoising autoencoders is a more direct application of your question; the idea here is that we have some data vectors for which we would like to learn a higher level representation for (each feature must lie between 0 and 1). To learn this representation, we add a single hidden layer and set the output layer to have the same dimensionality as the hidden layer. We generally use sigmoid activators and minimize the cross entropy between and . Each neuron in the hidden layer corresponds to a single learned feature in the higher level representation; ideally, at the end of the day, these will have meaningful interpretations. In practice, this doesn't do as well, because the weights can "work together" deterministically to minimize the objective. Ultimately, what we really want is a robust set of independent features that do a good job of representing the data. To achieve this, denoising autoencoders set entries of to 0 with some probability (usually around 0.1 - 0.4). By doing this, we get features that are much more robust - on the MNIST digits dataset, for example, we get pen-stroke features. Dropout Denoising autoencoders falls under a larger class of dropout-based methods introduced by Geoffrey Hinton; rather than dropping out only the inputs, we can try dropping hidden layer values to 0 as well. Concretely, if we have a hidden layer with some H units, each unit's output is set to 0 with probability . In the hidden layer case, is usually set to 0.5. For sigmoid autoencoders, it turns out that there's a nice proof where training a neural network with dropout is equivalent to training all neural networks whose hidden layers are a subset of the H units and averaging the outputs of those networks. This is quite remarkable, since there are effectively neural networks that are concurrently being trained (albeit with shared weights). We would expect such an idea to work well in practice given that model averaging has been shown to work very well with neural networks. In practice, dropout works well, and adds 1-2% or more improvement to classification. P.S. Probably worth mentioning on the off chance you were wondering -- it's usually not a good idea to run batch gradient descent with neural networks unless (1) you're debugging or (2) are working with tiny datasets since batch gradient descent tends to be very slow, and stochastic gradient descent (SGD) with moderately large batches tends to yield decent updates anyways. If SGD doesn't yield convergence, you may want to try computing the angle between the batch gradient and SGD gradient and see whether that's close to 0. As long as the angle is consistently under 90, SGD should be okay to use.    Embed Quote