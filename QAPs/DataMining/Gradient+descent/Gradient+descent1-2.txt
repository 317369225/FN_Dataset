★What's the difference between gradient descent and stochastic gradient descent?
In the Gradient Descent method, one computes the direction that decreases the objective function the most in the case of minimization problems. But sometimes this cant be quite costly. In most Machine Learning for example, the objective function is often the cumulative sum of the error over the training examples. But the size of the training examples set might be very large and hence computing the actual gradient would be computationally expensive. In Stochastic Gradient (Descent) method, we compute an estimate or approximation to this direction. The most simple way is to just look at one training example (or subset of training examples) and compute the direction to move only on this approximation. It is called as Stochastic because the approximate direction that is computed at every step can be thought of a random variable of a stochastic process. This is mainly used in showing the convergence of this algorithm. There might be many reason but one reason as to why SG is preferred in Machine Learning is because it helps the algorithm to skip some local minima. Though this is not a theoretically sound reason in my opinion, the optimal points that are computed using SG are empirically better than the GD method often.    Embed Quote