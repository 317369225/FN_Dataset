★Why does stochastic gradient descent have a tendency to overfit the data?I've read that stochastic gradient descent overfits the data because the earlier examples hold more "sway" than later ones. Why?
There are many wrong ways to use SGD, leading to different undesired results, the right way is problem specific. You seem to be describing a possible outcome of a scenario with non-strictly convex losses and fast shrinking ( t^-1) step size... There is still convergence to the optimum for a convex problem but potentially exponentially slow, so it looks like over fitting almost forever. In such cases use slower shrinking step size like t^-1/2. Overall, SGD is amazing, but you need to read the manual (and it got lost in shipping, sorry).    Embed Quote