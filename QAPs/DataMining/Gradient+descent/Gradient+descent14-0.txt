★Why does mean normalization help in gradient descent?
Geoffrey Hinton gave a good answer to this in lecture 6-2 of his Neural Networks class on Coursera. This answer will be mainly directed at how input scaling affects a neural net or logistic regression model. Essentially, scaling the inputs (through mean normalization, or z-score) gives the error surface a more spherical shape, where it would otherwise be a very high curvature ellipse. Since gradient descent is curvature-ignorant, having an error surface with high curvature will mean that we take many steps which aren't necessarily in the optimal direction. When we scale the inputs, we reduce the curvature, which makes methods that ignore curvature (like gradient descent) work much better. When the error surface is circular (spherical), the gradient points right at the minimum, so learning is easy.    Embed Quote