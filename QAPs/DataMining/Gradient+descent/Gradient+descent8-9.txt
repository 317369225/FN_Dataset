★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
For what I have learnt when I was doing my masters project. The following three factors interact to determine how difficult it is to find an optimal solution to any optimization problem and why traditional methods are just not enough. 1. The relationships between the objective and the constraints with respect to the optimization variables:  If an optimization problem consists of mostly linear relationships and even a few non-linear relationships, then it must be solved using sophisticated non-linear optimization methods. 2. The size of the optimization problem: An optimization problem becomes hard to solve as the number of variables and constraints increases. 3. The use of integer variables: If an optimization problem deals with integer variables  memory requirement and solver time grow exponentially with respect to the number of primal variables. P.S. I once tried to solve a convex optimization problem, believe me everything in this world is not THAT simple, I didn't even think of gradient descent.    Embed Quote