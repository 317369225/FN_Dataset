★What is an intuitive explanation of stochastic gradient descent?
Lets say you are about to start a business that sells t-shirts, but you are unsure what are the best measures for a medium sized one for males. Luckily you have gathered a group of men that have all stated they tend to buy medium sized t-shirts. Now you figure you're going to use a gradient descent type method t get the size just right. Batch Gradient Descent Tailor makes initial estimate. Each person in the batch gets to try the t-shirt and write down feedback. Collect and summarize all feedback. If the feedback suggests a change, let the tailor adjust the t-shirt and go to 2.Stochastic Gradient Descent Tailor makes initial estimate. A random guy (or a subset of the full group) tries the t-shirt and gives feedback. Make a small adjustment according to feedback. While you still have time for this, go to 2. Highlighting the differences Batch gradient descent needs to collect lots of feedback before making adjustments, but needs to do fewer adjustments. Stochastic gradient descent makes many small adjustments, but spends less time collecting feedback in between. Batch gradient descent preferable if the full population is small, stochastic gradient descent preferable if the full population is very large. Batch gradient descent methods can be made parallel if you have access to more hardware (in this case, more tailors and materials) as you can collect all feedback in parallel. Stochastic gradient descent does not readily lend itself to parallelization as the you need the feedback from one iteration to proceed with the next iteration.    Embed Quote