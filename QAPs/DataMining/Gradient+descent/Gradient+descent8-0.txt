★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
I think there are two parts to the question: Why doesn't one simply use to find the minimum of convex functions? I would say that on the contrary, setting derivatives to zero is actually a frequently used technique for finding the minimum of convex functions in unconstrained problems. Examples abound in the field of statistics and control. Consider the the Ordinary Least Squares (OLS) formula: The above is actually the derived by setting in the following least-squares minimization problem. Similar implicit minimization problems can be found in derivations for the Kalman filter, PCA, etc. This technique provides closed-form solutions to optimization problem. This means you get optimization solutions with bounded solution times, which is a very attractive property especially in real-time applications like control. With respect to matrix inversions and such, typically the matrix inverse is never calculated. In the above OLS case, the system is posed in this form: and solved using a linear solver. The computational effort of solving a linear system is lower than computing the inverse, plus one can more easily exploit special structure in the coefficient matrix like bandedness, symmetry, sparsity etc. and use techniques like pre-conditioning, Tikhonov regularization, etc. However, setting derivatives to 0 is only useful when the system happens to be a linear one (or at least, an explicit system of equations in which can be isolated). Otherwise, one may have to solve a system of nonlinear equations, which entails the use of some gradient-descent or Newton type method. The bottom-line: when your first-order derivative system is a linear system of equations, solving this system directly is typically much more computationally attractive than using gradient-descent (whose convergence can be slow). When your first-order derivative system is not a linear system, then alternative strategies (including, but not limited to, gradient descent) may be more attractive. Why do people use gradient descent for convex functions? I think the most compelling reason for using gradient descent is that it is extremely easy to implement. It is also relatively cheap computationally, which is a big factor when solving optimization problems with very large datasets (and when first-order derivatives are available cheaply). And as mentioned above, it can be useful when setting the derivatives to 0 does not result in a linear system that can be solved easily. As far as optimization algorithms go, gradient descent is at best a passable one (the fact is, it has a relatively poor convergence rate, and can potentially zigzag near the optimum, although line searches and other techniques can help alleviate such problems). However, in many big data applications, the bottleneck tends to be the problem size rather than the algorithm used for optimization, so when viewed in this light, gradient descent can be a decent option (especially stochastic gradient descent).    Embed Quote