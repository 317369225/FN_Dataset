★Convex Optimization: What's the advantage of alternating direction method of multipliers (ADMM), and what's the use case for this type of method compared against classic gradient descent or conjugate gradient descent method?
Advantages 1. ADMM is more general than other methods in the sense that the loss function doesn't need to be differentiable. Hence, it just works out of the box for many problems. For example, traditional methods (such as GD, SGD, or Newton) just don't work for optimization with regularization, i.e. , while ADMM can handle it easily. [*][**] 2. Simple to implement 3. Simple to parallelize. Parallelizing L-BFGS, for example, is less trivial. Parallelizing SGD is even trickier (if not impossible, without compromise). To be fair, we also need to list its disadvantages because they are important. Disadvantages (confirmed by Boyd in one of his talks and also in his seminal paper) 1. Its convergence rate is poor. Here's an excerpt from Boyd's paper. Simple examples show that ADMM can be very slow to converge to high accuracy. However, it is often the case that ADMM converges to modest accuracy—sufficient for many applications—within a few tens of iterations. This behavior makes ADMM similar to algorithms like the conjugate gradient method, for example, in that a few tens of iterations will often produce acceptable results of practical use. However, the slow convergence of ADMM also distinguishes it from algorithms such as Newton’s method (or, for constrained problems, interior-point methods), where high accuracy can be attained in a reasonable amount of time.For almost every problem, there is a method that is superior to ADMM. Again, from the authors themselves I believe that the convergence rate of ADMM is still not well understood although some guys showed that ADMM has linear convergence rate. My empirical prototypical experience is that its convergence rate is sub-linear. Chih-Jen Lin (author of LibLinear and LibSVM) had a talk at ICML 2014 and his empirical experiment (slide 23/37) showed similar sub-linear convergence rate of ADMM. 2. Another excerpt We emphasize that for any particular problem, it is likely that another method will perform better than ADMM, or that some variation on ADMM will substantially improve performance. Notes [*] Many people claim that the main advantage of ADMM is that it can handle (equality) constrained optimization, which I think is misleading. Equality constraints can easily be relaxed using the Lagrange multipliers method. [**] Even though ADMM is not the best method for the optimization.    Embed Quote