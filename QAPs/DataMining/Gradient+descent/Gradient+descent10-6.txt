★What are some tips for debugging a gradient descent algorithm that isn't converging?I'm interested in graphical and numerical methods for tuning hyperparameters, batch size, initial weights, etc.
It is unclear whether your gradient descent is stochastic or batch gradient descent. If you're using stochastic gradient descent (SGD) or any of its variants, then it is possible that you are converging too early due to quick decay in your learning rate (or, you're constantly overshooting the minima due to a large learning rate). If so, the most obvious solution is to obtain the best learning rate and decay rate using cross-validation. If you're using batch gradient descent, then you don't need to decay the learning rate. You could compute the gradients with respect to the entire dataset and perform one update at a time. In practice your dataset might be too large and it may be infeasible to perform batch gradient descent. In such a case, one useful experiment could be to sample a small sized dataset from your original dataset and perform batch-updates in order to determine a good guess for your learning rate and decay parameters. These would be helpful in your final parameter tuning experiment for SGD. Note that the sampling process must be done in a stratified way, in order for your sampled dataset to be truly representative of your actual dataset.    Embed Quote