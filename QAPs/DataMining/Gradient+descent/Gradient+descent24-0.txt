★Are there deep learning methods that do not rely on gradient descent?Gradient descent requires the building blocks of a (deep) neural net to be differentiable or be able to pass the gradient through during backpropagation. Are there deep learners using digital/non-differentiable components?
Many layers used in convolutional NN are non differentiable: for example max-pooling, or ReLU. But one can easily pass gradients throgh these layers during backpropagation. For example if the value of input to ReLU (rectified linear unit) is positive, then ReLU acts as identity function around this point, so it's derivative would be 1, and it passes gradient from upper layer to layer below without any change. If the input to ReLU was negative then derivative of layer around his point will be 0, so gradient passed to layer below will be 0. What  to do In the case of input equal to 0? In this case the derivative on the left to 0 is "0", and to the right "1". So sub-gradient spans the whole interval [0,1], and any value from this rage will be ok, for example mutiply gradient from top layer by 1/2 or 1.    Embed Quote