★Convex Optimization: What's the advantage of alternating direction method of multipliers (ADMM), and what's the use case for this type of method compared against classic gradient descent or conjugate gradient descent method?
To answer this question, I'm going to work with the following convex optimization problem: where is a (real) convex objective function and is a variable with dimension and we'll encode all the constraints on the variable into the domain of the objective function . A convex optimization problem is unconstrained when is finite over all ; otherwise, it is constrained in some way over a convex set (by definition) which is a subset of . Now, the answer: in general, gradient descent / conjugate gradient descent can only be used for differentiable and unconstrained optimization problems. ADMM can be used for any convex problem. I'll dive into a little more detail here. You can feel free to ask for clarification or more references, so I'm going to go fast. You can modify the gradient descent algorithms to handle constraints, etc. These are the projected gradient descent algorithms. Furthermore, if some more technical conditions on the objective hold (i.e., smoothness and strong convexity), gradient descent and its variants (projected gradient descent, proximal gradient descent, etc.) have worst-case convergence bounds that beat ADMM. (To the best of my knowledge, convergence behavior for ADMM is not as well-understood.) If your problem is in this regime, I'd suggest looking at TFOCS: Templates for First-Order Conic Solvers. You can, in some very special cases, get linear convergence. I have been told by some friends who work with truly large-scale datasets that these first-order methods can be made asynchronous and outperform ADMM on problems that they are interested in. However, what ADMM does get you is the ability to perform distributed optimization without any apology (i.e., it will work for any convex problem). It lets you break up a larger problem into smaller, more manageable chunks. The smaller chunks can be solved using your favorite solver (including gradient descent). A good example of this is in energy, Dynamic Network Energy Management via Proximal Message Passing. ADMM is used to coordinate many smaller optimization routines to cooperatively solve a larger problem and this often splits along nice problem boundaries. Feel free to comment or ask for clarification.    Embed Quote