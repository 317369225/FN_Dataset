★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
If you're interested in learning about gradient descent, here are 3 references I've personally found useful: Boyd's Convex Optimization is famous, readable, and free. Nesterov's Introductory Lectures on Convex Optimization is written in a more terse style, but it assumes more math background. Sebastian Bubeck's blog, which has notes from his course on optimization at Princeton, ORF523.If you read these things, you'll find proofs about lower bounds on optimization rates when only first-order derivatives are available.  For certain classes of convex functions, you can prove that you have to look at a certain number of derivative values to get to a solution that is within [math]\epsilon[\math] of the optimum function value.  And there are often gradient descent algorithms that achieve these lower bounds - they are in some sense unimprovable for that problem class.  So gradient descent algorithms aren't necessarily slower than other methods.    Embed Quote