★Mathematical Optimization: Why would someone use gradient descent for a convex function?Why wouldn't they just find the derivative of this function, and look for the minimum in the traditional way?
I'm no expert by any stretch, in fact probably a novice at best, so please correct me if im wrong, but I thought the whole point of machine learning is that since you don't know what the nature of the function you are trying to optimise is/don't know the structure within the data, you therefore use gradient decent as a method of finding the optimum within the data without knowing the function. Using prior information like 'the derivative of a convex function' is assuming you know something about what you are trying to optimise before optimising it.    Embed Quote