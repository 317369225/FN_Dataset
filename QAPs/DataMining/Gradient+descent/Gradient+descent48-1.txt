★If you had to use batch gradient descent for the training of an artificial neural network instead of stochastic gradient descent, which is better at avoiding local minima, could you improve resilience to getting stuck in local minima by adding random noise to the error function?Alternatively how else could better local minima resilience for batch gradient descent be achieved?
Backpropagation with momentum..Ensures better convergence and avoids local minima..For each weight change..Add to current weight delta a fraction of previous weight delta. New weight = current weight - ( weight delta + m * previous weight delta) where m ranges btwn 0 and 1.    Embed Quote