★Why does a matrix's spectrum affect its rate of convergence under Conjugate Gradient Descent?
I'm no expert on gradient descent techniques; Shewchuk's paper [1] provides a pretty good explanation on the subject. However I'll try to give a brief summary of what I learnt through the paper and hopefully that'll be helpful. Typically conjugate gradients (CG) is used to solve for s.t. . If is symmetric and positive semidefinite, then this is equivalent to minimizing over the cost The idea in CG is to obtain a sequence which converges to by generating a set of -orthogonal search directions for . It is done in such a way that the residual is not contained in . Furthermore, we seek to minimize in each iteration along the line . By doing this, we only need ever to search over each orthogonal direction once, so the solution to takes at most iterations. However CG can converge significantly faster if the matrix is well conditioned (similar eigenvalues). Consider the best case scenario where . In this case, all the level sets of are spherical, and the residual always points towards the minimum; thus in the best case CG can find the exact solution in 1 iteration. In a more general setting, the condition number of the matrix indicates how well the residual at each iteration will point towards the minimum, and thus how fast your will decrease - though typically exact convergence only occurs after   iterations. Note that since we have assumed positive semidefinite we have , with equality being the best case. [1] http://www.cs.berkeley.edu/~jrs/...    Embed Quote