★What kind of collaboration tools would reduce duplication of R&D effort in data analysis and sharing?
I've thought about this quite a bit, in fact several of my jobs have tried to address this in some way, sometimes adding to the infrastructure you describe. [1]  Some other examples: The earth science data management community has been spending the last few decades trying to describe, and sometimes build, the optimal data management infrastructure. These days the National Science Foundation's EarthCube program is pursuing a similar goal, using a strong social engagement strategy (getting all the science and data people in one room, so to speak). So my first thought was, If all the data people could gather in one room, they would go on to build most of the fragmentary tools you are complaining about. (At first, anyway.) But since that question has its own page (If all the "data people" could gather in one room, what kind of tool would they build?), let's consider your entire post. You've introduced three things that actually don't overlap much: Good data processing tools and frameworks. Optimal tool capabilities to enable scientific collaboration Techniques to reduce inefficiencies in R&D (1) The reason there are so many data processing tools and frameworks is that the target is endlessly complex. There are many different scientific and data management domains (your list for starters, then add '<science domain> informatics'); there are many different technology environments (cloud variants, network types, desktop OSs, development environments) and core technologies (languages, standards,  protocols); and there are many different goals (do research, make money, become famous, solve the world's problems, reduce duplication of R&D effort). So of course there are countless solutions, it's a competition within and across all of those categories. (2) Other answers here address scientific collaboration pretty well. The ideal collaboration environment may not exist, but it's interesting to apply GitHub, Google Docs, Wikipedia, iPython, Etherpad, and other technologies to this challenge, learning along the way and looking for even better tools. Progress will march on in this field—collaboration is a lot easier now than 5 years ago! (3) Since part of acquiring scientific/engineering research credibility is creating something new that gets traction, it's a safe bet that wheels will continue to be reinvented. And given the competition in new ideas, there will inevitably be quite a bit of inefficiency. In the usual cycle of invention and consolidation, I claim there are two things that help minimize 'wasteful' inefficiency. If a solution comes out that grabs strong leadership position (think iPod/iTunes in the music player industry, which is now the cell phone/mini-computer industry), that normalizes a lot of the playing field to consolidate around the leading solution. May not be the best solution, but people/companies need strong will to challenge it. Lacking dominant solutions, then widely used services that effectively consolidates solutions, compares them, and provides guidance should improve selection efficiency. If a problem space has an effective solution, or even just many solutions, a researcher is unlikely to propose another unless her or she thinks it is much more effective than what is there.My frustration was the same as yours, and still is to some degree. Still, since not-invented-here thinking is both human nature, and contributes to the improvement of solutions, I'm not expecting radical changes any time soon. John [1] I helped a large team create a web site for consolidating marine science data management best practices (Marine Metadata Interoperability project), and wrote the initial Science Concept of Operations for an NSF ocean observatory cyberinfrastructure (Cyberinfrastructure | OOI). Among other things.    Embed Quote 