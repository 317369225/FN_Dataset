What are some common fallacies or mistakes made by beginners in Statistics/Machine Learning/Data Analysis or they are prone to make?
There are a number of very common problems which affect probably the majority of published scientific research. For example: Statistical power. Many researchers never estimate the power of their studies, and consequently use very small sample sizes to falsely conclude that there is no difference between study groups. In many cases, published medical trials do not have the power to detect a 50% difference in outcome between groups. Base rate fallacy. If you're screening for a rare event, there are many more opportunities for false positives than false negatives. This means that most of your positive results will be false positives. Besides the obvious implications in medical screening, this also affects things like surveys asking Americans if they've used a gun in self-defense: because very few people have, the small false positive rate can be as large as the true positive rate. Stopping rules. It's common to increase the sample size of your study until you achieve a significant result or run out of money. However, this vastly increases the chance of a false positive. Truth inflation. Underpowered studies combined with publication bias means that only the studies reporting an excessively large effect size -- obtained due to luck -- will be published, while studies measuring the true effect size will not reach statistical significance.There are quite a few more common problems, so I will self-interestedly note that I have written a long guide to these issues which explains them (and others) in great depth. Or at least I hope it's great depth: Statistics Done Wrong    Embed Quote 