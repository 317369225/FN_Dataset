★What is the future of data analysis?
A while ago I wrote about this at http://www.stat.columbia.edu/~co... My conclusions were that we need progress in: Internet databases with variable and data set ontologies intended to manage background knowledge and related data sets, where the same variable appears and the same phenomenon appear in multiple tables, allowing priors to be based on more than a single data set. Research should be presented as raw data  in a standardized form, not as reports and aggregates that prevent  others from building on top of the finished work. Scalable visualization & modeling environments that make it easier to clean and transform data, experiment with models, to present insights, to reduce the amount of time needed to turn data into a model that can be communicated. (see R Project, Processing, Gapminder) Reliable statistical Bayesian model compilers that allow a statistical model to be specified in a higher level declarative model language and work with large databases (see BUGS). This is the main reason Bayesian methods are not as widely used as they would be otherwise. Doing machine learning with a programming language feels like coding in assembly, step by step - when in reality models stand on their own, and the computer can figure out how to fit them well. Interpretable modeling is important to bring formal models closer to human intuition. It is still not clear what is the importance of a predictor for the outcome - the regression coefficient is close, but yet often confusing. With more powerful modeling frameworks, it is going to be possible to focus on this - not being worried about what one can fit, but instead with model choice, model selection, model language, visual language. Support for new types of data and new types of prediction problems.    Embed Quote 