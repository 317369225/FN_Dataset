Why is the assumption of independence so important for statistical analysis?
Independence is important in statistics for three reasons: Independence often holds, at least approximately, for data we want to analyze. When it holds, you can usually carry out some analysis. When it doesn't hold, you're usually screwed. Independence often holds for data we want to analyze because it is built into common sampling methods. When market researchers use random digit dialing to call telephone survey respondents, or when patients are randomly assigned to treatment or control in a clinical trial, or when a web site assigns a user to an arm of an A/B test, experimental units are independent by design. In other cases, there are plausible physical reasons to assume independence, e.g. errors in multiple weighings of an object on a scale. In statistics, we are typically concerned in one way or another with the probability of observing some set of observed events . When these events are independent, the probability that they all happen has a very simple form: All we need is a model for the probability of each event. This can be a manageable calculation. It's especially easy if all these probabilities are the same, which is often the case. In the absence of independence, we need be able to calculate the fully general which requires a model for all the complex interrelationships among the events. Except in rare circumstances, we won't have such a model, and if we make one up it will be wildly wrong. Simplifying this completely general joint distribution into something workable is usually possible only by factoring it into components that are conditionally independent. But in the complete absence of independence, you are usually screwed.    Embed Quote 