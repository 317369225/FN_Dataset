Why is the assumption of independence so important for statistical analysis?
The value of many statistical results rests on consistency results. Consistency [1] effectively tells you when an estimate of some quantity using observations is guaranteed to approach the true value of the quantity. For example, we know that the sample mean converges to the population mean as the number of samples approaches infinity. It is easier to guarantee consistency for independent observations than for dependent ones. Whether we are trying to estimate some unknown quantity or use test statistics to reject a null hypothesis; such calculations typically involve characterizing the behavior of the underlying randomness of the problem. We are rarely interested in understanding the behavior of a single random variable (e.g. the result of the tossing of a coin). Most results in statistical inference rely on characterizing the average/expected behavior of sums of random variables. For example, we toss a coin n times and estimate the probability of heads by taking the proportion of heads. In this case we are dealing with sums of bernoulli random variables. Characterizing the distributions of sums of random variables usually rest on two key results (1) law of large numbers (L.L.N) and (2) central limit theorems (C.L.T) Independence of random variables (standard i.i.d assumptions) are usually essential to obtaining such results. In a few cases we still have LLN and CLT theorems that relax either the independence assumption or the identical assumption. However these are generally much weaker, are much more obscure and are not as general and widely applicable as the theorems that use i.i.d assumptions. Typically, in the absence of i.i.d, central limit theorems that show asymptotic normality or the Gaussianness of sums of independent identical random variables will often not hold. And we really want limit theorems to hold. To quote Kolmogorov: " The epistemological value of probability theory is revealed only by limit theorems" and limit theorems are much easier to obtain when we have independent observations. In simple terms, the independence assumption when true, helps cancel out variations, which helps you guarantee consistency of a method and lets you converge to your true result faster or with fewer samples. [1] http://en.wikipedia.org/wiki/Con...    Embed Quote Updated 15 Mar. 1,407 views.