Are there any modeling techniques for sentence meaning that abandon the bag of words assumption?
I deal with this problem all the time in my consulting work so I am very curious how other people treat this. First, I think its important to define what "sentence meaning" means (and how much of a bag of words (BOW) approach we can use under the hood) The standard BOW approach to meaning is Latent Semantic Analysis (LSA). So I will use that as the baseline Lee Becker mentions solutions to the problem of Entailment http://en.wikipedia.org/wiki/Ent... http://en.wikipedia.org/wiki/Ent... For example, does one sentence being true imply another is true. An interesting problem for sure... If that's the problem you are solving, ok.  Sometimes we just need to know "how similar" 2 sentences are "in meaning"...whatever that means... This can be done using a Mechanical Turk (Amazon, Crowdflower, etc) If we ask two people this question, we actually need to frame it as a relative question "given a sentence (A) , which sentence is closer in meaning to it (B or C)?"    Then we collect statistics over many, many examples and can build a statistical model. Also, one might  expect 2 sentences to mean the same thing if , say, you you searched from them in Google you would get  similar results.  Of course, Google is like a large scale bag of words approach -- unless you search for specific phrases. Here I mean that a bag of words (BOW) model typically even ignores bigrams and trigrams.  And this I think would be similar to Alyona Medelyan's mention of the  Wikipedia-miner: 2 sentences mean the same thing if they give similar results from Wikipedia-miner I don't know Wikipedia-miner but naively matching to Wikipedia pages is hard because many entities have a Disambiguaton Pages. (it would be a trivial problem to match phrases to Wikipedia pages otherwise) http://en.wikipedia.org/wiki/Wik... This problem is related to Word Sense Disambiguation (WSD).  Here we simply want to pick the meaning of a word in a sentence when it could have multiple senses.  That is, we might  "put our money in the bank" or we might "bank on a solution being correct"  or we might "go down to the river bank"  So here the problem is to pick the correct definition of a word from a dictionary Likewise, for mining the Wikipedia, the problem is to identify the correct Wikipage when one has (Does Wikipedia-miner address this problem?)  Sometimes this can be achieved using by detecting the difference between nouns, verbs, etc -- part of speech (POS)  tagging.  But not with great accuracy,  In fact, I think you can do just as good if you simply always pick the most common , and NOW techniques shine here if you have the kind of data Google has. (There is a class Google talk on this...I forget the reference right now) Also, notice that POS tagging is not so simple...it also has problems resolving ambiguities. Consider the classic example "Time flies like an arrow"  "Fruit flies like a banana" (from Pinker's book the Language Instinct) http://www.amazon.com/Language-I... In the first flies is a verb, in the second flies is a noun. Usually even POS taggers (like the Stanford POS tagger) need to use some context and statistics to achieve good results A common (but perhaps not so great) solution is to apply a probabilistic approach--just report all parse trees and their probabilities / confidence. See work by  Charniak in particular http://bllip.cs.brown.edu/resour... There is also an attempt to understand the similarity of sentences by looking at the similarity of the individual words.  Ted Pedersen has a perl package which uses a variety of techniques for this http://www.d.umn.edu/~tpederse/ (he has also done some interesting work on WSD)    Embed Quote 