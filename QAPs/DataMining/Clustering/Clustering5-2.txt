What are the most practical ways to select the "right" number of clusters?
There are some clustering algorithms that do not depend on the hard number of clusters. See DBSCAN, affinity propagation, hierarchical clustering, or nonparametric bayesian approaches. Spectral algorithms often provide a good measure (eigengap) for deciding the number of clusters easily in a single shot. Note: Some of these would be really slow on large datasets. But instead of putting effort on overengineering and coming up with a "one-for-all" solution, think clearly about what the meaning of "clustering" is for your task. After all, clustering is a task of separating data points in a way that you think they should be separated. It is you who defines clusters and puts meaning on them, not the data themselves. Think about best prior knowledge you can have on your datasets. For example, if you have a good guess on the "distance" (or dissimilarity) between intra-cluster data points and between inter-cluster points, a very simple incremental algorithm which creates/merges clusters based incoming distances can work very well. Then do some tweak with cross-validation.    Embed Quote 