★In Big Data ETL, how many records are an acceptable loss?It is often impossible to properly parse and modify 100% of records when you are dealing with large numbers of records in a distributed system. How many records can you afford to lose? How do you determine this amount? How do you make sure you aren't missing 'vital clues'?
I would think it depends. Its a function of what you need the data for. Even for financial data I suspect internal consistency is most important. Predictable data loss is expected even in accounting (hence reserves for returns / breakage etc.) <responding to Tom . I agree that I misused the concepts  a little bit when I used these examples - I was trying to point out that even financial data/reports are imprecise (and hence use reserves to compensate) since the sources are dirty or slow moving. These ideas predate our modern notion of ETL but IMHO do fit into the ETL stream. There is no 'fixed' notion of what the cash reserves or revenue of a company are - just a (hopefully very good) approximation_.> In general I would think that you only care about record loss during ETL if it meaningfully impacts the answer to some question on the transformed data. At a high level I would suspect you could probably lose even 1% of the rows and be okay if the lost records are truly randomly distributed across any dimensions you intend to query on. OTOH if the distribution isnt random then even a small loss  could skew your results badly. Thats why it might be interesting to keep the bad records around so that you can look back later to cross check. Another way of thinking about this might be in terms of sampling for query answering - The cases where a sample (in terms of either sample size or sampling strategy) could be bad are likely the same cases where ETL data loss causes grief..    Embed Quote