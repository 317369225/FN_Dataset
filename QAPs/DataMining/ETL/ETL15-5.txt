★In Big Data ETL, how many records are an acceptable loss?It is often impossible to properly parse and modify 100% of records when you are dealing with large numbers of records in a distributed system. How many records can you afford to lose? How do you determine this amount? How do you make sure you aren't missing 'vital clues'?
When I worked for Government we loaded many data sets from schools, colleges and hospitals.  The data was seldom as clean as expected and the overriding methodology was to insist on 1% error but try for less. In practice we had to accept that some sources where incapable of delivering perfect data, even when their funding depended on it. Even though 1% was considered an acceptable error, we would try hard to reduce this closer to zero.  In the end it becomes a time management issue. Keep in mind that with a large number of small transactions, 1% error can be caused by rounding error alone.  As long as each reportable dimension shows <1% error the data will be usable. Of course this is for reporting/warehousing  only.  Accepting data loss on transactional systems is madness.    Embed Quote