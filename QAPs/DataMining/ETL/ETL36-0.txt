★What are the best ETL options to process huge data with lots of transformation logic applied on it in a data warehouse environment?Huge flat text files of 30-40 GB in size and 500+ millions of records. The data needs to be maintained at granular level and transformed so much to support our analysts queries. The data is getting loaded into our DW which is on sql server 2008 R2.
I have a couple of answers depending on some assumptions I will make about the data environment.  It also isn't clear what the exact pain points are as it sounds like the data is being loaded into SQL server already (assuming its performance related). Do an "ELT" into Sql Server and then write SQL to transform data Assumptions: Your data isn't growing very much every day (less than 1% of total data) The flat files are in a pretty structured format, such as CSV You can do incremental loads and your data is easily partitioned Your metrics don't require reading all historical data to calculate. You have a pretty beefy dedicated server (like 96 GB RAM / 24 cores) for your database and its dedicated to pretty much just this analysis process ( no other major loads or uses) There is a way to use a good data management strategy by purging or aggregating stale/old data. The data lends itself well to "navigating".  That is, you can start at the aggregate then "drill" into the exact pieces of granular data you're interested in.Considerations: You have a good SQL Server DBA. Your developers understand how to tune queries, build data models that scale and write DDL to handle data volumes. Your data transformations can be handled with standard SQL functions. Setup a Hadoop environment and use Hive or Pig Assumptions: The 30-40 GB / 500 million rows of data is how much it is growing on a daily/weekly basis. The flat files are less structured (ie web logs).  Something like CSV is fine too assuming data volumes from above. There is value in maintaining the historical data in granular form. Ideally your source system can write directly to HDFS.  If not, batch loading the files is ok too but will increase your data latency. You can do all the data transformation (aggregation, sorting, filtering) in Hadoop and then load the data into SQL server.  That is, you can do your meaningful data analysis on the results of the processing.  While you *can* connect reporting tools to Hadoop via Hive, most people regret trying this approach. For those who need to do ad-hoc analysis or data exploration at the granular level, they're comfortable working in Linux environments. Your long term analytics strategy also includes things like advanced predictive analytics and machine learning.Considerations: If you need to extend the functionality of Hive, Pig or leverage map-reduce directly, you need some pretty good Java programmers. If you plan to maintain the Hadoop environment yourself, you'll need some good Linux admins who are trained/can be trained to maintain Hadoop. You'll want at least 5 servers to setup a production Hadoop environment, and I would argue anything less than 10 servers is not even worth pursuing. I would look to use a Hadoop vendor and pay for the support especially if your organization is brand new to Hadoop and distributing computing. Seriously, there's like 100,000 options/switches/levers/buttons and getting them right is a pain without some help.  And the way data works in a distributed environment is very different from single node environments.  Just ask anyone who has had to deal with skewed data on reducer keys. I would do a cost-benefit of setting up a Hadoop cluster.  While the software is free, there are other costs to consider such as operational, maintenance, and employee cost of Hadoop developers (Java & Linux experts) who currently demand large salaries.  The good news here is that scaling data volumes is relatively cheap. Things I would avoid It sounds like you only have one source and one target system with no other systems that need to be integrated.  Assuming this, I would avoid traditional ETL tools like Informatica or Ab Initio.  There isn't much value gained by loading data into another platform to transform it when there isn't much integration across multiple systems and source types. Switching platforms to another single node database platform. A couple other thoughts If your data volumes are in the 30-40 GB / day and the data is fairly well structured (CSV/TSV, XML, JSON) but don't have the engineering talent in house to setup and maintain a Hadoop environment (not at all uncommon), an MPP might be the better route (Exadata, Teradata, Netezza, Greenplum, Vertica, Exasol, ParAccel, Hana, SybaseIQ, and probably others that I'm not currently remembering).  Note that these aren't cheap solutions so you should do a thorough cost-benefits analysis first. If most of your files contain useless data but are still quite large on a daily basis (that is, your 30-40 GB daily file turned into less than 1GB/day once all the junk is weeded out), just setting up some shell scripts on a decent server might be "good enough" to get the data into a manageable size on SQL Server. A cloud based platform like Google Big Query is also a decent way to go assuming your flat files are relatively well structured.  The nice thing about cloud based tools is that they scale without you doing anything and many reporting tools can connect directly to them.  The downside is that getting data out of these platforms can be difficult and (relatively) expensive. I would make sure you have a short term (1-2 years) and long term (3-5+ years) analytics strategy and vision in place.  This would probably help make your technology decisions easier. I would also ensure that you have strong data governance established.  Scaling up without this will be a huge headache. As with considering any solution, I strongly suggest performing a proof of concept on several options.    Embed Quote