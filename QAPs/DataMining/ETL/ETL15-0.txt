★In Big Data ETL, how many records are an acceptable loss?It is often impossible to properly parse and modify 100% of records when you are dealing with large numbers of records in a distributed system. How many records can you afford to lose? How do you determine this amount? How do you make sure you aren't missing 'vital clues'?
In the end it depends on the impact.  As Anon User puts it, if there is a meaningful impact to the answers you're trying to obtain, then loss is not acceptable . However, I can think of many cases where 1-2% of your records could be lost, assuming random distribution, and you'd be fine.  On the flip side, if you're dealing with specific transactional information (e.g. in the finserv industry), then your loss better --> 0    Embed Quote