★In Big Data ETL, how many records are an acceptable loss?It is often impossible to properly parse and modify 100% of records when you are dealing with large numbers of records in a distributed system. How many records can you afford to lose? How do you determine this amount? How do you make sure you aren't missing 'vital clues'?
If you're talking about financial/transactional data the answer is not necessarily zero, even though this seems logical to a technologist. It should be a cost/benefit decision. When I worked for a middleware vendor, I thought the answer was zero and the answer was to use two phase commit. When I worked for a bank, I realized the answer was what was cheapest and less risky - ideally this is automatic via technology, but never underestimate how expensive to implement things like 2PC are in practice and the value of a well funded reconciliations department and system reconciliations (even when you think the technology gives you 0% loss). In short - optimise for whatever is cheapest - cost of losing data (and ancillary costs) vs cost of preventing loss in the first place. Usually it's cheap enough that preventing loss in many, but not all, cases via technology is what's implemented as long as the cost of the occasional loss can be picked up easily by other means and is not too costly to rectify.    Embed Quote