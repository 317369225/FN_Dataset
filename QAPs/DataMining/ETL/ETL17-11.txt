★Why should I use an existing ETL vs writing my own in Python for my data warehouse needs?I've been reading the Kimball Group Reader and they strongly recommend I use an existing ETL. Looking at solutions such as Kettle, SnapLogic, and Talend it seems their GUIs complicate the solution instead of making it easier to model and understand.
Disclaimer - I have been developing and designing ETL in Informatica for the last 7+ years. A lot of quality answers here already, but wanted to add a few points from my experience. My current customers' ETL ecosystem is reading and loading 1+ billion rows with 5000+ jobs running over each 24 hour period. The 5000+ jobs are neatly arranged into folders, workflows and 'worklets' by business area / business data deliverable. It would be a nightmare to do impact analysis and figure out upstream / downstream dependencies if a system of such complexity was entirely pure handcoded SQL scripts called by Unix scripts in turn orchestrated using crontab entries or even an enterprise scheduler like Control M or Tidal. Most organizations (or projects) may have to chose between Fast, Cheap and Good. Custom coding may be fast (time to market) and cheap, but not always good in terms of maintenance and support (see example above) ETL code written in tools like Informatica, Datastage, Ab Initio, etc may require more time (and cost) to build and deploy, but perhaps could have to a lower  long term Total Cost of Ownership in terms of system maintenance. ETL Solution Architects and/or system analysts and project teams could figure out the code to be changed, and what changes are to be made much quicker if the whole data flow in the system is available in a GUI based environment. ETL tools also have several features which enable better governance. Features in Informatica like Powercenter Proactive Monitoring enable monitoring of batch job development with email alerts being generated in case of deviation from coding standards (defined rules). Would such a feature be available if you were writing SQL which would be called by a Unix script? Such a feature would have to be custom coded, with the custom code being maintained for each additional rule / feature required to be added. In spite of these points, I would sometimes have no option but to write a SQL script and call it using a Unix script if I wanted to build and deploy really quickly - some examples are data extracts to support new product launches, etc. If an organization has relatively less complexity of data integration needs, then they could choose to not opt for a full blown data integration tool with up front licensing and yearly support costs.    Embed Quote