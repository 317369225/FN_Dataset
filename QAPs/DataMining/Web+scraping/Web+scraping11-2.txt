★What is web scraping and is Python the best language to use for this?If so why is python the best?
Web scraping involves both crawling the web for data and extracting and analyzing the data from the page We have build a tool that runs on the amazon cloud and can crawl at scale using multiple nodes / ip addresses github project: cloud-crawler slideshare presentation: Charles Martin it is written in ruby and uses redis as a backend , queue, and distributed cache We are about to release the 0.2 version that can crawl  javascript rendered pages. It has been tested at scale running 20 microinstances on EC2.  The long range goal is to run on hundreds of spot instances, using chef or perhaps Iron Fan as the node manager. The advantages to ruby are that one can rapidly develop a distributed DSL and deploy it to various nodes A Ruby DSL Design Pattern for Distributed Computing   The ruby guys are very good at production deployments and using ruby , redis, and chef have proven to be a great combination. String manipulation in ruby is very easy because it is based on Perl syntax.    Ruby is great for analyzing webpages using the very powerful and rock-solid Nokogiri html/xml parser.  (having used Soup and Nokogiri, I can say that Nokogiri is much easier to use.  I can't speak for the python soup) Using Nokogiri as a base, we have created a simple DSL for scraping ... that is, extracting data from pages and storing it to both Redis and to the cloud (AWS S3) Nokogiri can deal with broken HTML / HTML fragments easily.  Still, ruby also has many extensions, such as Sanitize and Loofah, that can help clean up broken HTML, and are well established and rock solid. The ruby world is also just so far ahead of python for cloud development and deployments.  The ruby Bundler system is just great for managng and deploying packages from github.  Also, compare capistrano, chef or puppet to python ansinble.  (or Iron Fan to Star Cluster)  Using Chef, we can start up and tear down nodes on EC2, at will, and monitor for failures,  scale up or down, reset our IP addresses, etc. p Also, the ruby world has great testing frameworks like Fakeweb and Capybara, making it almost trivial to build a great suite of unit tests and to include advanced features, like crawling  and scraping javascript using webkit / selenium.  Note that for some applications, it may be a critical feature to crawl using an actual browser as opposed to just html and nokogiri / jsoup The main disadvantage to ruby is the complete lack of machine learning and nlp toolkits, making it much harder to emulate the capacity of a tool like Pattern.  It can still be done, however, since most of the heavy lifting can be done asynchronously using unix tools like liblinear or vowpal wabbit -- this how we had to do it at Aarvark back in the days before python Django and SciKit Learn  Also notice the ruby community has been very good at migrating from ruby 1.8.6 to 1.8 to now 2.0  Python, as great as it is, has a huge upgrade issue moving from Python 2.7 to 3.0, and this could kill Python, the way this problem killed Perl. We are an opensource project and always looking for contributors.  Please drop me a line if you are interested and you need a production quality web crawler and scraper    Embed Quote