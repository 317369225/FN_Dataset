★Does there exist an R library for web scraping?
CRAN - Package RCurl CRAN - Package XML and from CRAN Task View: Web Technologies and Services Parsing Data from the Web txt, csv, etc.: you can use read.csv() after acquiring the csv file from the web via e.g., getURL() from RCurl. read.csv() works with http but not https, i.e.: read.csv("http://..."), but not read.csv("https://..."). The repmis package contains a source_data() command to load plain-text data from a URL (either http or https). The package XML contains functions for parsing XML and HTML, and supports xpath for searching XML (think regex for strings). A helpful function to read data from one or more HTML tables is readHTMLTable(). scrapeR provides additional tools for scraping data from HTML and XML documents. The XML2R package (to be on CRAN soon) is a collection of convenient functions for coercing XML into data frames. The rjson converts R object into Javascript object notation (JSON) objects and vice-versa. An alternative to the rjson is RJSONIO which also converts to and from data in JSON format (it is fast for parsing). An alternative to the XML package is selectr, which parses CSS3 Selectors and translates them to XPath 1.0 expressions. Custom formats: Some web APIs provide custom data formats which are usually modified xml or json, and handled by XML and rjson or RJSONIO, respectively. An alternative to XML is selectr , which parses CSS3 Selectors and translates them to XPath 1.0 expressions. XML package is often used for parsing xml and html, but selectr translates CSS selectors to XPath, so can use the CSS selectors instead of XPath. The selectorgadget browser extension can be used to identify page elements. The RHTMLForms allows to read HTML documents and obtain a description of each of the forms it contains, along with the different elements and hidden fieldsCurl, HTTP, FTP, HTML, XML, SOAP RCurl: A low level curl wrapper that allows one to compose general HTTP requests and provides convenient functions to fetch URIs, get/post forms, etc. and process the results returned by the Web server. This provides a great deal of control over the HTTP/FTP connection and the form of the request while providing a higher-level interface than is available just using R socket connections. It also provide tools for Web authentication. httr: A light wrapper around RCurl that makes many things easier, but still allows you to access the lower level functionality of RCurl. It has convenient http verbs: GET(), POST(), PUT(), DELETE(), PATCH(), HEAD(), BROWSE(). These wrap functions are more convenient to use, though less configurable than counterparts in RCurl. The equivalent of httr's GET() in RCurl is getForm(). Likewise, the equivalent of httr 's POST() in RCurl is postForm(). http status codes are helpful for debugging http calls. This package makes this easier using, for example, stop_for_status() gets the http status code from a response object, and stops the function if the call was not successful. See also warn_for_status(). Note that you can pass in additional Curl options to the config parameter in http calls. The XMLRPC package provides an implementation of XML-RPC, a relatively simple remote procedure call mechanism that uses HTTP and XML. This can be used for communicating between processes on a single machine or for accessing Web services from within R. The XMLSchema package provides facilities in R for reading XML schema documents and processing them to create definitions for R classes and functions for converting XML nodes to instances of those classes. It provides the framework for meta-computing with XML schema in R RTidyHTML interfaces to the libtidy library for correcting HTML documents that are not well-formed. This library corrects common errors in HTML documents. SSOAP provides a client-side SOAP (Simple Object Access Protocol) mechanism. It aims to provide a high-level interface to invoke SOAP methods provided by a SOAP server. Rcompression : Interface to zlib and bzip2 libraries for performing in-memory compression and decompression in R. This is useful when receiving or sending contents to remote servers, e.g. Web services, HTTP requests via RCurl. The CGIwithR package allows one to use R scripts as CGI programs for generating dynamic Web content. HTML forms and other mechanisms to submit dynamic requests can be used to provide input to R scripts via the Web to create content that is determined within that R script.Authentication Using web resources can require authentication, either via API keys, OAuth, username:password combination, or via other means. Additionally, sometimes web resources that require authentication be in the header of an http call, which requires a little bit of extra work. API keys and username:password combos can be combined within a url for a call to a web resource (api key: foo.org; user/pass: Page on foo.org), or can be specified via commands in RCurl or httr. OAuth is the most complicated authentication process, and can be most easily done using httr. See the 6 demos within httr, three for OAuth 1.0 (linkedin, twitter, vimeo) and three for OAuth 2.0 (facebook, github, google). ROAuth is a package that provides a separate R interface to OAuth. OAuth is easier to to do in httr, so start there.    Embed Quote