★What is the difference between Bayesian and frequentist statisticians?
Some additional comments.  Bayesian statistics is very similar to techniques in theoretical physics and chemistry for modeling 'slow moving' variables--semi-stationary or adiabatic adiabatic processes--or more, generally variables, you feel you  can treat implicitly rather than explicitly. By treating certain variables in a problem implicitly, you are saying you are confident enough about their behavior that you can approximate what is going on with prior-knowledge A really good example of this is in the color of polyenes (dyes and pigments). We know from chemistry that it is the valence electrons that drive the color--the pi electrons in particular.  So we can build a model of color (quantum electronic structure theory) to say that it is the motion of the pi-electrons that interacts with the light, and the other electrons (the signma-core) can be treated as slow moving / stationary variables.  This can actually be formalized using something similar to Bayesian statistics / machine learning (a kind of Gaussian process theory for Eigenvalue problems) .  The solution is not perfect, however, and it took a long time to figure out why such a simple model worked so well, and simple statistical models did not work. The hard part is coming up with the "right prior" Notice that frequentist statistics actually was developed around the same time (or probably after) the development of classical and quantum statistical mechanics (ala Boltzman, Gibbs, etc).  In these systems you have 10^23 particles so you really can apply the Central Limit Theorem (although there are other issues). As we move to smaller systems in chemistry and physics, it has always been clear that using just pure statistical (frequentist) approaches would break down. For reference, Jaynes has a great discussion on the relationship between (quantum) statistical mechanics and information theory http://bayes.wustl.edu/etj/artic...    Embed Quote Updated 2 Aug, 2011. 3,387 views.