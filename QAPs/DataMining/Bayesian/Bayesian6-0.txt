What is a good way to learn about Bayesian methods?
You ask, where do I start? Start with something simple, and what could be simpler than only two possible outcomes.  Call one Success, the other Failure.  We don't know what the probability  of success might be.  It could be anything from 0 to 1. How might you get a better handle on what    might be?  If you can perform an experiment, that would get you some information.  Repeated experiments of this type form what is called a Bernoulli process. Bernoulli process A single trial    for a Bernoulli process, called a Bernoulli trial, ends with one of two outcomes— success where    and failure where  Success occurs with probability    while failure occurs with probability  The term Bernoulli process is just another name for a random sample from a Bernoulli population. Thus, it consists of repeated independent Bernoulli trials    with the same parameter  The problem for statistics is determining the value of this parameter  . All we know is that it lies between 0 and 1. We also expect the ratio    of the number of successes    to the number trials    to approach    as    approaches  , but that’s a theoretical result that doesn’t say much about what    is when    is small. The prior density function The Bayesian approach is not to specify what    is, but to put a distribution on it, that is, assign a density function  .  This function    is called a prior density function. For instance, you might assign a uniform distribution for    That would make    for all    in [0,1]. Alternatively, if you know something about the situation, you might assign something other than a uniform distribution.  If you have reason to believe that  is near 1/2, you would choose a density function that bunches up near 1/2. The sample and the posterior density function. Now perform an experiment to values for the random sample  .   Given that outcome, we have a conditional density function  which we'll abbreviate as    That density is called the posterior density function. Using Bayes theorem for densities, that can be found in terms of the reverse conditional probabilities.  We can state that as the proportion                        Thus, the posterior density function is proportional to a conditional probability times the prior density function Suppose, now, that there are    successes among the    trials. With our convention that    means the trial    ended in success, that means that . Then                        Therefore                        Thus, we have a formula for determining the posterior density function    from the prior density function    based on the number    of successes among    trials. Continuing the case of the uniform prior Let's look at what happens if we had taken the prior density function to be uniform, constantly 1 for    between 0 and 1. That last proportion for the posterior density function implies that                        That's a Beta distribution with parameters    and  . I'll denote that  Note that our prior distribution, the uniform one, is actually the beta distribution  This makes computations pretty easy.  Start with a    distribution.  Each time you get a success increase the first parameter by 1, but each time you get a failure increase the second parameter by 1. If you did happen to have extra information before you started, it's easy to take that into consideration by changing the parameters of the Beta distribution you choose for your prior. [Adapted from my class notes at Page on clarku.edu.] Bayes’ pool table example The process we just completed is what Thomas Bayes (1702–1761) did. His work, An Essay towards solving a Problem in the Doctrine of Chances, was published in 1763 after he died. He illustrated the problem with balls on a pool table. I’ll paraphrase his illustration using the terminology we developed above. Suppose a ball W is placed on a pool table so that “there shall be the same probability that it rests upon any equal part of the plane [rectangle] as another.” We’ll suppose the length of the table is 1 and that the distance of W from one end, call it the left end, is . We don’t know where W is placed, so our prior distribution on is uniform, that is, the density function    on the interval [0, 1]. Next suppose another ball O is repeatedly randomly placed on the table    times, and in    of these placements ball O is closer to the left end than ball W is.  Given that outcome what is the posterior distribution for  ? We just worked out the answer. The prior distribution for    was uniform on [0, 1]. Therefore, the posterior distribution is Comments This was just a simple model with two outcomes.  There are other models used to model other situations. It was also just the beginning.  We haven't used this model yet to answer any statistical questions.                Some density functions for assorted beta distributions    Embed Quote 