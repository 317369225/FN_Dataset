★What is an intuitive explanation of the latent Dirichlet allocation's graphical model?
Different people use different notation for the graphical models, so you should probably include a diagram to show what you mean by right and left side of the diagram. I will assume that by you meant the Dirichlet prior for the topic distribution, . The Generative Process for LDA - To create a document we first draw a topic mixture from the document-topic distribution, . Once we have the Topic distribution for the document, we draw/sample a topic from that distribution( this is akin to saying that if we have a 6-sided dice with known probabilities for each of the numbers, then you cast the die once to see what number you get). Once you pick a Topic from the Topic Distribution, you go to the topic-word distribution, for this particular topic. From this topic-word distribution( which is again a Multinomial distribution, with Dirichlet prior ) you draw a word, and this word goes into the document. This process is repeated for every word of the document. The above was the generative model. The sampling process is the reverse of the generative model i.e. the generative model tells us how to get words given the document-topic distribution and topic-word distributions, the sampling process tells us how to get the document-topic and topic-word distribution given the words of the document. We work backwards from the observed variables( i.e. words in the case of LDA) to obtain the latent variables( i.e. the topic assignment z for each of the words). With the values of z, the topic assignment for each of the words we can get the topic-word distribution, and document-topic distribution, for the document is the Dirichlet prior for the topic-word multinomial distribution .    Embed Quote