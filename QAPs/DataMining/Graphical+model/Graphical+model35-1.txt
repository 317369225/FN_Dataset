★How important is hyper-parameter optimization in Bayesian graphical models, such as Latent Dirichlet Allocation?
Hyper-parameter optimisation is all about how much data you have relative to the number of parameters.  The two cited papers ([1,2] in the question) where the first to really make a substantial statement about this that the community noticed. The standard which is the prior on the document-topic probabilities (see [1]) is not like a standard prior because its a prior on the document-topic probabilities, and most documents are expected to have a not-so-huge number of words.  So the standard line of  "priors don't matter in the limit of large data" does not apply for these hyper-parameters.   But as we get a larger number of documents we can and should estimate , which is what [1,2] are about.  Mallet does this with its asymmetric-symmetric prior variant.  However, their techniques are somewhat dated and we showed in our recent work ("Experiments with Non-parametric Topic Models", Buntine and Mishra KDD 2014) that you can do much better.  In fact you can also estimate which is the topic-word prior. The bottom line is, when a hyper-parameter has thousands of data points that it affects, and the impact of it is not drowned out with larger quantities of data, you should attempt to estimate it.    Embed Quote