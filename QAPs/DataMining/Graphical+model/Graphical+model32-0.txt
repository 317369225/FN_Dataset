★What is good way to understand the two formulas regarding this given probability graphical model?
If I'm interpreting things correctly, there are N different f's, each with a corresponding random variable x. The x's are also dependent on some vectors µ and σ. The notation then refers to the collection of all f except the i-th. Conceptually, in this Bayesian network, the joint probability of can be divided between the joint probability of , and i.e. , since this is effectively just dividing up a path of the tree into two independent parts. w is ignored here since its conditional probability can be treated as a constant and lumped in with z. Specifically, grinding out Bayes' rule via the Chain rule (probability): The second to last step here removes some irrelevant conditional variables, since the variables that are two steps away have no relevance if you are conditioned on the variables one step away. You can skip the intermediate steps if you imagine applying Bayes' rule while factoring out . I'm not totally sure what is meant by proportional () here, so the first and final steps may be invalid. The overall idea should be right though. Then the last integral is iterating over all possible values of μ, so we have , again by noting that the is conditionally independent from the other indices . I don't quite see how the can be removed from the first part of the expression, but it probably makes sense if read in context. Note that Conditional independence is the key here, so only if immediate variables are conditioned on can further variables be removed from the conditional.    Embed Quote