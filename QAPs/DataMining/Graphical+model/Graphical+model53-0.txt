★In a directed graphical model, learning is easy but inference is hard. Is this true?
In directed graphical models, some kinds of learning are easy. Specifically, it's easy to do parameter estimation when all of the following conditions or assumptions hold: independent parameters, conjugate priors (or maximum likelihood estimation), known structure, and fully-observed data. In most cases, it's reasonable to assume independent parameters (or use a simple form of parameter tying that keeps parameter estimation tractable).  Conjugate priors are also very common: they're mathematically convenient, and they're good enough most of the time. A known structure may or may not be a reasonable assumption, depending on the kind of problem you're trying to solve.  If you're trying to learn the relationships among the variables, such as which diseases are associated with which symptoms, then you probably want to optimize over different structures, and that's usually NP-hard.  If you already know the relationships and independencies in the domain, then a pre-specified structure could be fine. The condition that's most often violated is fully-observed data.  Sometimes, there are missing values in the data, such as medical tests that are run for some patients but not others.  Other times, the model has latent or hidden variables that are never observed.  For example, in topic models, the topic of each word in a document is never observed directly. When there are missing values or latent variables, the values of these variables must be inferred while learning.  The standard way to do this is with the expectation maximization algorithm or some form of gradient descent, both of which require performing inference in each iteration until the algorithm converges.  Even when inference is easy, it's a non-convex problem, which makes it difficult to find the best parameters -- different initial parameters could lead to different results. Recently, there has been some exciting work on spectral methods for learning latent variable models.  With spectral methods, parameter estimation is a convex optimization problem that's easy to optimize.  However, expectation maximization often finds better parameters in practice. Inference is almost always hard, both with directed and undirected graphical models.  In general, computing or approximating probabilities is NP-hard.  Computing exact probabilities typically requires time that is exponential in the treewidth of the graph, unless there is some other kind of structure to the factors or conditional probability distributions that makes inference easier. These topics are covered in most textbooks about graphical models.  See Koller and Friedman's textbook on Probabilistic Graphical Models for the most complete coverage on this topic.    Embed Quote