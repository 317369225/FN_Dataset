How do data analysts at companies like Facebook and Twitter run queries on extremely large data sets?
To get better response times at large volumes, a common technique is batch and cache:  use something batched like Hadoop (or your large, likely join-heavy SQL jobs), run it on a scheduler, and cache the results.  Of course this doesn't work for ad hoc querying, but it's good for predictable low-latency reporting (assuming a little lag is okay). For faster ad hoc querying, there's also the option of using something that allows you to do more in memory and less on disk.  For your normalized SQL data, you could consider MemSQL.  For insert-only & unschema'ed data, you can do a ton with a highly tunable database like Cassandra, which lets you determine on a dataset-by-dataset basis how much is in memory vs. on disk. One issue you're likely running into with your SQL server, and one you would run into with any inherently centralized (i.e. not distributed) database (MySQL, MongoDB, Redis, etc), is that your queries aren't run in parallel: as datasets get very large, parallelism is a must. Parallelism comes from distributed databases and distributed query paths. Some distributed databases + query systems that are good for rigid schema stuff: Vertica (company) is the old standard, and I've heard good things about Citus Data. Some distributed databases + query systems that are good for arbitrary schema stuff:  Apache Hadoop+HBase is the old batch standard, and Apache Storm + Cassandra (database) is a great realtime alternative, depending on your needs.  Additionally, a batched analysis hack we've seen people using in the AWS universe is an interesting combination of Amazon S3 and Amazon Elastic MapReduce, where flat text files (one JSON object per line) are stored in S3 and processed with EMR.  This can get quite expensive as the dataset grows, but analysis can be highly parallel and reasonably quick at arbitrary scale (given that you have the budget).  Dev time on something like this is pretty minimal when compared to Hadoop+HBase. At my company (Keen IO), our job is to abstract away a lot of this difficult stuff, and put a developer-friendly API on top of it.  Under the hood, we use Storm + Cassandra, plus something else out of Twitter called Trident, which is described well here:  Trident: a high-level abstraction for realtime computation.  This lets us process billions of arbitrarily-structured events, with arbitrary ad hoc queries, really really fast -- and because Cassandra is so tunable, we can move entire datasets or subsets of datasets into memory where applicable, on a customer-by-customer basis.    Embed Quote Updated 30 Mar, 2013. 9,742 views.