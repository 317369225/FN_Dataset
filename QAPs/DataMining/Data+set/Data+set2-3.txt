★What papers have shown that for machine learning, data set size is more important than the model being trained?
This statement taken at face value is demonstrably false. Citing Neils Bohr: "The opposite of a great truth is another great truth." An obvious counter example to this rule is this. Suppose you are training a text classifier to classify text into one of 20 classes. It is obvious that 10,000 train documents used to train a Support Vector Machine would do better than 100,000 documents used to train a decision tree. The reason for this is that decision trees are ill equipped to handle the enormous dimensionality of text data. Likewise I have seen text datasets with hundreds of classes where training a support vector machine on 5-10% of the data trumps training Naive Bayes or a "Nearest Centroid" classifier on all the data (though I have never seen SVM performance differ from Naive Bayes so much on datasets with say 20 classes).  All these are instances where the model being trained is far more important than the size of the data at least to an order of magnitude of data. And I have seen exactly these kinds of mistakes made in the industry on multiple occasions in the very best of places, so there are indeed a significant number of machine learning products that are sub-optimal owing to the inferior model, rather than inferior data.     Let's now talk some theory. The generalization error (error you get when you run the model on real world unseen data, not your train data) is the sum of its "bias" and "variance". For simplicity, think of the bias term as the model error when it's asked to make predictions on the train data. Intuitively, we can never expect the model to do any better on unseen data than on data it has already been fitted to, which is consistent with this. The variance term increases with the "degrees of freedom" that the model is assigned, and reduces with the amount of train data that is present (which is why more data leads to less generalization error). However if you have a very biased model (i.e. one without enough expressive power for learning a good model), no amount of data will help you bring down the generalization error to respectable levels. On the flip side, one can always build a ridiculously "overfitted model" (one with high variance) which explains the past seen data perfectly but  generalizes very poorly to future unseen data (this should remind you of stock market analysts), which does us no good. Building a great machine learning product involves being able to model the problem in the best possible way to optimally tradeoff bias and variance, which is difficult for many important problems where a machine learning approach holds most promise. This requires a strong understanding of the theory and also good intuition about the nature of the data, which is often well informed with past experience with similar data. There are many kinds of sub-optimal modeling that no volume of data can "patch up", regardless of what this question suggests. Of course to add to this, there is the issue of the computation efficiency of training algorithms, especially if you plan to use a lot of data for training, which needs to be traded off against potential accuracy also.     Part of the reason this question irks me is the kind of talk I hear in the Silicon Valley especially by those not trained in Machine learning, where large volumes of data are treated as some kind of Pixie dust that will enable any person with some common sense but no understanding of machine learning to build highly effective machine learning products. This attitude is largely powered by the fallacy inherent in this question which is unfortunately widely prevalent.      I think the real version of this thumb rule which is less pithy but much more accurate is this. Really smart Machine Learning engineers/scientists who are both theoretically strong and also hard headed about what works well in practice will likely converge to the best machine learning model quickly enough, rendering very small the marginal value of trying to continue optimizing the model (such people are however a very tiny percentage of the set of folks trying to do machine learning). However it will usually take much longer to procure the best possible labeled data from different sources, experiment with the best possible features etc. and consequently a lot of the time spent improving your machine learning product is best spent on these things. Getting more data and designing better features will therefore lead to larger marginal utility for most of the product building time. In another words, the only version of this question's thumb rule that is true is a trivial tautology - "Once you have optimized the model to the maximum possible extent, changes to the model cannot give you improvement, only more data can." :)    Embed Quote Updated 30 May, 2012. 1,267 views.