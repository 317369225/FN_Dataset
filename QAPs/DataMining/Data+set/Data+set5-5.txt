How do data analysts at companies like Facebook and Twitter run queries on extremely large data sets?
Hadoop/Hive/Hbase/Pig/Cassandra/etc. is used for analytics and massive writes. However SQL is still used for transactional data. Facebook and others use sharding and cell architectures here. Shards allow for each DB server to have only a subset of the total data. Cells make sure that each subset is independent from the others hence failures, upgrades, backups, etc. are only affecting a small subset. Additionally other types of data storage is used. Graph databases for social graph data (e.g. Hama, Giraph, Neo4j), in-memory for mostly read data (Redis, memcache), google uses dremel for fast querying (no alternatives know in others), real-time event processing (e.g. Twitter's Storm, Yahoo's S4, Google's percolator, etc.), etc. The summary is that outside of SQL solutions there are now a long list of alternative solutions. A good source to understand real-life architectures from the large Dotcoms is highscalability.com    Embed Quote 