★What problems in biology involve the collection and analysis of multi-terabyte data sets?
Any kind of medical imaging produces huge amounts. Just like the camera on your mobile phone is chewing up Gbytes, so are the back-end imaging devices for NMR, CAT scans and other X-rays, sonography, etc. While these multi-Tbyte images are sometimes reduced to a single bit ("cancer v. no cancer") after human interpretation, current practice is to retain them. I'll note that the multi-terabyte datasets produced by DNA sequencing of a single organism are generally due to the intermediate images produced on the sequencing instrument. Standard practice is now to throw these images away after analysis. Once base-calling and other processing takes place, you're looking at just multi-Gbyte scales. On a related note, it's interesting to note that instruments like the Pacific Biosciences sequencer produces huge streaming *movies* to capture dynamic effects of DNA interacting with polymerase in a well, while instruments from Illumina, Life (SoLID) and Complete Genomics produce series of static *pictures*  -- much smaller data sets that still refine down to the same results (DNA sequence).    Embed Quote 