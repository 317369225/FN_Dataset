How much data is enough when analyzing a data set?
Although it might sound very intuitive that adding more data to my model will improve the results but theoretical results do not support that idea. Suppose you fit a model/ML Algorithm to your training data and you observe that the model makes huge errors in calculating cross validation data. Now you have a number of options for taking your next step. You can get more data You can add more features or number of parameters to your data set You can add polynomial terms You can play around with the value of regularization parameter (lambda) etc.In machine learning there is a concept of bias and variance for checking model efficiency. High bias corresponds to situation when there is a under fit on the test data and high variance corresponds to over-fit on data. You use your model for calculating both test data values and cross validation(CV) data values and then use these values to calculate training error and CV error. Plot this data for different degree of polynomial you are using in your model. Based on various shapes of these curve you decide whether adding new data points will help or not. Prof. Andrew Ng explain this very nicely in his lecture on ML Diagnostics on Coursera . Week 10: Coursera    Embed Quote 