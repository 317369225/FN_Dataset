How do data analysts at companies like Facebook and Twitter run queries on extremely large data sets?
I'll try to give a high level abstraction in layman terms. Remember before computers when you had to look a word up in a dictionary? Or look a name up in a phone book? Luckily because of alphabetical ordering, we can skip roughly to the correct page, and fairly quickly find what we were looking for! Now imagine if our data got so large it couldn't fit into one book. We have a book for each letter perhaps. If we store the books in alphabetical order, we can quickly find the correct book and once again quickly find the correct word. One can set up a huge amount of servers in a similar (but more optimized) way which would allow you to find things quickly. What is Map Reduce? A map generally means a "Key" mapping to a "Value". Like in the dictionary you have the "Key" (A word) mapping to its "Value" (the definition of the word). "Reduce" basically means take all items with the same key, and reduce all these items into one aggregated item. (Of course I'm overly simplifying but I'm just trying to give an inkling as to how it might work). For example, lets say we want to run a query on twitter to find the 10,000,000 most common words used in tweets. the "key" will be the word. The "value" will be how often that word occurs in some small number of tweets that can fit on a single machine. For example if one machine was processing the following tweets: "I like cake and carrots and beans" and "I like my new android phone" It would output "Key - Value" (Word - Count) pairs: I - 2 like - 2 cake - 1 carrots - 1 beans - 1 and - 2 my - 1 new - 1 android - 1 phone - 1 Now we need to aggregate these counts with the counts generated by all the other computers running in parallel. To do this, we can store all the Keys (words) in alphabetical order much line the phone book example earlier. Perhaps there is a server that specifically "reduces" words starting with aa. So all words starting with aa might be sent to that server with their counts. aals 1 aals 1 aardvark 1 aardvarks 1 aardwolf 1 aardwolves 1 aardwolves 1 aargh 2 aargh 1 aargh 5 aargh 2 aargh 1 aargh 1 aarrgh 1 aarrgh 1 aarrgh 1 aarrgh 2 Now since these are in alphabetical order, all the keys that are the same are next to each other, so the reducer can go through and add up the counts for each word. aals 2 aardvark 1 aardvarks 1 aardwolf 1 aardwolves 2 aargh 12 aarrgh 5 Now this can all be done in parallel over thousands of computers, we can analyse many tweets at once. We now have the count of each word, we want the 10,000,000 most common. We can now flip things and treat the "Key" as the count and the "Word" as the value. That way everything will be ordered "alphabetically" in terms of its count. We then go the the very end server that has the most frequent word. We can move our way up the servers until (where we get less frequent words) until we have the 10, 000,000 most common words. There are many creative ways to use "Map Reduce" to solve large problems. Think for example of the question "how can I work out all the websites that link to my website". The only way to do that is to go through every single website and check if that website links to your website. To do this for all websites on the internet, Google might use map reduce. They could have a "Key" as the "Link on a webpage" and the "Value" could be the "website that displayed that link". after mapping, we have an alphabetical list of all the links to all the websites. We can then reduce on each individual link. For a given link (e.g. to my blog), the reducer can go through and group all the websites that link to my website.    Embed Quote 