How do data analysts at companies like Facebook and Twitter run queries on extremely large data sets?
Back in the day before new languages and tools we still did rudimentary "partitioning" by archiving and purging records by month or by year. If you had ten archived databases each with a years worth of transaction data and you wanted to know the hundred largest largest transactions of all time out of the millions of records in each database. You can walk the databases from oldest to newest. Ask the first database for the top ten and adding those to a temp table then the next for all that are greater then the max in the temp table. Repeat for each yearly archive database, saving the current database for last as this process of fetching the archive data may take a while and the past is static. Finally query the temp table which will typically have fewer than a hundred records in it (depending on year to year variation and how extreme the outliers) and get the final top ten from there.    Embed Quote 