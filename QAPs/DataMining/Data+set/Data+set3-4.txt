How can I crunch big data sets with my laptop?
That doesn't seem like that much data. Any computer made in the last 5 years or more could do a decent job at it as long as it has around 2 GB of RAM. Most laptops these day's come with 2-4 GB, more often 4. However, you don't even have to load the whole file into memory necessarily, it depends on what you are doing. You can save memory by batch processing the data if you need to. Basically you read in some number of lines at a time, process it, then move onto the next set of lines. Check Stackoverflow for a language specific example, but here is one for python : Processing large files, line by line If the data set is so large it's causing you problems on your machine you could get a subscription to something like Domino Data Lab : Domino Data Lab  or  Wakari : Web-based Python Data Analysis. I am not affiliated, I just heard about them both before. Domino Data Lab runs on the Amazon elastic compute cloud so you may have to pay a small amount of money. It will be in the cents to a few dollars range for a small job. Amazon EC2 does have a certain amount of server time they'll give you for free as well, so you could alternatively check that out : AWS Free Tier however it's going to be more technical to get a server up and running than just using Domino Data Lab or Wakari. . I'll also leave this here because it might help you with the rest of your project : Plotly    Embed Quote 