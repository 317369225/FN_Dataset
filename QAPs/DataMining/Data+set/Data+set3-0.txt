How can I crunch big data sets with my laptop?
Some thoughts: Do development on your laptop with small samples of data (e.g. a 5% sample). When you're ready to trying something on the full dataset, run your code in the cloud. A quad-core machine with 8GB RAM and an SSD drive costs just over a dime per hour at Digital Ocean. You can use a powerful machine for 8 hours per day for a week for about 5 bucks. I wouldn't use Hadoop. The overhead of writing and running map-reduce jobs isn't justified unless you have much, much more than 1GB of data. From a program execution standpoint, Java will probably be significantly faster than Python and R. As several people have mentioned, storing data in a condensed format like binary will help make larger sets fit into memory. Often the trade-off is that the more you compress data, the more work both your code and the CPU have to do to work with that data. This may or maybe not be worth it depending on your coding ability and your CPU speed. Longer term, you're giving up a lot of productivity/development cycles because you don't have a good development machine. If you can afford it, you can get a solid laptop for under $1000 these days. It costs more up front but saves you a lot of time and headaches going forward.    Embed Quote 