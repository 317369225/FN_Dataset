★Why is it that, in many data sets, there are about six times more numbers starting with the digit 1 than with the digit 9?
Benford's Law: Suppose a data set contains many entries, spread over many orders of magnitude.  Then the probability of finding that the first digit in the decimal expansion of a given number in that data set is a 1,2,...,9 is given by 0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.068, 0.051, 0.046.  (This is true regardless of the scaling of the data set, or where the data comes from, etc.) Explanation: The intuitive (if you know a bit of math) explanation for Benford's law goes as follows.   The probability that the first digit of a set of numbers is 1,2,...,9 is the same regardless of the scale; i.e., the units you use to measure those numbers.   Thus, consider the following argument: Let be a given number in the data set.   If we rescale the data set times, each time with (independent and identically distributed, random) scaling constant , , then Note by taking the log we have reduced the problem to a sum of random variables .   Regardless of the distribution of the , a central limit theorem dictates that the distribution of  is Gaussian as .   The precise nature of this distribution, however, won't be all that important.   Denote by the probability density function on .  Denote by the probability that a number in the data set (, or ) has first index .  Working with the log variables, Approximating that the integrand is constant at each step of the integral in the above formula, reduces the integral to Here are constants dependent on the probability distribution on the log variables.  Approximating that (the distribution is widely spread around many orders of magnitude), gives that Asking that these probabilities be normalized leads to the probability of finding a 1,2,...,9 being 0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.068, 0.051, 0.046.  This empirically agrees well with Benford's law.    Embed Quote 