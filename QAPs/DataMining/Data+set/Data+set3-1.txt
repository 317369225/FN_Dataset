How can I crunch big data sets with my laptop?
This data set shouldn't be a problem if you have a modern machine with decent amount of RAM (at least 2GB). I did a quick experiment with a data set that I have from an old Kaggle competition. After truncating it to get to 1.0GB, I used python and the pandas library to load the file in memory. Pandas data types are backed by numpy arrays which are efficient both in terms of memory and speed. When the load was finished the python process was using 1.88GB of memory so you can see that the overhead is less than 2x in that case. Once you have the data in memory you can do all kinds of queries using operations similar to the ones R's dataframes support. This includes filtering, aggregation, joins and more. I have successfully used the python+pandas combination in many cases. You get performance which rivals C code but at the same time you're working in a high-level language with powerful libraries.    Embed Quote 