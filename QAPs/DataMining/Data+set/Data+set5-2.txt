How do data analysts at companies like Facebook and Twitter run queries on extremely large data sets?
At Google we use Dremel/BigQuery. A bit of history: Google published the MapReduce paper in 2004 (Google Research Publication: MapReduce). That revolutionized the world of big data (started it?) and most of the answers on this page point to solutions based in this algorithm. Then in 2010 Google published a new paper: Dremel (Dremel: Interactive Analysis of Web-Scale Datasets). This new algorithm reduced the time needed to analyze multi-terabyte datasets from hours to seconds. The good news for you: Dremel has been externalized by Google as a service: Google BigQuery. You can load your data to it and analyze it in seconds, using the SQL language you already know. As with the MapReduce paper, others have taken the Dremel idea and implemented open source solutions that analyze data way faster than MapReduce/Hadoop can, but it's hard to beat the simplicity that a cloud service as BigQuery gives you (upload & analyze). Some links: What is BigQuery, 5 minute video: Video interview: Analyzing 30 Terabytes of data with BigQuery. Lots of BigQuery links: http://www.reddit.com/r/bigquery    Embed Quote 