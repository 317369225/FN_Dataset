★Support Vector Machines: What are the advantages of choosing the v[nu]-SVM formulation VS epsilon-SVM?Besides "feeling" that the parameterization of v[nu] over epsilon is more natural? Reference for v-SVM (http://www.kyb.mpg.de/fileadmin/...)
This is my naive look at it, (I've only skimmed the paper) but Proposition 1 asserts a few things: nu is an upper bound on the fraction of margin errors (and, hence, the fraction of training errors). v is a lower bound on the number of SVs. Thus, while epsilon-SVM implicitly creates a sparse solution by generating the "epsilon tube" around w, nu-SVM seems to provide an explicit parameter for controlling the sparsity of your solution (by selecting the number of SVs). Otherwise, it looks like the learning guarantees + bounds on the generalization error are the same.    Embed Quote