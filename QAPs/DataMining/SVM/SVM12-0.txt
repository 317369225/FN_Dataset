★Support Vector Machines: What is the intuition behind Gaussian kernel in SVM?How can I visualize the transformation function that corresponds to the Gaussian kernel? And why is Gaussian kernel popular? This question extends to other popular kernels as well (such as polynomial kernel).
Intuition Behind Kernels The SVM classifier obtained by solving the convex Lagrange dual of the primal max-margin SVM formulation is as follows: where is the number of support vectors. If you know the intuition behind a linear discriminant function, the non-parametric SVM classifier given above is very easy to understand. Instead of imagining the original features of each data point, consider a transformation to a new feature space where the data point has features, one for each support vector. The value of the feature is equal to the value of the kernel between the support vector and the data point being classified. In this space, the original (possibly non-linear) SVM classifier is just like any other linear discriminant. Note that after the transformation, the original features of the data point are irrelevant. It is represented only in terms of its dot products with support vectors (which are basically special data points chosen by the SVM optimization algorithm). The loose analogy one of my professors used while explaining this idea was as follows: A person has seen lakes, rivers, streams, fords, etc., but has never seen the sea. How would you explain to this person what a sea is? Perhaps by relating the amount of water in a sea to that found in a water-body the person already knows, etc. In certain cases like the RBF kernel, defining the transformed features in terms of the original features of a data point leads to an infinite-dimensional representation. Though this awe-inspiring fact is often mentioned while explaining how powerful SVMs are, it perhaps sinks in only after repeated encounters with the idea ranging from introductory machine learning to statistical learning theory. Intuition Behind Gaussian Kernels The Gaussian/RBF kernel is given as: Like any other kernel, the RBF kernel can also be understood in terms of the feature transformation via dot products given above. However, the intuition that helps best when analyzing the RBF kernel is that of the Gaussian distribution (as provided by Akihiro Matsukawa). The Gaussian kernel computed with a support vector is an exponentially decaying function in the input feature space, the maximum value of which is attained at the support vector and which decays uniformly in all directions around the support vector, leading to hyper-spherical contours of the kernel function. The SVM classifier with the Gaussian kernel is simply a weighted linear combination of the kernel function computed between a data point and each of the support vectors. The role of a support vector in the classification of a data point is tempered with , the global prediction usefulness of the support vector, and , the local influence of a support vector in prediction at a particular data point. In the 2D feature space in the figure, the heat map of the kernel function of each support vector is shown decaying away from the support vector (alongwith the resulting classifier). Notion of Universal Kernels (This comes from learning theory, is not intuitive, but good to know.) Gaussian kernels are universal kernels i.e. their use with appropriate regularization guarantees a globally optimal predictor which minimizes both the estimation and approximation errors of a classifier. Here, approximation error refers to the error incurred by limiting the space of classification models over which search is performed, and estimation error refers to error in estimation of the model parameters.    Embed Quote