★What are Kernels in Machine Learning and SVM?I'm trying to get into SVM, but I cannot get the idea of kernels. What are they and why do we need them?
This is a simple, easy to follow, low on mathematics description of SVMs and Kernels. So what is an SVM? SVM is a technique which helps us to solve Classification problem. To understand SVMs, firstly we need to understand what Classification problem really is. Classification Problem : In typical setting of Classification problem, we'd be given some red dots and some blue dots in some space and we'd be required to find out a curve (called separating boundary) that can separate all blue dots from all red dots. SVM : As it turns out, it is much easier and efficient to find out boundaries which are in the form of a straight line (or an analogous construct in higher dimensions called hyper plane) compared to curvy boundaries. Hyper-plane is just a generalization of a line in 2D and plane in 3D. SVMs help us to find a hyper plane that can separate red and blue dots. Inseparable Data : SVMs can probably help you to find out a separating hyper plane if it exists. What if there is no hyper plane which can separate red and blue dots? For example - imagine a circle in 2D with dots all over it such that adjacent dots are of alternating colors. There is no straight line (hyper plane in 2 dimensions) which can separate red and blue dots. SVMs won't find a solution here simply because there is no solution! We're pretty much stuck - and this is where Kernels come to rescue us. Kernel Functions : Let me try to explain it using a simple 1-Dimensional example. Assume that given points are as follows : A 1-dimensional hyper plane would be a vertical line. Clearly no vertical line can separate the given data set. However, if we project all points up to a two dimensional space using the mapping , we would get following data set in 2 dimensions : Now we can indeed find a hyper plane (an arbitrary line in 2 dimensions) that separates red and blue points - and hence our data can now be separated using an SVM. One possible separating line is shown in red ink. So the central idea is to be able to project points up in a higher dimensional space hoping that separability of data would improve. This mapping ( in our example ) is called the Kernel function. Bonus cookie-tip : In practice we often map our data to very very high dimensional spaces using Kernels. In fact, certain Kernels like RBF kernel map data to infinite dimensional spaces. As you might be thinking, mapping and producing such high dimensional representation is a computationally daunting task. A related concept called Kernel Trick lets us pretty much bypass this computation cheaply. Kernel functions are almost never used without this Kernel Trick.    Embed Quote