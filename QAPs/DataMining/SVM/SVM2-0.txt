★What is the computational complexity of an SVM?
It is very hard to characterize correctly. First, there are two complexities involved: at training time and at test time. For linear SVMs, at training time you must estimate the vector w and bias b by solving a quadratic problem, and at test time prediction is linear in the number of features and constant in the size of the training data. For kernel SVMs, at training time you must select the support vectors and at test time your complexity is linear on the number of the support vectors (which can be lower bounded by training set size * training set error rate) and linear on the number of features (since most kernels only compute a dos product; this will vary for graph kernels, string kernels, etc). Solving the quadratic problem and choosing the support vectors is generally hard. In general, just testing that you have an optimal solution to the SVM problem involves of the order of n² dot products, while solving the quadratic problem directly involves inverting the kernel matrix, which has complexity on the order of n³ (where n is the size of your training set). More references for this in http://citeseerx.ist.psu.edu/vie... . However, one hardly ever needs to estimate the optimal solution; and the training time for a linear SVM to reach a certain level of generalization error actually decreases as training set size increases http://citeseerx.ist.psu.edu/vie... . In general, this depends a lot on what techniques you're using, but expect to have training times of the order of n² for all but state-of-the-art linear SVMs or approximate solvers.    Embed Quote