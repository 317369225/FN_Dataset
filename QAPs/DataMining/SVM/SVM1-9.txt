★Support Vector Machines: What is the best way to implement an SVM using Hadoop?
From what I remember of using LibSVM/LibLinear, the resulting models are sets of attribute values that can be averaged together to produce a final model. So you could easily distribute the training dataset using Hadoop and train N classifiers, then combine. But in reality you should be able to sub-sample your labeled document space for training purposes, train a single model, cross-check it against the held-out documents, and then do exactly as Pete says - distribute that one model (e.g. use Hadoop's DistributedCache) to every task.    Embed Quote