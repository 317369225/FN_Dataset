★What is in machine learning, Latent SVM? How is it different from normal SVM (binary case)?A link/tutorial detailing what it is, is also cool!
Usual SVM: you learn a w using (x,y) pairs. Latent SVM: you assume that (x,y) pairs is not enough for describing the input-output relationship, but this relationship depends also in unobserved latent variables z. The following answer is the derivation of the learning procedure for a Latent SVM and summary of [1], which should serve as a tutorial in Latent SVMs. This means that now you want to learn a prediction rule like this: In order to do such a thing we would need to modify our loss function so it will include the latent variables as well: where and We can derive an upper bound for the loss that is similar to the hinge-loss upper bound: [math]\Delta( (y_i, z_i), (\hat{y}_i, \hat{z}_i) ) <= argmax_{(\hat{y}, \hat{z}) \in Y \times Z } [ <w, \Phi(x_i,\hat{y},\hat{z})> ] [/math] [math]+ \Delta( (y_i, z_i), (\hat{y}_i, \hat{z}_i) ) - (max_{z \in Z } [ <w, \Phi(x_i,y_i,z)> ])[/math] In order to do the loss-augmented inference trick and simplify the above expression we have to redefine the loss function and assume that it does not depend on the , this way and by the definition of we can put inside the max: [math]argmax_{(\hat{y}, \hat{z}) \in Y \times Z } [ <w, \Phi(x_i,\hat{y},\hat{z})> + \Delta( y_i, \hat{y}_i, \hat{z}_i )] - (max_{z \in Z } [ <w, \Phi(x_i,y_i,z)> ])[/math] We sum over all the pairs and also put the regularization term (to enforce sparse solutions) and come to the optimization problem of the latent SVM: [math]1/2 ||w||^2 + \Sum_i argmax_{(\hat{y}, \hat{z}) \in Y \times Z } [ <w, \Phi(x_i,\hat{y},\hat{z})> + \Delta( y_i, \hat{y}_i, \hat{z}_i )] - \Sum_i (max_{z \in Z } [ <w, \Phi(x_i,y_i,z)> ])[/math] Here we have an optimization problem that consists of the difference of 2 convex functions, a way to solve such a problem is to apply concave-convex procedure. 1. Yu, Chun-Nam John and Joachims, Thorsten. Learning Structural SVMs with Latent Variables. ICML 2009    Embed Quote