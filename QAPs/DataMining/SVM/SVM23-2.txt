★Is it worth trying PCA on your data before feeding to SVM?
This answer may be somewhat tangential to the question at hand because the use of PCA was secondary (after application of a feature ranking algorithm to reduce the dimensionality of the data set) prior to classification with an SVM. I worked on an image classification project last year that used a one-vs-all SVM approach for classifying art images by subject matter with four class labels. I had some success using PCA as a metric for ranking features prior to classification. A feature ranking algorithm was first used to reduce the number of features for each class pair and PCA was then used to help merge the top “best” features from across all rankings prior to classification. I experimented with a number of different well known features used in scene recognition and image classification.  I also combined feature vectors which proved to be complementary. The raw feature vectors that resulted from this were quite large (> 10K feature elements in some cases) which I believe lead to larger than desired variance during train/test.  In an effort to minimize the variance and test error I tired using a feature ranking algorithm to reduce the number of features for each of the 6 class pairs. Each class pair resulted in a different feature ranking and I needed a way to collectively merge the top N ranked features from across the 6 ranked lists. A uniform merging is one approach, but I chose to try and use PCA as a basis for deriving a simple metric that could be used to select a “better” set of the top N features from across all the ranked lists. I used 20 PCs for the ranking but fewer seemed to work just as well.  The following plot shows the intuitive appeal for the first two PCs. For example, the proximity of the two class labels ("Still Life" and "People") lead to greater emphasis for those labels when selecting the final set of N features across all class labels. On the other hand,  for "People" and "Seascapes" the separability in PC space is more clear and less emphasis was therefore given to those feature rankings  based on the metric derived from PCA. In the end I am not sure how much benefit this would have with other data sets. The end result was reduced variance and about 1-2% reduction in test error over uniform ranking (however classification accuracy was already pretty high ~90%).    Embed Quote