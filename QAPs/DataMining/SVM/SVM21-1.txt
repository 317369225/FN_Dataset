★Why scaling is important for the linear SVM classification?When performing the linear SVM classification, it is often helpful to  normalize the training data, for example by subtracting the mean and  dividing by the standard deviation, and afterwards scale the test data  with the mean and standard deviation of training data. Why this process  changes dramatically the classification performance?
Quote from A practical guide to SVM classification " The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. " Please read the section 2.2 Scaling in this guide, this guide is mainly targeted for beginners.This section also has reference to paper which explains the importance of scaling in detail and also some examples which you can try with libsvm to understand the effect of Scaling. Also, you could take a look at my blog http://intelligencemining.blogsp... where I explain the effect of normalization. Both scaling and normalization try the achieve the same. and to answer your specific question in other comment: why the test set needs  to be scaled with the mean and std of the training set instead of its  own? You are training based on a reference,  if you predict / apply model based on some other reference, it does not make sense. Lets take a simple example of employee records that I have used as a example in my blog. Each record has attributes salary and age, you do not want salary to dominate age, so you scale. The max salary in your training was 1000, lets assume you scaled it to 1.0.  The max salary amount in test set is 2000, if you scale this using the maximum in test set as reference you will get 1.0, where as if you scale it using the max from training set as reference you will get 2.0. is 2000 == 1000, so when you change your reference the meaning changes. The all the model application is just crap. But you may say, I am getting good results when I just scale using statistics from test set, yes if the statistics from both test and training set are similar, if they follow the same distribution. But think, how will you predict just one example , thats more practical. You build a model and in real worlds mostly you predict only one example at a time, so it makes sense to use the statistics from training set for scaling.  I hope it helps.    Embed Quote