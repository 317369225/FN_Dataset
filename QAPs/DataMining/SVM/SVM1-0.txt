★Support Vector Machines: What is the best way to implement an SVM using Hadoop?
Thought it might be worthwhile to put all the proposed solutions in context to see when one might use one over the other. The atbrox blog (http://atbrox.com/2010/02/08/par...) that Soren points to gives details on map-reduce implementation but the version of SVM implemented can be swapped out based on the desired properties of the solution: 1. If the original problem is easy enough such that data required to train the model can fit on a single machine, then train using a single machine (with say, LibSVM) but classify using multiple machines (as Peter Skomoroch suggests). To check if your problem is "easy", look to see the performance on your test data as you add training data. If test performance plateaus before you reach max memory on a single machine, you're good to use this version. 2. Compared to LibSVM that solves the dual problem, Chu et al, 2006 (http://www.cs.stanford.edu/peopl...) solves the primal SVM formulation directly using a second order method like Newton's method. This gives an exact solution that can be implemented within map-reduce and has all the guarantees as long as your newton method converges. For derivation details, see the original paper by Chapelle 2006 (http://citeseerx.ist.psu.edu/vie... ). This algorithm will allow you to use the kernel trick (if you need to use this) as described in section 3. The only assumption the algorithm makes is that you are able to do a matrix inverse, an O(s^3) operation, where s is the number of support vectors. To evaluate if this is true for your data, intuitively, if the data seems separable, then the decision boundary shouldn't be too complex and the number of support vectors required is small and definitely does not scale with the data. If this is not the case, then you have to resort to approximate solutions. 3. The PSVM solution by Chang et al, 2007 ( http://books.nips.cc/papers/file... ) is an approximate solution to the SVM objective. There are no guarantees associated with it. But google has a version implemented as Jonathan pointed out: http://code.google.com/p/psvm/ Special cases: 4. If you care about having a sparse solution for your SVM (i.e. feature selection is important), then use the Problem 6 formulation in the following paper (that Amund pointed to): http://jmlr.csail.mit.edu/papers... The original problem is NP-hard but they solve an approximate version of the objective. 5. All of the above solutions assume that the number of features given is not very large and the size issue is only due to the large number of data samples. If the number of features is also really large, you'll want to split across features as well. Use the version Boyd's group proposes (as Matt mentioned): http://www.stanford.edu/~boyd/pa... This is also approximate and does not give you a way to use the kernel trick. In summary, for most problems, I'd say Chapelle/Chu would be your best bet. I'd love to hear feedback if you see mistakes in my summary.    Embed Quote