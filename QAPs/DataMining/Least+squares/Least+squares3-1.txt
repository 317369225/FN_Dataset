★What is the difference between linear regression and least squares?My understanding is that they both try to fit a straight line through the data.
"Least squares" can refer to any model fitting that tries to minimize the sum of squares between two sets of points, or between a set of points and a curve. The most common use of least squares is in linear regression, more precisely "ordinary least squares" regression. However, you *can* do linear regression with other fitting methods. For example, linear quantile regression models a quantile of the dependent variable rather than the mean; there are various penalized regressions (e.g. least angle regression) that are linear, and there are robust regression methods that are linear. "Linear" means that the relationship between the dependent variable and independent variable(s) is linear in the parameters. Thus, is linear because none of the b are exponentiated.    Embed Quote