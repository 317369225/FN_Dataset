★What is the relation between least squares estimation and singular value decomposition?
SVD is one efficient way of solving large linear LSE problems. LLSE works like this The pseudoinverse is given by is the estimated or fitted coefficients. The fitted values (the hat matrix in regression analysis) are Note that is an orthogonal projection onto the columnspace of . The residuals are Pseudoinverse of , is calculated using SVD. is calculated simply by inverting non zero elements in , then transposing the result. (Also , are orthonormal and easy to invert and transposes=inversion). So now, the pseudoinverse can be calculated using, Or, The usefulness of SVD becomes apparent only for large problems where machine error becomes a factor, and we use a truncated SVD and thresholding approach. Of course, this only works for spaces where error metrics are defined using the L2-norm. [Note though I use the transpose symbol, I mean the conjugate transpose for the SVD. Pay attention to the left/right pseudoinverses - it really depends on how your problem is framed (full row rank/full column rank)]    Embed Quote