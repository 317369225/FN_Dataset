★What are the advantages of least squares regression?
There are many advantages to using least squares (LS) regression. Of course these advantages are only relevant when the true relationship between the response and predictors is linear (or at least close to linear). If this is not the case then LS regression will perform poorly. As Mario mentioned, if we make a few assumptions about the nature of the noise in our observations (namely that they are i.i.d) then there are many nice properties: LS estimate corresponds to the maximum likelihood solution. This allows us to obtain the guarantees of maximum likelihood estimates (consistency, asymptotic normality,...). This then allows us to build hypothesis tests and obtain confidence intervals for estimated regression coefficients Further, by the Gauss-Markov theorem we have that LS regression line is the Best Linear Unbiased Estimator of the true coefficients (Gauss–Markov theorem) Also mentioned by Mario, the LS problem can be efficiently solved using just matrix algebra There are also disadvantages involved with the use of least squares: LS looks to minimise a squared norm. This means that if an outlier is present this can adversely affect results. LS also requires us to invert the sample covariance matrix, , which may not be possible. This such cases approaches such as Ridge Regression or the LASSO can be employed    Embed Quote