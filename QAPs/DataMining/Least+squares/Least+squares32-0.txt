★What are some reasons to fit a linear regression using the least square by minimizing the l2 norm?Specifically, first, l1 norm is robust to outliers. Why not use l1? Second, computationally, how does the performance of solving the l2 minimization problem(either solving the normal equations or gradient descent) compare to linear programming (using simplex or interior interior point method ) for l1 norm?
L1 actually preceded lest squares historically, being proposed in the 1750s by Boscovic and a bit later by Laplace vs the 1800s for L2 by Gauss (and a few other people, too). The main good reason least squares is used is that it is computationally cheap, which mattered a great deal until recently. All it requires is solving a linear system and it generates all inferential statistics as a byproduct. A ton of useful matrix results give diagnostics as well. It is also efficient if the errors are Gaussian. Under some situations of contamination, it can be VERY bad, though. Finally the asymptotics are "nice." This is very much unlike the situation for L1. In my experience modern interior point L1 methods are pretty good given all the work Roger Koenker, et al., did to refine the quantile regression problem. However they are still slower than the non-iterative L2, which just requires a QR decomposition, which isn't iterative. In practice this often is totally meaningless. It takes longer to load the data and save results than to run many models. This can be annoying if you have to bootstrap, or have a really big dataset. Also quantile regression does not allow (yet) for extensions to structures like mixed or longitudinal data. Finally it suffers from local optima issues. They answer different questions, too. If you want to know about conditional means, you need L2.    Embed Quote