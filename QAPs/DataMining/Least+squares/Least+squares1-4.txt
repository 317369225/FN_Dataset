★Why do we use least squares rather than mod?Appreciate this probably a very naive question. Obvious we want to minimise both + & - value of the error, but why square it rather than just using the absolute Presumably squaring is better than linear (because bigger errors get more weight?)
In addition to the other answers: If you assume that your datapoints are independent and normal distributed, the least squares method is the result of maximum likelihood. Imagine that all of your measurements are drawn from a normal distribution around a function which includes some parameters that you wish to estimate from the data. The true distribution (which you do not know of course) is (I ignore the possibility here of an x-dependent variance) Since all of your measurements are independent, the likelihood of obtaining the data given that the model is true is Now obviously, we somehow wish to find the parameters that maximizes this likelihood. To do that we see that we must maximize the exponent, ie. minimize which is the sum of the square deviations. So you see, the idea follows from the form of the normal distribution. In addition you can also estimate in a similar fashion, or include priors on the fitting parameters to give a Bayesian treatment.    Embed Quote