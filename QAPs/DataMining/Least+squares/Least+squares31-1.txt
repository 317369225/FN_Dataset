★What is the advantage in using a least square regression in relation to a simple linear regression?
Least Squares - refers to the notion that you would like to minimize the squared error between the data you are trying to approximate and some functions or class of functions you are trying to approximate the data with. For example, if you assume that you can use polynomials of order n to fit your data, then you can pose the following optimization problem – Find a polynomial of order n that minimizes the squared error between the said polynomial and the data. This kind of a formulation is referred to as solving a least squares problem. If your data has no noise, it is a problem of finding the least squares approximation and if you have noise as is the case in statistics then you are solving a least squares regression problem. In this example, using polynomials to approximate your data puts them in a linear form (think linear operators), which is why you have a linear regression problem. Linear refers the kind of model you are using to fit the data, while least squares refers to the kind of error metric you are minimizing over. If you use a different error metric such as the norm, instead of the norm as in least squares, you would call it regression. With this clarification, I hope it is now obvious that it doesn't make any sense to ask for the advantages of least squares over linear regression.    Embed Quote