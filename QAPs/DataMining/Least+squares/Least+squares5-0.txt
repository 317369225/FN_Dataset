★What is alternating least square method in recommendation system?
In SGD you are repeatedly picking some subset of the loss function to minimize -- one or more cells in the rating matrix -- and setting the parameters to better make just those 0. In ALS you're minimizing the entire loss function at once, but, only twiddling half the parameters. That's because the optimization has an easy algebraic solution -- if half your parameters are fixed. So you fix half, recompute the other half, and repeat. There is no gradient in the optimization step since each optimization problem is convex and doesn't need an approximate approach. But, each problem you're solving is not the "real" optimization problem -- you fixed half the parameters. You initialize M with random unit vectors usually. It's in feature space so wouldn't quite make sense to have averages over ratings. http://labs.yahoo.com/files/HuKo... (Collaborative Filtering for Implicit Feedback Datasets) explains ALS pretty well, along with one particular formulation for applying it called ALS-WR. (They're not factoring the rating matrix, quite, but using ratings as weights in the loss function -- but the algorithm and algebraic form of the solution are similar, just with that cu weight term.) Myrrix has a fairly clean and easy to follow implementation of ALS-WR in Java. (There's also a Hadoop-based version): http://code.google.com/p/myrrix-... Apache Mahout has an implementation too.    Embed Quote