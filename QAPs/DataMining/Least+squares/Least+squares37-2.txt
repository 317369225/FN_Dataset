★Is it better to do QR, Cholesky, or SVD for solving least squares estimate and why?In particular, referring to inverse of
The standard recommendation for linear least-squares is to use QR factorization (admittedly a very stable and nice algorithm!) of . Cholesky factorization of is faster, but its use for least-squares problem is usually discouraged due to the claim that it “squares  the condition number”. However, I don’t really buy that argument. The conditioning of the linear least-squares problem itself is the condition number of the  and this property cannot be changed by changing factorization algorithm. If the condition number is so large so that the factorization algorithm matters, it is a sign of a nearly ill-posed problem. To obtain sensible results in such cases, the problem formulation should be changed, for instance (and this is very common) by introducing regularization. One way to regularize is to use SVD on , monitor the singular values, and disregard solution components associated with small singular values. A simpler way,  very standard, and a method that give very similar results, is to introduce Tichonov regularization; the normal equations will then typically be of the form . (Other choices than ) can be used, depending on the specifics of the problem.) The Cholesky factorization is perfectly fine for the regularized matrix. Another option for regularization that is sometimes promoted is to use the conjugate gradient algorithm on the normal equations (without regularization). In many important cases (that is, when the normal equations approximate a compact operator), the CG algorithm itself is regularizing, so the user can monitor the solution and stop before noise starts to affect the solution too much.    Embed Quote