★What are some reasons to fit a linear regression using the least square by minimizing the l2 norm?Specifically, first, l1 norm is robust to outliers. Why not use l1? Second, computationally, how does the performance of solving the l2 minimization problem(either solving the normal equations or gradient descent) compare to linear programming (using simplex or interior interior point method ) for l1 norm?
In addition to the answers above, I must add that the derivative of the L1 norm is discontinuous, unlike that of the L2 norm, making it harder to design algorithms that behave smoothly. I do not think we can have an absolute certainty that the L1 norm is better than the L2 norm in all cases. The L1 norm is said to perform better for high dimensional data though. As to the second part of the question, linear regression is one of the easiest things to do to the data, before proceeding to more difficult algorithms. So, even if other algorithms are going to be more accurate, it is an easy nice sanity check on the data, and a reference point which can be used to judge how "better" the more complicated algorithms perform w.r.t. linear regression.    Embed Quote