★Is it better to do QR, Cholesky, or SVD for solving least squares estimate and why?In particular, referring to inverse of
Assuming is an [] matrix: Cholesky is useful if you do not need the covariance matrix. So faster than naive Gauss-Jordan (GJ is of order , Cholesky is faster by a factor of 3). But unstable (for near-singular and rank deficient matrices). QR is more stable than Cholesky (cannot handle rank deficiency?), . It depends on the particular approach to calculating the upper triangular matrix (the complexity number here is calculated using the Householder transform approach). SVD is the most stable, but slowest, . SVD solves both underdetermined (rank-deficient) and overdetermined (more data than equations) problems in a least squares sense. Here's a ref: Page on princeton.edu (Provides a stability estimate in terms of condition number to machine precision) And my notes are based on PTVF numerical recipes.    Embed Quote