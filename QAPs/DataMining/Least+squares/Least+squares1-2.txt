★Why do we use least squares rather than mod?Appreciate this probably a very naive question. Obvious we want to minimise both + & - value of the error, but why square it rather than just using the absolute Presumably squaring is better than linear (because bigger errors get more weight?)
A least squares model has the nice property that its consistent with saying that your residuals are normally distributed. For example, building off of Anonymous' answer, if you had the data points 5, 6, 11, 14, then 9 would be the estimate of the mean of the data points using the least-squares method (it is also the sample mean). There's a nice property for normal distributions (and many other named distributions) that the best guess for the mean is just the sample mean, 9. You don't get this nice property when you're using least absolute error instead.    Embed Quote