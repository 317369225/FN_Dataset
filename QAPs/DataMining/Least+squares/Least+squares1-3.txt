★Why do we use least squares rather than mod?Appreciate this probably a very naive question. Obvious we want to minimise both + & - value of the error, but why square it rather than just using the absolute Presumably squaring is better than linear (because bigger errors get more weight?)
Least absolute deviation was actually proposed first, by Boscovic in the 1760s and then Laplace a bit later (independently?) to solve problems in astronomy and geodesy. However, it was computationally intractable for hand calculation due to it not being differentiable and so in the early 1800s Gauss developed least squares. (So did Legendre and Robert Adrain, with Gauss possibly lifting it from Legendre and Adrain probably working independently.) Gauss also developed Gaussian elimination at the same time to solve the same problem. The issue of computation of LAD was not solved conceptually until George Dantzig proposed the simplex method to solve linear programming nearly two centuries later. It wasn't until the advent of quantile regression software in the 1990s that it became practical at all. Inferential statistics were also a bear. Others have noted that Least squares has a connection to maximum likelihood for the Gaussian. LAD is the natural MLE for Laplacian errors and it gives the conditional median as the answer. This is very nice because the median is much more robust than the mean. A simple generalization lets you estimate other regression quantiles. Roger Koenker and his colleagues and students have shown lots of useful properties for Quantile regression. It's still not used nearly as often as it should, but there are issues with it as well. It's much harder to generalize LAD to situations like dependent data structures such as are found in longitudinal research, for instance. This still hasn't been satisfactorily solved. It also has a non-uniquess problem due to the fact that it is fundamentally based on order statistics but there are ways to solve this, such as the jittering technique developed to do ordinal quantile regression, where jittering is done between the categories.    Embed Quote