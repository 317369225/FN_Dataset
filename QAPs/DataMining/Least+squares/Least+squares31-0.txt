★What is the advantage in using a least square regression in relation to a simple linear regression?
Is your question, why does standard linear regressing find a best-fit line by minimizing least-squared error (L2 norm) between predicted and actual dependent variable value? As opposed to minimizing something else like absolute error (L1 norm)? If that's it, then the most direct answer is that this is the right choice if you believe that there is a real linear relationship between your series, and that any deviation from that linear relationship is just due to noise / errors that are normally distributed around the value predicted by the line. Given this assumption, the line you find by minimizing least squares is the one that maximizes the likelihood of observing the data you did. And that's probably uncontroversially good. So it's the normal error distribution assumption that leads to minimizing the L2 norm.    Embed Quote