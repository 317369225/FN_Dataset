★What is the latent log-linear model with latent variables and how do you train such a model?One thing with log-linear model that I cannot "capture" till now is the idea of "latent log-linear model" - a log-linear model with latent features. I cannot figure out what is exactly the hidden features; and how to train such a model?
I will add two more examples showing the concept of latent variables. Deep Semantic Parser in Natural Language Understanding Let's say you store some geographical facts about USA in the database and you want to build a front-end system where the user is allowed to ask questions about stored facts. However, to make it more accessible, you are not assuming any knowledge of the query language - you allow the user to specify queries in natural language. To achieve this goal, you are building some Machine Learning tools that can translate sentences from the natural language into the query language (so called semantic parser). This approach, however, is doomed in long-term as you need to collect annotations in the formal language. Of course, it would be much cheaper to collect just queries and corresponding answers in natural language. Let's say represents the uncertainty attached to the answer given the question. This however looks as an extremely hard problem, but what if we had the formal representation of the questionin the query language? Such a latent formal representation would significantly simplify the problem (a common trick in latent modeling). Once you have it, the only thing you need is executing the query on your database. Let's formulate it using the Bayesian approach: The first part is just a deterministic execution of the on the database, and the second is a log-linear model that measures the compatibility between the formal query with the question, that is: We could train such model by first generating a set of possible formal queries, and next use a gradient descent approach to fit the parameters of the distribution - sort of EM algorithm. I took this example from Page on berkeley.edu Deformable Parts Model in Object Detection This is not a strictly log-linear model, but it shows that the idea of the latent variables can be readily extended to other models. Let's say that you want to have a method detecting people. You train a template that corresponds to some view of the whole person, but you start wondering what may happen if you also use human parts in the detection (legs, head, ...). This may intuitively help in the detection. Hover, your data don't contain such parts annotated, there are only bounding boxes around the people. But we have already seen a similar problem before - some information would help in the predictions if available, but is actually missing in the training set. Nonetheless, you define an SVM objective function where, in addition, you maximize the score by matching all the parts. For the training you use (again) EM approach - detect all parts, re-train the model, repeat the procedure till convergence/get bored. I took this example from Page on stanford.edu    Embed Quote