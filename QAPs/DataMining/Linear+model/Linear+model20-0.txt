★What is the latent log-linear model with latent variables and how do you train such a model?One thing with log-linear model that I cannot "capture" till now is the idea of "latent log-linear model" - a log-linear model with latent features. I cannot figure out what is exactly the hidden features; and how to train such a model?
exRecall: A log-linear model defines to be proportional to an exponentiated linear combination of features of y.  The parameters are the coefficients in the linear combination. A conditional log-linear model similarly defines from features of (x,y). Latent variables: You can define , having defined using a conditional log-linear model based on features of (x,y,z). Here z is the latent variable.  At training time you observe (x,y) and at test time you only observe x.  But you never observe z.  It just makes the model family more powerful. Examples: In the simple case , this simply gives a mixture model, specifically a mixture of 3 log-linear models.  Note that some features may be shared across the 3 mixture components. This method commonly arises with structured variables.  Suppose x is a French sentence and y is its English translation.  It is hard to model using features of (x,y) alone.  To make the feature engineering easier, let z be a latent "alignment" that indicates which words within y are translations of which words within x.  Features of (x,y,z) can ask natural questions like "Does 'house' appear as a word-translation of 'maison'?" and "When an adjective immediately followed a noun in x, were their word-translations in y in the reverse order?"  Note that to obtain  , we must sum over all latent alignments, e.g., by dynamic programming. Training:  You can still train by maximum likelihood if you like, but only up to a local maximum since the likelihood function is in general no longer convex.  Given a sample of pairs , the gradient of the conditional log-likelihood with respect to the parameters is - , where p represents the current model. Note that this gradient is the difference of two conditional expectations of the feature vector -- the "clamped" expectation that conditions on supervised , and the "free" expectation that conditions only on .  Each expectation requires a sum over values of the latent , perhaps possible by dynamic programming or MCMC sampling.  (Alternative training method: Instead of following the gradient, you can use EM to alternate between imputing the latent z variables and maximizing the expected conditional log-likelihood on the resulting completed data.  But this amounts to essentially the same algorithm.)    Embed Quote