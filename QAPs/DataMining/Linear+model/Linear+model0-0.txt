★Why naive bayes a linear model?
In the Naive Bayes model, we estimate the posterior probability of each class given the features as: [math]P(y=c|F_1,\dots, F_n) \propto P(y=c) P(F_1, \dots, F_n | y=c)[/math] where the last step uses the Naive Bayes assumption: that the features are independent of each other given the class. In other words, we simply calculate and pick the class for which this number is the highest. Because we only care about the relative ordering of the classes we might as well consider the log probability rather than the probability. Taking the log then, what we care about is: This is the score for a class c, and we pick the class with the highest score. Now suppose each can take r different values, from 1 to r. I can rewrite the above expression as: where is an indicator function, equal to 1 if and 0 otherwise. Now let us stare at that last expression for a while. The parameters of the Naive Bayes model are the prior probabilities of the class and the probability of feature given class. We might as well re-parametrize so that the parameters are the log prior probability and the log probability of the feature given class. Denote the log prior probabilities as $b_c$ and the class-conditional feature log probabilities as $w_{c, ir}$. The only remaining term in the expression is . Denote this as $f_{ir}$. Then the score for the class c becomes: which is a linear function of the features    Embed Quote