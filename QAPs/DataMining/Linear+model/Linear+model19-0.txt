★What does the "conditional" mean in a conditional log linear model?
I learned it this way - I am writing a foundation of where the conditional stuff comes from. Suppose you have a coin and you toss it to find outcomes. Obviously, you would say that the outcome of head has a probability of 0.5 and that of a tail is 0.5. Now, what if the outcome of head depends on few features of the coin such as its weight, diameter, thickness, smoothness and so on? Now, the probability of being head or tail is not straight 0.5/0.5 but becomes a function of these features. So what I do is express the probability of head as a function of these features. I give a weight of 0.2 to the weight, 0.1 to the diameter, 0.3 to the thickness and 0.4 to the smoothness. So in general, the probability of a head/tail becomes conditional on these features that are weighted. The equation would look like: Here, unlike the simple case where we have     p(y1) = 0.5 and p(y2) = 0.5 we now have the probability of y {head | tail} being conditional on x which captures the features. Of course, we need to somehow express the compatibility of each of the features with the outcome (as to how much effect it has on head / tail) and this is captured by the equation Fj(x,y). In addition, these features have to be weighted so that for a given y {head | tail} it shows how much effect does this feature have. If for a feature, the weight w is small, that means this particular feature has a very minimal effect on predicting if the outcome is going to be y for this x. Now that the outcome Y is conditional on X, it is named as conditional. It is log linear because we weight the features in a linear way and we take the log of the final equation for a better representation.    Embed Quote