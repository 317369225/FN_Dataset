★How accurate is it to characterise atheism as a null hypothesis?This question is a follow up on the discussion on Mark Harrison's answer to Which position has the burden of proof: atheists or theists? As the title puts it, how accurate it is to characterise atheism as a null hypothesis? Is there something that is missed by this characterisation?
A null hypothesis is a standard way to test whether a claimed or expected entity exists. For example, in medicine testing, the relation between the medicine and healing is put on test. The design of such a test is that one starts with the null hypothesis: 'there is no effect of the medicine'. Then test subjects who took the medicine are compared to those who took a placebo. If there is a statistically relevant difference in healing, the null hypothesis is rejected, i.e. the effect exists. In the case of a god hypothesis, one would formulate the null hypothesis as that there is no god, IOW atheism is the null hypothesis. This null hypothesis can be falsified: if one piece of evidence is found to contradict it, the existence is considered proven. This is the basic mechanism of science itself. One famous example is the old hypothesis that only white swans existed. The null hypothesis then was 'there are no non-white swans', something that can be falsified. When in Australia black swans were found, the null hypothesis was rejected. That is the mechanism of the null hypothesis. Now, there can be two problems due to human failure: type I error: this happens when people mistakenly see evidence that is not there. This often happens due to confirmation bias, which is the reason that many test designs include blind testing. This error is sometimes called false positive. type II error: this is the opposite, it is when people fail to recognize evidence that would reject the null hypothesis. These two errors are due to human failure, or due to other reasons. It could be, for example, that newer and better instruments are developed that are better able to find evidence. For this reason, it often happens that scientific research corrects previous results. For example, when Miller and Urey did their famous experiment about abiogenesis, they found 5 amino acids. With better instruments, it was discovered years later that there were in fact 22. So this was a type II error. Now, regarding atheism being a null hypothesis. If a person claims that his god exists, we can test that claim by stating that it doesn't. If then at least one piece of evidence is found to the contrary, the null hypothesis has to be rejected, IOW the claim that that god exists must be accepted. It so happens that for many thousands of years billions upon billions of people all over the world have claimed that all kinds of gods existed. It is principally trivial to test all of these claims by stating the null hypothesis that none of them exist. If then one piece of evidence would be found to the contrary, that null hypothesis has to be rejected and the existence must be accepted. This has, however, never happened. This is a remarkable fact because in essence, the number of people who have spent a lifetime trying to find evidence for that existence is fabulous. This is in fact statistically relevant to an incredible degree, with an infinitesimally small margin of error. It is therefore almost certain that all god hypotheses are just plain wrong. People who insist their god or gods exist are thus prone to type I errors. They see evidence when there isn't any. This can be explained by evolutionary biology. There is a phenomenon called pareidolia: this happens when people see patterns that aren't there. For example, you see faces in clouds. This is obviously your mind playing tricks. However, this is an important trait that has helped survival of the species. Humanoids were easy preys for all kinds of predators. The ability to recognize predators is therefore essential for survival. The ones who would fail to recognize a predator even once had far less chance of survival and thus propagation. Natural selection would diminish their gene pool. The ones that were good at recognizing predators were much better fit to survive, and a false alarm was not such an issue. This is type I and type II errors in action. Making type I errors is a crucially important trait in the survival of a species like humanoids. It has helped us tremendously. But it has side effects such as imagining signs of the supernatural if there aren't. The same is true for predators. Failing to recognize a prey when there is one would soon lead to starvation. It's better to mistakingly go after something that turns out to be not a prey than to not recognize one if it is there, which would soon lead to starvation. The null hypothesis mechanism is a fabulous concept that we deal with thousands of times on any given day. For example, the I and 0 in computers have the same binary principle: either it's there or it isn't. Your phone is either ringing or it isn't - pick it up when it isn't would be a type I error. Now, of course it would be ridiculous to think about null hypotheses all the time, but just to illustrate how essential the mechanism is.    Embed Quote