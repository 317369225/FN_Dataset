★Is Measure Theory relevant to Machine Learning?
Yes, any theory of machine learning that one builds using a measure theoretic approach to probability will be more general and elegant. Knowledge of measure theory can make you more competent at understanding the state of the art in the theory of machine learning and can help you articulate your own ideas better. One direct application of measure theory is the use, in machine learning and information theory, of functions that measure the divergence between different distributions. The measure theoretic formulation of the problem of characterizing the 'affinity' (or otherwise) of two probability distributions yields interesting insights into applications. The formulation is based on the Radon-Nikodym derivative. I explain below as I understand it. First consider the measure theoretic analog to changing variables in the Reimann integral. The Radon-Nikodym derivative is similar to the fundamental theorem of [high school] calculus. Suppose, and are measures on ,  absolutely continuous w.r.t , with referring to integrating over a measure, for some , Then, is the Radon-Nikodym derivative, or alternatively, if is a probability measure, we have the measure theoretic definition of a probability density function . For a random variable , Discrete densities are distributions with as the counting measure. Continuous densities are those with as the Lebesgue probability measure. With this background, one can define arbitrary functions of two distributions and , with special and useful properties (such as the metric property, approximation of classification error), giving us, the affinity, divergence, or distance. Measure theoretic formulations of these divergences give more rigorous and insightful ways of constructing and analyzing these divergence/affinity/distance functions. Some examples are: 1. Hellinger distance: 2. Relative entropy/ Kullback Leibler divergence: (Note: the symbol is used to indicate that .) 3. f-divergences: Each of these divergence functions has different desirable properties. For instance, the Hellinger distance is a metric, and the relative entropy has an entropic formulation making it easy to interpret as information content. The point of bringing measure theory into this analysis is that the formulation becomes elegant, general, and potentially, more useful, without the need to separately develop theories for continuous and discrete random variables. In general, divergence functions can be used wherever one needs to compare two distributions, as a characterization of goodness of fit. Specific applications of such theory in machine learning, object tracking (http://ieeexplore.ieee.org/stamp...), feature selection (http://www.sciencedirect.com/sci...), classification (http://ieeexplore.ieee.org/xpls/...), etc. The above description is inspired by David Pollard's book, "A User's Guide to Measure Theoretic Probability," [Cambridge University Press, 2006 reprint] http://books.google.com/books?id...    Embed Quote