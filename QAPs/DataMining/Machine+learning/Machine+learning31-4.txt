★Will deep learning make other Machine Learning algorithms obsolete?Every once in a while a new algorithms comes and makes all others (in the same domain) seems kind of obsolete when it comes to the same domain. Will deep learning make that related algorithms (backpropagation NN, GMM, HMM, ...)?
No, different problems will always have different methods that work best.  It is about finding the best method for your data.  There is no method that is universally best for all problems (aka no-free-lunch theorem, see below for links). Every method has some bias and some variance, the closer your model is to the true underlying model the better you can do on average with that method.  A basic example is an estimator for the mean of a population.  Taking the sample mean (average of the data points) is a good estimator that is often used, but this will have greater risk (chance of being wrong) then an estimator that only returns a fixed value for the mean, if it happens to return the underlying mean value.  I.e., the fixed estimator has a very strong bias and very low variance, but if its bias happens to be correct, then it will give the best results. Key ideas are: no free lunch theorem Machine Learning Lesson of the Day - The “No Free Lunch” Theorem The No Free Lunch Theorem in Machine Learning No Free Lunch Theorems bias-variance trade-off Bias–variance tradeoff Understanding the Bias-Variance Tradeoff Some more practical examples: For example, if you have a series of response values for a few input parameters, you plot the data and there appears to be a clear linear trend, are you going to try and learn a model using a Deep Belief Network, or fit it with simple linear regression? Looking at Kaggle Machine Learning competitions, the method that performs best varies from competition to competition, a lot of times random forests do better even when compared to deep learning methods.  Most of the time it is an ensemble of a variety of different categories of methods that gives the best result. Although deep network methods have become very popular for visual learning tasks like image classification, I would argue Bayesian nonparametric methods (like Latent Dirichlet Allocation and the Hierarchical Dirichlet Process) are equally popular and effective for text data - albeit some of the extensions could be considered deep learning they approach it from a different perspective then the deep network approaches - i.e., less of a black box and more of a conceptual model of the generative process. In addition there are many other considerations that must go into choosing a method - as others have mentioned methods may be chosen over deep learning approaches in order to meet specific needs for interpretability, resource constraints, algorithm/software complexity, system/software maintainability, extensibility, etc.    Embed Quote