★What were the Top 10 problems in Machine Learning for 2013?
Shane Lewin's answer makes a crucial point about gathering large amounts of good data. I have witnessed that problem first hand at LinkedIn while working on Groups You May Like and Companies You May Want To Follow. Beyond that, I believe Andrew Ng (Machine Learning Professor at Stanford) has a few good answers on what today's problems are, and what has already been done to start solving these problems (his presentation explains how we can start making Abhishek Shivkumar's 9th point, Intelligent Learning, a reality today). Andrew Ng's NIPS 2012 Presentation Video (44 minutes): Machine Learning and AI via Brain simulations Slides: http://www.cs.stanford.edu/~ang/... Recap of Ng's presentation: 2001: Performance is less about the specific model used and more about quantity of data. Models trained with the largest quantity of data win overall. 2010: Once enough data is used, Ng makes a similar point about the number of features used in unsupervised models: performance is less about the unsupervised model used and more about the number of parameters trained. The unsupervised model trained with the largest number of features wins (again, provided you have enough data of course). Today: The training of huge deep neural networks at scale is possible. Google currently uses these huge models to power Android voice transcription, Google Street View house number identification, etc. The algorithms Andrew Ng and others at Google used to train these huge models at scale were: Asynchronous Distributed Gradient Descent and L-BFGS. In particular, these numerical methods are built to work in parallel. They made it possible to train huge models on clusters of tens of thousands of computers, hence bypassing the need to have every parameter fit in the memory of one computer. Now that these methods have been de-risked by Google, many others will be able to start solving this large model fitting problem too. Next 10-15 years: the rise of unsupervised models. Last year, a stacked RICA model (like Sparse Coding, Independent Component Analysis or Auto-Encoders) of 1.15 billion parameters was trained on 10 million 200x200 images. The model learned to recognize faces, cats, etc. by itself, without any labelled data. The results were the following: a random guess gave you 0.005%, previous state of the art gave you 9.5%, and this stacked RICA model with 1.15 billion parameters gave you 21.3%. Improving these models points to huge new problems which need to be solved. Ng's last long term point is that with the current progress of computational power, data+models will outperform most human ingenuity on the long run. (Ng's wiki: Main Page - Ufldl)    Embed Quote