★What are Kernels in Machine Learning and SVM?I'm trying to get into SVM, but I cannot get the idea of kernels. What are they and why do we need them?
SVMs depend on two ideas: VC dimension and optimization. Given points in the plane and the fact that they are separable (Nikhil's answer), there are infinite lines (halfplanes in 2D) that would separate them. The SVM finds the best one by solving an optimization problem on those separating halfplanes. The nice thing is that it is a Linear Programming problem which is fast and easy to solve. The VC dimension is related to a fact we took for granted: are the training points separable? Well,  when it comes to points in 2D, the maximum number of points that are guaranteed to be separable are 3, which is precisely the VC dimension. Note the diagram in Nikhil's answer projects the three points on a line to 2D, and he drew a classifier that works. The VC dimension depends on the classifiers (hyperplanes in the case of SVMs) and the dimension which are data lies in (2D in this case). Now if the points are in n-dimensions, the VC dimention is n+1. So instead, if we have m points, we need the points to lie in a space of dimension at least (m-1). When this happens, we are sure to find a solution to the optimization we perform. This is where the kernel comes in. We project the m points lying in n-dimension, n< (m-1), into an (m-1) dimensional space so that we are guaranteed to find a linear classifier in that dimension. To make it concrete, as in Nikhil's example, we had three points in 1 dimension. Not solvable. So we project it into a space of at least (3-1)=2 dimensions, and we will find a solution. Edit: As Yan King Yin points out, three points in a plane cannot be separated if they are collinear. VC dimension excludes these cases (measure zero sets), otherwise half planes couldn't separate anything.    Embed Quote