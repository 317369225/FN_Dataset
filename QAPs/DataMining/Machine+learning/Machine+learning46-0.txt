★What is the relationship between machine learning and the scientific method?Scientists follow "the scientific method" to arrive at conclusions about data, although there has been much debate about what exactly this method consists of. Machine learning also arrives at (tentative) conclusions about data - is there a relationship between the two?
Other answers explain things quite well from a certain perspective. I thought of a more literal interpretation, and I just have a couple of references to show that explicitly relates scientific discovery and machine learning. The first thing is in kernel discovery. Kernel methods are widespread in machine learning for classification and regression, but choosing a kernel can be really tough sometimes. I'll go through the problem motivation a little bit, but if you know about kernels just skip this part. Briefly, a kernel is a function that defines correlations between variables. Imagine you have some data sampled from any one of these curves: Then an appropriate kernel function to choose to try and model the original curve as best as you can (like with, say, a Gaussian process regression code) would be a periodic function because there is some clear periodicity in your data (i.e. saying and in figure D are just linearly related doesn't make sense, it's more complicated). The periodic kernel looks like: where is the th (adjustable) variable (they account for scaling, frequency, etc.). Let's say you have something like this: Well, then you notice that you have a linear trend as well as some periodic structure, so you create a new kernel by adding a linear kernel to the periodic kernel given before. Linear kernel looks something like: But what if you have something truly complicated, like this: This is not straightforward to model, and a few ideas come to mind but it's not like we would really expect them to work without messing around a bit. This is where an automated kernel discovery algorithm might help. David Duvenaud and collaborators at Cambridge's ML group wrote a pretty easily readable paper on their methods for kernel discovery search here: [1302.4922] Structure Discovery in Nonparametric Regression through Compositional Kernel Search. Basically, it's meant to do something similar to what we just did above--we try and understand something basic about the time series and apply what we know about existing canonical kernels in order to make a new one by composing them through multiplication or addition. The difference is that it becomes automated and there is a clearly-defined objective, that you want to minimize fit and test error, but also battle overfitting and applying Occam's razor. If your kernel searcher thing tells you that you needed 80 different kernels with 400 different hyperparameters to be tuned, you know that you really messed up. By the way, they've made the code available for Gaussian processes so you can mess around with it yourself: jamesrobertlloyd/gp-structure-search There's also another interesting piece of work that is similar, but applied to physical theory. Michael Schmidt and Hod Lipson developed an algorithm that can derive some basic laws of classical mechanics (Page on cornell.edu). They implement some modified versions of what's called symbolic regression, which is kind of like curve fitting at a very, very rough level. Their algorithm attempts to search for underlying laws that might fit a given data set, instead of just tweaking parameters of a previously known (or guessed) function. This is similar to the way scientists come up with ideas for modeling physical phenomena analytically, although this was more true when the mainstream physics like classical mechanics was in its infancy since now we have more complicated tools and arguments to justify certain ideas (I'm speaking abstractly here, but more concretely you can think about the discovery of quantum mechanics--it was not simply putting together expressions but something much more complicated that involved several key observations from the experiments, which a computer could still do, just not in the way specified by Lipson and Schmidt). But if you imagine being Galileo or Newton, before there was any real physics, you'd think that you'd try a bit of guessing and heuristic hand-wavy arguments to come up with a law of gravitation after looking at the data. Similar thing, just automated. These are really simplified models of how humans do science, but it shows that it is possible to model a creative process like theoretical discovery at a very coarse level at least. Similar things have happened for the mathematics community with automated theorem provers (like the one that solved the four color theorem), though this fits more under the umbrella of artificial intelligence instead of machine learning.    Embed Quote