★What are the limits of machine learning?
Formally, Decision Theory tells us the Bayes Risk is the best any classifier can hope for.  Here is a simplified explanation. First, let us assume our goal is to minimize the probability of misclassification for a binary classification problem.  The probability of error is:  . But what is ?  Well, to we talk about errors, we need a decision criteria.  So, we construct the posteriors and .  To classify, we choose the class with the higher posterior probability.  Misclassifications, then, correspond to choosing the class with the lower posterior probability. As an example, consider some data to be distributed as such: Assuming we know the prior for y, we can use Bayes' Rule to calculate the posterior.  It'd look something like this: where represents our decision boundary.  The gray region in this example exactly corresponds to the Bayes' Risk.  Intuitively we see then that we want features that will push these two posteriors as 'far' away each other as possible, to minimize the AUC. Source: Professor Malik's Machine Learning course CS189 at UC Berkeley    Embed Quote