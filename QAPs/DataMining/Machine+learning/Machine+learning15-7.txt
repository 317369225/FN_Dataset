★What is the difference between statistics and machine learning?
Machine Learning is a fairly new field ; Statistics is quite old. Machine Learning involves a variety of very advanced mathematical and numerical techniques to solve fairly hard problems in inverse theory, matrix factorization, transduction, etc.     As with many hard problems, insight is gained from several distinct disciplines, including theoretical physics, electrical engineering, computational chemistry, applied and pure math, statistics, computer science, operations research, geophysics, etc.  Many of the practitioners have training in 1 or more of these fields and/or may have adacemic faculty appointments in these departments. A good understanding of the academic discipline can be found in The Journal of Machine Learning Research http://jmlr.csail.mit.edu/ and The research interests of the faculty on the editorial board (which is quite large!) I also follow papers from researchers and staff at the Toyota Technical Institute   (founded in 2003) and jointly at the University of Chicago http://www.ttic.edu/ By old I mean the universities like Chicago began setting up institutes like this maybe 10 - 15 years ago. Sample problems include 1.  Reconstructing and smoothing images using spectral and kernel techniques 2.  Fast methods of convex optimization for very large, sparse data sets, sometimes with L1 constraints, including parallel and on-line  solvers .  Examples include structural SVMs using cutting plane & bundle methods, least mean squares / stochastic gradient methods, fast conjugate gradient solvers, interior point methods, subgradient solvers for L1 regularization, etc 3.  Formulating many related problems, such as SVD, using maximum margin techniques and convex optimization. For example, reformulating the non-convex Kmeans++/non-negative matrix factorization problem as a pure convex optimization.   In other words, solving both the theoretical and practical convergence problems, and implementing the approaches highly scalable algorithms. 4.  Deep Learning Methods for object recognition.  Notice these methods rely deeply on ideas from statistical physics (Ising Model, Hopfield NN) and  solvers from computational chemistry / protein folding.  As Hao Wooi Lim points out, these have nothing to do with statistics, and are some of the most powerful tools of machine learning. 5.  Mathematical advances in operator theoretic analysis, including advances in statistical learning theory to, say, online learning, extending Reproducing Kernel Hilbert Spaces to Banach Spaces with L1 Norms, etc. 6.  New approaches to learning / statistical inference, such hybrid regression/clustering methods (semi-supervised / transductive methods ) 7.  New approaches to finding patterns in data, such as Gunnar Carlsson's work in computation algebraic topology and learning 8.  Lot of cools things in Information Geometry , large scale ensemble sampling , etc. also, note that a. Partial Least Squares is considered a statistics technique, but it comes from computational chemistry b.  Linear Regression was invented by Gauss a long long time ago Generally good ideas  cross academic disciplines  quickly, and many famous machine learning scientists now had training in statistics, theoretical physics, etc. Indeed,  Hopfield, who invented the Backprop Neural Network, and what grew into what is now Deep Learning, was a physicist who originally worked on Raman Scattering.  The method is based, in part, on a Restricted Boltzmann Network after Ludwig Boltzmann, who co-invented Statistical Mechanics.    Embed Quote