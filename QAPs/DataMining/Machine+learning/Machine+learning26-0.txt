★What are some good books on machine learning?
This answer attempts the very ambitious problem of producing an approximately complete list. Please leave comments and tell me what's wrong and/or what is missing -- right now it's a pretty small list so I've surely left something off. Introductory remarksI think of most of ML as a contribution to the question of how we can perform statistical inference. So I will attempt to describe these books in terms of how they approach this problem (e.g., whether they are theoretical or practical, frequentist or Bayesian, and so on). Some of them will not be about ML per se, but will be about subjects on which ML people depend greatly. Generally I judge this based on whether you could publish something about it at a conference like NIPS or ICML, or whether ML people are likely to make up a significant amount of the audience when talks are delivered on the subject. In no particular order: Foundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar An ambitious book that covers an impressive subset of the theoretical basis of machine learning. Includes excellent treatment of fundamentals (learning complexity, kernel methods, boosting, PAC learning, regression), as well as some subects that are almost never covered properly (ranking, multiclass, online). Additionally gives a nice ML perspective of some things (JL-lemma) that usually are not talked about well in textbooks. Learning From Data, Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin Sort of like Foundations of Machine Learning, but seems to be built for an undergrad curriculum. Contains stuff about the theoretical underpinnings, but omits the more complicated theory. It is well-written and intuitive, with theoretically intense parts clearly marked and roped off for those who are, e.g., too mathematically "young." Information Theory, Inference, and Learning Algorithms, David J. C. MacKay [free pdf] A presentation of statistical modeling that is unified across information theory, coding theory, statistics, physics, ML, computational biology, cryptography, and signal processing. It is fantastically and uniquely illuminating. Contains my favorite treatment of many subjects (Ising models, Monte Carlo methods, fountaint codes, etc.). All of Statistics, Larry Wasserman Most of ML is collected into a series of tasks (regression, classification, clustering, etc.) whose only commonality is that they are types of statistical inference. Among the virtues of this remarkable book is that it approaches the subject of statistical inference in a very general fashion, and in a way that is graceful and approachable, but still rigorous. This is a good starting point for ML, as most of ML is a specialization of some subset of the topics presented here. IMO, it is the best book of its type. Machine Learning: A Probabilistic Perspective, Kevin P. Murphy Mostly a roundup of the recent explosion in Bayesian inference. Unlike other books on this list, ML:APP seems to have been assembled by collecting papers, stapling them all together, making the notation consistent, and adding a bit of glue to unify them. For example, most of the figures are pulled (with permission) directly from other papers. This is not a bad thing! I'm very glad someone did it. But it does sometimes feel rushed, and there are many typos. Pattern Recognition and Machine Learning, Christopher M. Bishop Bishop's book is useful mainly because it is quite thorough, and contains a good treatment of many of the esoteric corners of well-known methods. But I also found it to be quite dense, and often didn't understand exactly what Bishop was saying, or how he derived something. I find that it is very useful when I consult it about a subject after having already read 2 or 3 other texts. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Trevor Hastie, Robert Tibshirani, Jerome Friedman [pdf] The quintessential (frequentist) ML text. It's more computationally-minded than I would have expected from classically trained statisticians, but its lack of robustly Bayesian material dates it in some ways. (I'm not saying one perspective is better, I'm saing you should know both. Probabilistic Graphical Models: Principles and Techniques, Daphne Koller, Nir Friedman A dense, comprehensive, and canonical treatment of graphical models. Covers Markov Random Fields, Contitional Random fields, Bayes nets, and so on. Gaussian Processes For Machine Learning, Carl Edward Rasmussen, Christopher K. I. Williams [free pdf] The classical treatment of the Gaussian process, an important class of statistical model. If you enjoyed this, you might like my Quora blog, which is mainly about this type of thing.    Embed Quote