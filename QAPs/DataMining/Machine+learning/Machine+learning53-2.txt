★What are the limits of machine learning?
The limits of machine learning are mostly to be found in the poor generalisation performance (to the general population from which the data sample is supposed to be drawn), of the hypothesis/model that you developed off the data you got. To build on your example in the question, you can STILL build a model that fits randomly generated data very well (if you allow your model to have enough complexity or degrees of freedom to capture every nook and cranny that lies within the data sample): The in-sample error on that data set can be made arbitrarily small (in fact, exactly zero), BUT the out of sample generalisation error would be terrible because your model will have spent all of its complexity fitting the noise in the data which, in your example, is all there is to the data: noise. So, model performance really is a combination of two factors: how well a model can fit the given data sample (measured by in-sample error) but - more importantly - how well the model will perform on the rest of the data from the population of origin, outside of the sample that was used to train the model. The theory of machine learning lays out the relationships between required sample size, desired performance of the model on the general population from which the training data was sampled, and complexity of the family of models within which the chosen model was searched, the latter being captured by a quantity known as the VC dimension of the model family (or hypothesis set). VC stands for Vapnik-Chernovenkis, BTW. Generally speaking, there can be a tug of war between improving the in-sample error and improving the out-of-sample error. This tug gets worse the noisier the sampled data is. The tug gets better the larger the size of the sampled data set (off which the model was trained). Putting pressure on the learning process to keep the model simple leads to better generalisation, because this stops the model from having (and spending) too many degrees of freedom to espouse every spurious noise element in the sampled data. This can be done either by starting from less complex hypothesis sets in the first place, or by confining the learned model to a subset of a complex hypothesis set (using techniques known as regularisation, which constrain the values that the parameters of a complex model can take).    Embed Quote