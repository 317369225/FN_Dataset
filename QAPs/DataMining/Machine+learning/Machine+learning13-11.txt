★How do you explain Machine Learning and Data Mining to non Computer Science people?
I'll be borrowing pararth's mango analogy. So you are in the market trying to buy a mango - which one will you buy - the yellow one or the green one? the big one or the small one? You have never ate a mango ever - how do you know which ones are sweet and which ones sour? Have you ate any fruit at some time? And you know that good apples/oranges are bright colored and that stinky apples/oranges are bad. You could come to the conclusion that any fruit that is bright and not stinky and big is probably good and then go buy some bright big non-stinky mangoes. (transfer learning) What if you've never had any fruit at all ever? All you could do is to bucket all similar looking/smelling/size-shaped mangoes together and not come to any conclusion about which bucket is sweet or sour then you can buy one fruit from each bucket and be assured (hopefully) that not all of them are sour (unsupervised clustering). So you bought some mangoes and one of the mangoes you ate was sweet and you come to the conclusion that *this* mango is sweet. That's a pretty useless conclusion because you've have already ate *this* mango, useful conclusions are ones you can come to on other mangoes - the ones you have not yet paid your hard earned money for. So next time when you go mango shopping and you want to buy similarly (or probably more) sweet mangoes, you may do one or more of the following things. Look for and buy mangoes that had same color/smell as the sweet mango you ate earlier (clustering based classification - this is supervised learning if you have ate at least 100 mangoes or semi-supervised if you have 100 mangoes with you but only ate 1 or 2 so far). The more mangoes you eat - the more truer conclusions you'll come to. (reinforcement learning). You need to eat not just more mangoes but need to try and eat mangoes with diverging features and sometimes try even the ones you predict to be sour because (ex 1: the totapuri mangoes are good when they are green not when they are yellow, ex 2: what if the yellow sweet smelling mangoes you bought from a bad vendor were sour - do you blame the vendor or the color/smell?) (once bitten twice shy or even going to a point of no return because of a single bad experience) Do not buy mangoes on friday the 13th because all the ones that your friend bought on that day were sour and he fell sick (overfitting). Wear the same shirt that you wore the other day (overfitting) when you went mango shopping and all mangoes were sweet. You conclude that mangoes from that specific vendor which weigh exactly the same as your last sweet one with the exact same color and exact same smell are the only ones that are good. You are not completely wrong but how will you find more such mangoes? (curse of dimensionality). Give a score to all the mangoes as follows - bad smell (-3 points), good smell (+3 points), good color (+2 point), bad color (-2 point), good shape (+1 points). This kind of classification as compared to the one we did in (1) above may be more useful if say the mangoes were being auctioned instead of being sold at fixed price and you need to come up with the right price to pay. (regression) Ok, this is all just general learning - what is machine about it. Let's say you created a website called mangobook dot com and you've somehow convinced a billion people in the world that they should post pictures of every mango they buy and then comment on the photo about the taste after eating it. You have information about hundreds of billions of mangoes from around the world now - more than any single person could eat in a thousand lifetimes. A person reading all these posts on your website could then come to similar learnings as above. For example you could learn that "only people who live in bangalore like totapuri mangoes" and then you can sell this information to advertisers and profit. Since it is impossible for a single person to read all posts on your website you would make computers do this reading and learning. And the best part is you don't have to write any new code - you need to just talk to someone from internet-movies dot com or amazing-online-products dot com, get their code and just run it over your mango data instead of on their movie viewership data or product purchase data - that's all. But how does this code work? Ok so we said we have common code for learning over mango data and movie data - which means we need to first convert mango data and movie data into a single format. Let's create a spreadsheet so that for every mango that was eaten or a movie that was seen there is a row and the columns can only take "true" or "false" values. For mangoes, these columns could be "mango's weight was less than 200grams", "mango's weight was more than 500 grams", "mango's color was around 570nanometer wavelength +/- 50", "mango variety is alphonso", "mango was bought on friday the 13th", "mango buyer lives in bangalore" etc... Note that this is just one way and that using this way you should not create columns of type "mango weight was 102.324 grams" - if you do then you will suffer from curse of dimensionality. Note also that inspite of all the hype of machine learning - your domain knowledge of mangoes and your creativity becomes very important in deciding what column names to use. (feature extraction, feature selection) There will be one more final column for each row that is named "buyer loved the mango she ate". Now we could simply calculate the probabilities and conditional probabilities as follows. If there were 100 rows in your spreadsheet and 30 of then had "true" in the column named "buyer loved the mango she ate", then that is the prior probability that any random mango is good. Now look at these rows only and see how many of them had "mango's color was around 570nanometer wavelength +/- 50" and "mango variety is alphonso" both as true - say there are 20 such rows. What did you learn? Prefer alphonso mangoes that are 570 nanometer wavelength yellow. You could extend this to all combinations of columns (Naive bayes, nearest neighbour, decision trees). If you remember regression that was mentioned earlier in this article then we assigned points like bad smell (-3 points), good smell (+3 points), good color (+2 point), bad color (-2 point), good shape (+1 points). how did we get to those points? Initially you don't know and you just give all columns the same point - say (+1) and then you consider each of the columns one by one and see if changing the point scheme for that column by +1 or -1 helps in better segregation of the mangoes ...that is all bad mangoes get negative total points and good mangoes get positive total points. You continue changing the point scheme until there is good enough segregation or until you are tired. (perceptron, gradient descent, hyperplane, support vector machine, kernel trick, ...) One last warning is that if you have only ate a few mangoes and all the sweet ones you ate had good smell and most of them (but not all) were yellow - you might come to the conclusion that "good smell" and "sweet" are equivalent and assign +100 points to "good smell" and 0 points to all other features. On the other hand you might also be tempted to assign at least a "-1" point to "was mango bought on friday the 13th". Both of these might not be good for your analysis. (regularization of feature weights - use as few features as possible (occam's razor) but don't rely heavily on a single feature). One really last thing is that you might say that most of the learning in this article was intuitive and gut-feeling based - which is fine except when you are the ceo of north american mango imports corporation - then you have lots at stake and you better not make mistakes in identifying good mangoes lest your company goes bankrupt. You need to understand the risk/probability of going bankrupt when you buy millions of dollars worth of mangoes from India that you may not be able to resell in the US. And that's when math comes to rescue (training loss, generalized loss, Hoeffding's inequality etc). Also machines don't have intuition or gut-feeling and hence need a structure around which they can develop and measure different hypothesis' (gradient descent for example) and these structures also need to be practical enough that you can implement them and they are not just pure math (stochastic gradient descent, surrogate loss models) p.s: I believe even cs people should be explained in this way.    Embed Quote