★Why do we need characteristic function for probability distribution?How is behave in mathematically?
If a random variable follows a certain distribution with density , then the characteristic function is the fourier transform of . Therefore, the characteristic function completely determines of the distribution of , and vice versa. In this way, is simply another "representation" of the distribution . A couple applications: Manipulating sums of distributed random variables. If are independent, with density , then in general, it is difficult to work with the density of . However, it is easy to find characteristic function of : In particular, when the 's are identically distributed, this calculation  simplifies to , which can be used to prove the central limit theorem, as Justin Rising mentioned above. The calculation above also gives an easy proof that the sum of independent Gaussian random variables  is Gaussian. Just check that the whenever takes the shape of characteristic functions of Gaussian r.v.'s, then so does .  A same idea can also be used to prove that any linear transformation of a Gaussian vector (Multivariate normal distribution) is again a Gaussian vector. Just check that the shape of the characteristic functions match. Calculating moments of the distribution. If you have a convenient form of , then it is easy to find the moments of the distribution: . For example, by differentiating(*) we have , so that by evaluating at , we have . Higher moments can be found by repeated differentiating (see Characteristic function (probability theory)). (*) Assuming satisfies certain regularity conditions.    Embed Quote