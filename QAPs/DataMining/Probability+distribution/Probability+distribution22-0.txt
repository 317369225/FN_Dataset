★What is an intuitive explanation of the delta method for approximating probability distributions?
It's just a Taylor approximation, and is used in many asymptotic derivations in statistics. Suppose you want the approximate mean and variance of g(X), where g is a differentiable function and X is a random variable. In general it would be a terrible blunder to say "E(g(X)) = g(EX)". However, if g is linear, say g(X)=a+bX, then we do have E(g(X))=g(EX), and also Var(g(X)) = b^2*Var(X). Consider the first order Taylor approximation which is good if is very close to . In statistics we often have a sequence of random variables that converges in probability to some constant, say . Then asymptotically (i.e., for large n), the probability is very high that will be very close to , allowing us to use the Taylor approximation above to obtain and Furthermore, if is asymptotically Normal then so is , since a linear function of a Normal is Normal. Warning: a key idea here is that for a differentiable function, if you zoom in enough it looks linear. This is being combined with asymptotics that are zooming in on a certain point. However, such asymptotics may or may not yield a good approximation for a particular n. For example, an exponential function such as g(x) = e^x is highly nonlinear; it does look linear if you zoom in enough, but n may need to be very large in order for the asymptotics to kick in well enough that a reasonable approximation can be obtained here from the delta method.    Embed Quote