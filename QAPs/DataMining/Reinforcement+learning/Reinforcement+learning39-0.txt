★How can autoencoders be applied in a reinforcement learning setting?
Supposing you're using Q-Learning, you may include the observation, action and Q value as inputs to the autoenconder and train it with the appropriate target Q values according to Q-Learning rule. When you need to estimate a Q-Value for some state-action pair, just ommit it in the input and reconstruct it as if it was a missing value. EDIT: just adding some information. When you need to select some action, give the current observation and a maximum Q value (estimated for your problem) as inputs, and see the action as the missing value to be reconstructed. Something similar to my suggestion is done with the TD-Falcon-DA algorithm, but using clustering instead of autoencoders.    Embed Quote