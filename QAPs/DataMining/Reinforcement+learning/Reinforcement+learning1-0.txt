★Is reinforcement learning scalable?
While there can be many different types of scalability, I will discuss two types of scalability in RL. The first is the scalability in terms of number of states and action spaces, namely, scalability to continuous state and action spaces. The recent trend in RL (or approximate dynamic programming) has been on how to best approximate the value function using various function approximation methods (ex. non-parametric methods), as to generalize to infinite number (as the state space continuous) of unseen states. I believe we do have fairly good results in this area, both in theory and practice. However, when dealing with continuous action spaces, value-based methods fall apart as the policy improvement step requires optimizing action over a non-convex value function. Policy search is one way to get around this problem, but is currently limited to gradient-based optimization. The second is the scalability in terms of number of data points, namely reinforcement learning with big data. Theoretically speaking, in approximate value-based methods, we have sample complexity guarantees that say that the approximate policy will get close to optimal policy when we have a lot of data. Computationally speaking, it really depends on how you represent your value function: if you are using non-parametric methods, then it would not scale very well with number of data points. If you are using 10 Radial Basis Functions, then it would scale very well with number of data points. It also depends on whether you are doing model-free or model-based reinforcement learning. If you are further interested in this topic, there will be a workshop on big data for RL at AAAI 2014 [1]. [1] The AAAI-14 Workshop on Sequential Decision-Making with Big Data    Embed Quote