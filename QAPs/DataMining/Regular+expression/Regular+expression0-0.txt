★How are regular expressions implemented?
This is a tricky question because there are actually two different things called "regular expressions". They both match text but have different amounts of power and can be implemented in different ways. There are "regular expressions" in the more academic sense that can match regular languages and "regexps" which can match significantly more complex languages. If you've studied the subject in school, you're probably familiar with the former. They're also implemented in a handful of tools like Google's re2 library and lexical analyzers like Lex. On the other hand, if you're familiar with regexps from languages like Perl, PHP, Python, JavaScript, Java or pretty much anything else mainstream, you've seen the more powerful version. A lot of these rely on a particular style called PCRE - Perl Compatible Regular Expressions. For simplicity, I'm going to refer to the academic ones as "regular expressions" and the more powerful Perl-style¹ ones as "regexps". Regular ExpressionsActual regular expressions are actually pretty limited. However, their efficient implementation is actually much more interesting than the implementation of the more powerful version! Regular expressions match strings over some alphabet . A string is just a finite, possibly empty ordered list of elements from ; the set of these strings is denoted . Each expression defines a subset of : in a slight abuse of notation, . In general, a set of strings is known as a language, and the sets which can be defined using a regular expression in particular are known as regular languages. The possible regular expressions for are defined inductively. There is always the null expression which doesn't match anything at all and which only match the empty string. Thinking of them as sets, and . For each symbol , we also have the expression which matches the string containing just a single : . On top of these base cases, we can also combine existing regular expressions to form more complex ones. The easiest method is concatentation: given regular expressions and , matches followed by . So if and then . Another operation is alternation which is just set union. Given and , matches if either or match. Given the same and as above, . The last operation is the Kleene star which is repetition. Given an expression , matches the empty string (), , and so on. This is also where the notation comes from for the set of strings! Given the above, . Note how concatentation and alternation have a nice algebraic structure. is the identity for concatenation: . Similarly, is the identity for alternation: . And serves as a "zero" for concatenation: . Since there aren't any inverses, this forms a semiring with alternation as , concatenation as , as and as . If we throw in the Kleene star (also called a Kleene closure), we get a "closed semiring" which turns out to be a surprisingly versatile structure. (See "Fun with Semirings" for more details.) Often, actual regular expressions have some additional operations like for matching at least one instance of an expression. But these can always be desugared into the operations above; for example, can always be rewritten as . Compiling to an NFA We can implement a regular expression efficiently by compiling it into a finite state machine. I find it easier to think about nondeterministic finite automata (NFAs) for this. An NFA has a set of states and a set of transition functions between them; however, each transition function can lead to any number of states (ie the transition function is nondeterministic). For simplicity, we'll say an NFA always has a start state (labeled 0) and can have any number of accept states (drawn as double circles). The NFA "accepts" a string if it is in an accept state after going through the whole string. The transition function in an NFA takes an input symbol from and the current state t... (more)Loading...