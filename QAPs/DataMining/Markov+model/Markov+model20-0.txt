★Hidden Markov Models: How do you get the next predicted observed value from a HMM?
You're along the right lines, but thinking about the problem explicitly in terms of the probability density functions might help make what's going on clearer. If you knew for certain that the 1001st observation came from state , then making a probabilistic prediction of its observed value would be easy. It's just the likelihood function , where is the set of parameters for component in the HMM. In your case, this is a mixture of 4 Gaussians: , where , , and represent the mean, standard deviation, and mixing proportion of the Gaussian of the component of your HMM, respectively. It sounds like you've already inferred these parameters. But since you don't know the 1001st state, you need to make use of the product and sum rules of probability: and (marginalisation over y). Using these rules you can 'marginalise out' the state uncertainty to form a predictive density over the 1001st observation. First, apply the product rule using the transition matrix to get the joint distribution over the 1000th and 1001st states (assuming you already have the posterior distribution over the 1000st state from inference): . then, marginalise over the 1000th state using the sum rule:   (abusing the notation slightly with indicating the component of the state at time step ). Next, use the product rule to incorporate the likelihood function: . Finally, use the sum rule one last time to get a prediction over the 1001st state: . This is your answer. The prediction will be a mixture of Gaussians (assuming that none of the Gaussians in the likelihood function have identical parameters).    Embed Quote