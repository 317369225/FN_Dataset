★What machine learning problems can be solved using graph theory/algorithms?
I'll focus on spectral learning methods. What this means is that it exploits linear algebraic properties of the matrix G where G_ij represents the link between nodes i and j in a graph. Of course, this graph can be derived from something not-to-graphy, like the similarity function between every pair of documents in a Wikipedia, but the methods stay the same in that case. Examples PageRank: Probably the simplest of all of these, just take the first eigenvector of a matrix derived from the hyperlink graph of the web, representing the stationary distribution of a random web-surfer. From a linear algebra perspective it couldn't be simpler. Yet it's an enormously powerful ranking and reputation technique. There are a hundred variations, which mostly involve tweaking the "teleportation" aspect of the random surfer so that, to personalize for someone who likes cars and not horses, you teleport randomly to car sites more than horse sites. Partitionining: Using either the eigenvectors of matrices derived from G (usually the Laplacian; details not important here) you can come up with a way to partition the nodes of a matrix that can be extremely effective at minimizing crossings, or the ration of crossings between the clusters to the amount of links within the clusters, etc. Using more advanced techniques, you can use random walks from a given node to find a good "local cluster" in absolutely blazing fast time. http://www.math.ucsd.edu/~fan/wp... You can also use the Laplacian for semi-supervised learning. This essentially puts potentials on every node such that a penalty based on graph structure for sharp changes between nodes is minimized. In a learning context that lets you find probably labels for unlabeled data when you have a small amount of labeled data.    Embed Quote