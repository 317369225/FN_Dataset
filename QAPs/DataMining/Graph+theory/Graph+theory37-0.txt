★What machine learning problems can be solved using graph theory/algorithms?
Taken literally, the answer would be "almost anything".  Graphs are a flexible representation, and are ubiquitous in describing computation, from the call tree of an executing program to code dependency diagrams to neural nets and PGMs.  We use properties of these graphs in defining algorithms and demonstrating that they work, which exercises concepts from graph theory and sometimes graph algorithms.  Many crucial operations in ML algorithms that aren't typically associated with graphs have very useful graph-theoretic interpretations.  For example, Coates and Even (1967) [http://comjnl.oxfordjournals.org...] give an algorithm to rapidly invert matrices represented as a graphs. On a practical level, graph algorithms tend to become useful when a task involves a sparse relation.  That sparsity could come from the data, e.g. if you were trying to use social relations to predict properties of individuals.  Or it could come from the structure of your model, e.g. if you were trying cluster a set of points, so that involvement in a cluster mediates any other relation between two points.  In these cases, analyzing the relations as a graph can help find units of computation that can be performed independently and then reconstructed.  It can also show what properties the reconstruction might have, and how long it might take. Probabilistic graphical models (PGMs) are a well-explored machine learning framework for exploiting these kinds of problem structures.  Using a PGM usually involves proposing a probability distribution regarding your data that factorizes into functions of just a few variables each, and representing that factorization with a graph.  This isn't the best place to give a full description of PGMs and what they can do, but here are some facts related to your question. Many problem-specific models (e.g. Item response theory or Collaborative filtering) can be described as PGMs. Many other ML techniques, such as Neural networks / Boltzmann machines can be described as PGMs. (Neither this or the bullet above mean you can ignore literature specific to each model!) There are many different algorithms for how to use a PGM once it's been described, including methods for supervised and unsupervised training, inference of 1 or more likely configurations of variables, inference about the posterior probability of individual variables, and even max-margin classification tasks. PGMs have practical applications including machine vision, activity recognition, detecting duplicates and inferring hidden properties in relational data, understanding geospatial behavior of mobile devices, aggregating data from sensor networks, natural language processing, and way way more.    Embed Quote