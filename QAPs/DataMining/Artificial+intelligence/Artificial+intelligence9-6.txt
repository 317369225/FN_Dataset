Isn't developing artificial intelligence playing God?
There is a ethical component to artificial intelligence. So far its not advanced to the point of being a practical question, but when we do get artificial intelligence, figuring out how to grant them rights is a serious question. Also bear in mind; if we choose /not/ to develop artificial intelligence, it would also be playing god, by your definition. It would be making the decision that AI, as an entire class of life, should not exist. Same thing with having children; choosing to have kids is a serious decision about their life; choosing not to have kids is just as serious a decision. Yet it is one that parents are free to make. The resulting kids also have fewer rights than an adult, and the parents are again allowed to make decisions for them, including serious ones. That is also within the purview of a parent's responsibilities. Eventually the children will grow up and have the same rights as everyone else. AI will be similar. The decision to make them is ours, and is a serious one. When we manage it, they will not have full rights, and we will have a lot of arbitrary decisions to make over them. Eventually we will decide they are ready to have full rights. There will be many debates along the way; robots rights activists are almost certain to form, as well as people who declare them to be mere machines. Personally, I think a major sign that the time has come would be the AI asking for its rights. Not just some pre-designed response, but the ability to actually comprehend the concept of rights, realize it doesn't have them, and take action to correct it. Prior to that, partial rights will become nessecary. All in all, it will be a judgement call. We don't know what functional AI will look like yet. We don't know where the line is, but as we approach it, hopefully we can recognize it.    Embed Quote 