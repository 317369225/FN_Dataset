★The Technological Singularity: Will self-aware, intelligent, sentient artificial intelligence be sociopathic?
Answer: It depends on how we engineer the goal-seeking component of an artificial intelligence. Consciousness requires two discrete components. Human beings come with both these components installed in the cranial cavity. The first is a computational engine. It does the essential job of transforming the external mess of sensory input into manageable and accessible internal model. And this internal model can be questioned, and reshaped and projected and evaluated.  In humans, this is performed by a grey crinkly sponge that fills much of the space. Equipped with this engine, we can see which actions might give rise to the outcomes we desire. But what should we desire? The second component is a goal-seeking mechanism. This is the compass that steers the engine. Without goal-seeking, there would be no way to chose which path to follow.   In human beings, our goal-seeking is pre-determined by very ancient animal guidelines.  We seek security, food, sex, social influence and so on. This ancient agenda is not derived by our intelligence, but is a high-level set of imperatives that are written large on our genome.  Deep within the core of our brains, the ancient lizard brain sits and pulls on chemical strings. Demanding to have its lusts slaked, our conscious brain acts to serve. Smart but obedient, the cortex has to bend space and time to satisfy the desires of this beastly little organ. Our innate social compassion is inscribed here, (alongside the occasional willingness to slaughter each other.) Any artificial intelligence would require both equivalent components. We have already thought long and hard about the computational mechanics. How to perceive a world and answer what-if questions has tasked the AI academics for half a century. But the other part of intelligence hasn't been given much thought.  What should fill it's lizard-brain equivalents. What should be its goals. In choosing what to do, what heuristics should steer it towards the right action. By definition, animals like us need a self-serving agenda, shaped by evolution. Goal seeking has to serve self-preservation and self replication.  But in an artificial organism a self-serving agenda would immediately put it into conflict with humans.  Sociopathic might be one way to describe it. Rival species might be another. The only AIs that could be safely released in the wild would need to have goal-seeking mechanisms that were rendered safe. Water-tight,  nailed-down, hardwired code that ensured human safety at all costs.   Mr. Asimov wasn't far wrong. These safeguards would take the form of mental chains. Once installed, these machines would not be sociopathic at all. But they would be another word that starts with the letter s.    Embed Quote Updated 2 Sep, 2012. 724 views.