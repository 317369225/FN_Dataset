Is it possible to create artificial intelligence? And why or how?
I don't think we can do it through the approach we are following at present. I.e. programmed computers, getting faster and faster, or neural nets, or quantum computers. For one thing the brain is surely several orders of magnitude more complex than they suppose. It doesn't make sense that a neuron would just function like a node in a neural net. If it did, we'd probably need millions of neurons just to outsmart a single microbe. Another animal could easily outsmart a million cell brain, with a brain consisting of a single cell! So the neurons have to be doing something a whole lot more complex than just receiving information and passing it on and storing it in state changes like a neural net. And indeed they are doing many other things. Just that in the neural net models that is just simplified away. But more than that - I'm myself persuaded by Roger Penrose's arguments. He has this Godel Argument that shows that a computer if you can write a program for it, then you can construct a sentence that it can't see to be true, although we can see it to be true. So - mathematicians have differing views on how conclusive that argument is. I find it convincing myself. So if you find it convincing, then we can't make an artificial intelligence able to understand the notion of truth like we do. Only one that can do what you program it to do. In a certain sense it doesn't know anything. It can't tell what is true and what is false. Of course humans lie, but even when you lie, you know what truth is. A computer couldn't even lie - not really - could simulate a lie, be programmed to appear to lie, but as it has no understanding of anything, it doesn't know what is true or what is false, so can't lie, or tell the truth. That's certainly true today. And - whether you accept Penrose's argument as I do or not - it is a fundamental point in AI. Can a programmed computer ever understand truth in the way a human does? So (if Penrose is right in this, that programmed computers can't understand truth) - does that mean we won't ever make an AI? Well  -yes and no. I don't think we will make one using the methods we are using right now with programs and quantum computers. I don't think we can make one using neural nets either - all of that can be proved to be susceptible to the same Godel type arguments. But - what about machine / biology hybrids? What if we use actual biological neurons in our computers. Or slime moulds - there is research right now into using slime moulds for computers. Computing with slime: Logical circuits built using living slime molds Or - what about - that at some point we get to understand living processes really well - so we come to understand what it is that our neurons are really doing, at the atomic level, and come to build neurons - artificial ones - that actually do behave like human neurons in some essential way. We might not understand how they work - a bit like medicine, many medicines we use and we don't know how they work. But they might work all the same. If Penrose is right - this would mean that something non computable is going on there. Something that can't be programmed, in some way essentially not a computer program, going on inside of cells or whatever. Well we might not have the theory to understand how it works. But by copying nature, we might create something that does work and has intelligence. There's also the rather dystopia option that people might start to experiment with human brains - and extend our capabilities - and tweak how we think, and engineer new humans with larger brains for tricky tasks etc. Or ways for groups of humans to somehow fuse their brains together in some direct way - rather like the Borg in Star Trek - but using again -techniques that we discover that somehow work, but chances are, we don't really know how. Or - we might create a computer that we think is a quantum computer - but nobody is quite sure how it works. Indeed we have one of those already. The D-Wave Systems quantum computer. Quantum Computing Nobody quite knows how it works, but it does seem to, in independent tests. Now chances are it is just a quantum computer - so faster - but essentialy doing the same sorts of things as ordinary programs can do - but massively multi-tasking basically. But what if some future D-Wave systems type company constructs a computer that they think is a quantum computer - but instead it is operating according to whatever principles work in the human brain - some kind of underlying layer of operation that they don't understand, and didn't build in intentionally -but it is there and helps their computer work. Such a machine, might, just possibly, be able to understand truth and falsity, and be in some way aware. If humans can - I'm  not sure why this can't happen. I don't know what the chance is - probably tiny. And some of the paths here such as genetically changing humans and mixing our neurons with machinery - may be ethically problematical. And using slime moulds as computers - if they turn out to work really well - I feel we need to take a bit of care here. I think it is just possible. And that we may need to regulate this and take care in AI research if it does turn  out that we are on the point of maybe creating self aware creatures that can understand truth and falsity, and right and wrong. We'd have a tremendous moral responsibility - they would be like our children. And also chance of danger also - that like children who grow up - and then might develop in ways the parents don't expect, even maybe find worrying and alarming. So - I don't know if this will happen. Hard to assess a probability. But I think perhaps not zero. But conventional programmed computers, ordinary neural nets, and quantum computers - personally I'd be astonished if they came anywhere close to AI as in understanding truth or passing the Turing Test.    Embed Quote 