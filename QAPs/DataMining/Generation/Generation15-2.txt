★In programming, is a random number generator truly random?Or are the random numbers generated by a hidden algorithm?
The first thing to note is that in order for a sequence to be truly "random", it must be infinite.  In computer science, we can only imagine infinite sequences as the output of some algorithm - and then by definition it won't be random. From a more mathematical perspective, there are three "popular" definitions of randomness that turn out to be equivalent (http://en.wikipedia.org/wiki/Alg...).  I'm actually only familiar with the first two listed there :) One is Kolmogorov randomness (http://en.wikipedia.org/wiki/Kol...), which in short measures the "complexity" of a finite bit string by asking the question "how long is the shortest program that would output this string?".  For an infinite sequence to be Kolmogorov-random, all its finite initial segments must be above a certain complexity relative to their length. The other is Martin-Löf randomness.  This isn't quite as intuitive, but you can think of it as "my infinite sequence has to satisfy all probabilistic formulas".  For example, we'd expect that in a random sequence, 0 and 1 occur equally often; that is, for a sequence , you'd expect that It turns out that there are infinitely many such formulas, called "effective sequential tests", or "Martin-Löf tests", and that they have a pretty neat definition, which is a little in-depth, but is given a great treatment in http://math.uni-heidelberg.de/lo.... Either way, it turns out that any computably enumerable sequence (http://en.wikipedia.org/wiki/Rec...) is not random.  One consequence of this is that since the digits of and are computably enumerable, they are in fact not random. If you're into math, randomness is one of those fields that still has lots of open questions which are really interesting to think about.  I'd get a good grounding in computability theory, first, but after that it's pretty awesome. Peace, --Jay  ...    Embed Quote