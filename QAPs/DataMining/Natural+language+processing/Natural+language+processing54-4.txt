★What is the bottleneck holding back true (non-statistical) Natural Language Processing?
Just a thought: Basically, humans also use statistics of sorts to process language. Human language is by nature not a well-defined logical language - there are many ambiguities in a piece of text that we automatically process by "what it most-likely means" (also - we sometimes get it wrong, you know...). So, there's a theoretical limit to a non-statistical algorithms. Now, again - basically - the less text you have in a sample, the more guessing work you should do... And working with long text, building something like a AST or sematic modeling of a large body of text, is still rather demanding...    Embed Quote