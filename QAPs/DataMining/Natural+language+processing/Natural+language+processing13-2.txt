★How mature is Natural Language Processing technology as of May 2014?
Even for large translation systems like Google Translate, we've hardly begun to scratch the surface of what can be done with NLP. A lot of language modeling and translation in industry is still done with direct approaches such as n-gram type methods with part of speech tagging and TFIDF, because you can feed computers a ton of data and still expect to get somewhat decent results. Problems occur when you want to model long range dependencies and do more advanced things than word co-occurrence frequencies, though. As a simple example, if you have a sentence with ambiguity like "John saw Mary with the telescope," how do you distinguish between the two cases where John has the telescope, and Mary has the telescope? You can't just use simple n-gram methods or FSTs - you need more involved things like parse trees which can better encode the difference. Direct approaches to modeling and translation can't really model these long-range differences. The thing is that a lot of these more robust methods are computationally very expensive, and for many things (e.g. detecting spam emails), you don't necessarily need perfect linguistic understanding. It's cheaper to go with a less robust language model with more data, and this might be one reason why more powerful NLP techniques haven't yet made it out of academia. Philosophically, there's also the problem of what it means for a computer to understand language. That is, can you still consider it understanding if a computer just follows a set of rules for outputting words at the right time? (Chinese room experiment)    Embed Quote