★How does perplexity function in natural language processing?
Perplexity is often used for measuring the usefulness of a language model (basically a probability distribution over sentence, phrases, sequence of words, etc). When evaluating a language model, a good language model is one that tend to assign higher probabilities to the test data (i.e it is able to predict sentences in the test data very well). For a test set with words W = w_1, w_2, ..., w_N, the perplexity of the model on the test set is PP(W)=P(w_1, w_2, w_3) ^ (-1/N) Note that the perplexity is normalized by the number of words. However, perplexity is not a definite way of determining the usefulness of a language model. A model with low perplexity on a test set may not work equally well in a real world application whose data may not be drawn from the same distribution as the test set. However, in the lack of efficient means to evaluate language model, perplexity is a useful metric for comparing language models. Chapter 4.4 of "Speech and Language Processing" by Daniel Jurafsky and James H. Martin contain textbook information for perplexity and n-gram language models. The later part of chapter 4 also talks about the relationship between perplexity and model entropy. A quick google search with the correct keywords should be able to get you a pdf chapter of the book.    Embed Quote