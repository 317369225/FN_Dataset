★What are the biggest open problems in natural language processing?
In most cases, when a question like this is asked, people start telling about theoretical challenges to be resolved. However, in NLP, there is a huge gap between the availability of the solution in general and the availability of a reliable product implementing the solution well enough for the real world. So in the real world, even before reaching the more esoteric topics like sarcasm or Winograd Schemas Challenge, there is this huge scalability gap to be closed. 1. Scalability in domains. Today, most reliable solutions work for specific domains. Generic solutions are handicapped in one way or another. 2. Scalability in languages. English gets the lion's share of attention for obvious reasons. Even languages with tens of millions of speakers are under-served. 3. Scalability in medium. Processing speech makes text-based challenges look like a child's game. Even in text, colloquial and misspelled content (e.g. social media) is a lot more difficult to work. 4. Very long sentences (especially in Subject–object–verb languages like Japanese or German), and cross-sentence links. What about things like sarcasm and Winograd? Sure, they must be solved, but in the real world, their proportion in the real-world datasets is so minuscule, that very few people will actually notice when it will be solved. Most of the world's communication is not the likes of War and Peace but collections of simple and relatively closed sentences. Aren't these engineering rather than scientific solutions? Perhaps, but much is to be redone at the level of models and mechanisms. It's more than a bit of optimisation here and there.    Embed Quote