★How can I improve sensitivity of a classification predictive model?
Since you are working with a binary classifier, you have a sensitivity and specificity for a given decision threshold.  For example, the score from the classifier can usually be expressed as the likelhood ratio p(X|class1) / p(X|class2).  Bayes optimal decision rule gives you a way of picking a decision threshold that is a simple function of the class prior probabilities p(class1) and p(class2) and costs for making an error (scalar values for a miss and a false alarm).  The costs and priors are design parameters.  Note that the data you train your classifier with may not have intrinsic priors that reflect what you expect to see in test data.  Picking the right priors and costs (and consequently the right decision threshold) requires some thought.  This problem, picking the right priors and costs, is sometimes referred to as calibration. It might be helpful to extract the classifier scores from R, convert them to likelihood ratios (this may require applying Bayes rule if the outputs are posteriors p(class1|X) and p(class2|X) ) and then plot the ROC (receiver operator) curve for your test data set.  This at least gives you a way of visualizing the trade off between the two types of errors (misses and false alarms) as a function of the decision threshold.    Embed Quote