★What is the best way to aggregate many models in a predictive modeling task?
These techniques are usually referred to as ensemble learning. The goal is to produce an predictor with better predictive performance than any of the constituent models. The most popular models are: Bootstrap aggregating: each predictive model votes with equal weight. Voting schemes is usually weighted majority voting. Example: Random forest. Boosting: If you need to train multiple models you can emphasize the instances misclassified by the previous models and make sure that the new model gets them correctly. This can sometimes lead to better performance but can also lead to overfitting. Example: AdaBoost. Bayesian Model Averaging: Approximates the Bayes Optimal Classifier by taking a weighted average of the likelihood of the data given the model and the prior over the models. BMA Stacked Generalization: Combines the predictions of several other learning algorithms as additional inputs and makes a final prediction. Stacked Generalization (Stacking)There are some other methods like stacking or For more details please check Ensemble learning and some videos available at    Embed Quote