★How do I do predictive modeling without domain expertise?
TL;DR Domain expertise can be overrated in predictive modeling competitions. Just familiarize yourself with key packages like gbm, randomForest, and e1071. Important Note - Domain expertise is much more useful in real life predictive modeling, where the objective is much more than just minimizing an error metric. In this interview with Will Cukierski, data scientist at Kaggle, he explains that domain expertise can be overrated when dealing with particular predictive modeling competitions, and you can get very far with just a common set of statistical skills. (excerpt from an interview. emphasis is mine) You entered many Kaggle contests before you started working for Kaggle. What were some of the biggest lessons you learned? Indeed, many years back I competed in the Netflix prize. As looking at spreadsheets goes, it was a thrilling experience (albeit also quite humbling). I took out a $3,000 loan from my parents to buy a computer with enough RAM to even load the data. A few years later, I was in the final throes of my doctorate when Kaggle was founded. I made it a side hobby and spent my evenings trying to port what I researched in my biomedical engineering day job to all sorts of crazy problems. The fact that I was able to get anywhere is evidence that domain expertise can be overstated when working within different fields. If I can price bonds, it’s not that I understand bond pricing; it’s that I can learn how bonds were priced in the past. This is not to say that domain expertise is not important or necessary to make progress, but that there is a set of statistical skills that support all data problems. What are these skills? People make them sound more fancy than they really are. It’s not about knowing the latest, greatest, machine learning methods. Will it help? Sure, but you don’t need to train a gigantic deep learning net to solve problems. The lesson Kaggle reinforced for me was the importance of the scientific method applied to data. It was really basic, embarrassing things: e.g. When you do something many times, the results need to be the same. When you add a bit of noise to the input, the output shouldn’t change too much. If two models tell you the same thing, but a little differently, then you can blend them and do better. If two models tell you something completely different, then you have a bug–or even better, a massive flaw in your entire understanding of what you’re doing. Training on a lot of different perspectives of the data is better than training on one perspective of all the data. Look at pictures of what you’re doing! Write down the things that you try, because you will forget a few hours later! The competition format forces you to do all of these basic things right, more so than having them lectured at you, or reading them in a paper. I’m also happy to report that I have paid back the loan to my parents, though the jury is still out on whether I’m any wiser in the face of data. Humility is one of most used tools in my arsenal! Check out the rest of the interview at Page on gnip.com Nowadays, if you just plug in the data into a random forest model or similar, you can actually do quite well. No domain expertise needed. DataRobot (a predictive modeling company) recommends the following three R packages for Kaggle competitions: gbm (Gradient Boosted Trees) randomForest (Random Forests) e1071 (Support Vector Machines) They recommend some more packages at 10 R Packages to Win Kaggle Competitions Important Note. I will repeat this again since it's so important. Domain knowledge is extremely important for real-life predictive modeling applications, but is overrated for predictive modeling competitions. There is a significant difference between the purpose and objective of both.    Embed Quote