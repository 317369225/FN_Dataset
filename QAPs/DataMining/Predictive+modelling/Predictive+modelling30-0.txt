★What predictive modelling tools are best for data sets of size 3GB to 30GB if I do not want to use Hadoop?I do not do clickstream analysis and most of my problems can get solved without putting data onto HDFS.   I am looking for a tool that can run GLM , RandomForests , GBM etc on about 3GB to 30GB of datasets.
Use some of the larger Amazon EC2 instances that have 80 GB of RAM. Then, use any normal tool that you are used to like R or Python, as all of your data will fit in RAM and the larger EC2 instances have many processors that scikit-learn (Pythont) or caret (R) can use to calculate your models in parallel.    Embed Quote