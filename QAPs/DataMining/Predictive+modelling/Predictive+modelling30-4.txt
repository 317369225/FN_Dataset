★What predictive modelling tools are best for data sets of size 3GB to 30GB if I do not want to use Hadoop?I do not do clickstream analysis and most of my problems can get solved without putting data onto HDFS.   I am looking for a tool that can run GLM , RandomForests , GBM etc on about 3GB to 30GB of datasets.
Could you say more about your problem? If you don't have time constraints, you could run practically everything without parallelism. Modern SQL servers can hold GBs of data, but it really depends on the schema, data types and the complexity of the analytic solution. There are servers that can hold 30GB in memory for analytics, so that's also not an issue.    Embed Quote