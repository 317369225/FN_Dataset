★What are the most significant challenges and opportunities in predictive analytics?
Privacy and Ownership of Data Privacy is probably going to be easier to resolve than ownership. In terms of ownership, there are inevitably going to be conflicts between the producers and aggregators of data, the users who are represented in the data and the consumers of data as each have very different viewpoints as to who has created value.  Should a user own data related to their activity or profit from its exploitation?  Should the company that collected the data own it, or should it be made available to the user who would provide the greatest benefit by using it?  If data is anonymised, does it still belong to the user?  Insights that are gained only via the combination of two data sources should be owned by who?  Is there such thing as a data monopoly and is that anticompetitive?  If so, what would the remedies be?  Is a company that targets its own ads based upon user data different to another company that scrapes the same user data to target ads?  Should a government that collects information about or on behalf of its citizens keep it locked up, sell it exclusively to the highest bidder or make it generally available to everyone? There are technical solutions that can help resolve some of the privacy questions.  For example, it is relatively easy to remove information and/or add noise so that it's not possible to identify any individual within a data source.  For the rest, much of it is likely to be fought out via case law and legislation which will vary significantly across jurisdictions. As a counterweight, there are many organisations that believe that data should be open and that openness and interoperability provide them with a competitive advantage.  Certainly, smaller companies tend to be more in the open data camp and gradually move towards the proprietary camp as they grow.  It would be nice if open data proved to be a dominant strategy in most circumstances, but the developments such as the gradual restrictions that Twitter has placed on their data feeds would indicate that data itself (and exclusivity of this data) is being seen more and more as a profit centre these days. I'd say that society is better served by raw data being mostly open and allowing insights from analysis to be owned largely by those who produced them, as there is no zero-sum game here.  But I'm sure that a lot of data producers would disagree as to others getting a free lunch. Analysis of User Data Leaving aside privacy and ownership issues, the major focus of analysis of user data (by this I assume that we mean data produced by observing a user's activities) is in determining intent.  This is certainly the focus of a lot of the predictive analytics used in online advertising, and the reason that search advertising is far more effective than display advertising (because the searches represent intent).  But it's also important for other fields, for example predicting traffic patterns in real time requires information about where people are going. A lot of the progress in this area will be towards tools that allow data from multiple sources to be seamlessly brought together in the one place where the (increasingly powerful) techniques from the machine learning community can be brought to bear. Because the signal is often very noisy and observations are sparse or far removed from the underlying process, generative models (which model data by trying to generate similar data from an underlying random process) are going to be important as they provide a framework in which both modelling steps (inspiration) and parameter fitting (perspiration) can be applied. Scaling of Algorithms Clearly, a lot of progress has been made in the last 20 or so years.  It's now possible to build larger-scale systems far faster than before.  Having more data is almost always going to improve data-based systems. That being said, the current focus on map/reduce workflows severely constrains the kinds of models that can be built at scale and these often lack elegance or fitness to purpose.  In my opinion, people are often using a sledgehammer to nail in a tack and spinning up more virtual machines instead of performing even basic profiling of code.  Worse, data and computation can seem to be a viable substitute for analysis and insight and often are used as such. The other problem with scaling algorithms is that communications and synchronisation overheads go up and so a lot of efficiency can be lost, especially where the computation doesn't fit nicely into a map/reduce model.  Does it really make sense to scale an algorithm from 1 to 100 machines in order to process 10 times as much data and improve accuracy by 1%?  Especially given the electricity requirements of these machines and the fact that the computation being performed is to show someone an ad which will make them infinitesimally more likely to buy a product? I think that we'll continue to push the limits in terms of scalability (especially the big guys) for systems where there is a clear social benefit such as search, bioinformatics and natural language processing.  However, we'll also see more of a focus on smaller and more elegant systems (especially those that can fit on a single machine), and there will be more emphasis on making systems the right size (in terms of data, computation, etc) for the problem at hand and a focus on using analysis and insight on data, over computation, as an advantage. Data Ecosystems and Exchanges The emergence of data exchanges is clearly related to the problems with ownership of data described earlier.  They allow data to be exchanged under a clear set of rules with ownership and conditions contractually determined.  They also allow for a company to have a viable business model as a data provider and so provide useful data to the whole ecosystem without having to also compete on how the data is used. Interestingly, whilst having a common format helps, most of the efforts to enforce commonality of representation (endless blahXML formats from endless industry consortiums) haven't really done much.  People tend to find a way to mash together data no matter how it's represented; much better to release raw data early than hold onto it until it's "cleaned up" (you could always release the cleaned up version as well).  For this reason, technology that can be used to clean up data and particularly technologies to structure textual data will be increasingly important. The major challenge with these (apart from issues of ownership and privacy, which don't go away) are with the determination of value.  Data about traffic congestion might be worth much more to a delivery network than to a "100 worst traffic spots in Springfield" mashup website.  Or in the ad world, data about "fashion intenders" is much more valuable to an online fashion retailer than a car dealer, even though there is some value (as people who can afford high fashion tend to change their cars more often).  The big challenge around data ecosystems and exchanges will be the development of pricing models which measure and reflect the differences in value of data to each customer.  These models will also need to reflect the value of combinations of data sources (two data sources may be more valuable when combined than the sum of their values separately).    Embed Quote