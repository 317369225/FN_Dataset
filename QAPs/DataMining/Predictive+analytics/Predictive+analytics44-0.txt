★Typically, how long does it take for a data scientist to create a predictive analytics model?For example, how long would it take to develop a model for predicting customer lifetime value? What level of accuracy is expected for this kind of model?
I've been building a lifetime value model for my company.  I'll outline my steps. Data pipeline and process: I cheated a little bit by latching onto our current metrics as opposed to building the whole thing myself from scratch.  This reduced the engineering by a good deal of time, but didn't eliminate it.  The drawback was that the data wasn't exactly the way I wanted, so I had to be flexible in my modeling choice. Preprocessing: In my industry, we have a very cohort-centric view of the universe, because revenue generalization is heavily dependent on a person's age.  This informs the modeling.  Frequently I twist my brain in a bunch because I'm simultaneously considering real-time and player-time. Modeling: I started with a time series model of certain high level groupings of my data.  I didn't do any fancy time series modeling since I'm fairly weak there, and also because for domain specific reasons, I would have had to adjust my model considerably to account for heteroskedasticity. Validation: the bane of my existence, because I find it incredibly time-consuming to build a validation system for time series data while also keeping everything clean.  My general solution is to keep the primary process clean while tolerating ugliness in the validation code.  :( I wanted to push down my estimates to more granular subsets of the population, and so I basically rolled my own (bullshit) version of Bayesian shrinkage, using a domain (cohort)-centric formulation. I process my data locally while in prototype, but I have to host my script somewhere to cron it.  This step is not that bad. After processing the data wherever, it has to be put back into a place where it is accessible to anyone.  This will be a sql script. Designing some kind of interface for people to actually see the data.  We use spotfire, so there isn't much coding to do here.  But design is something I try to really put a lot of thought into. I've spent about 2 months doing this, and most of it is done, but I'm still ironing out bugs, and doing validation for the shrinkage part of my model, so that I'm comfortable tuning this part.  Validation is the absolute worst part and most time consuming for me with this complicated time series data. For the record, I work at a free to play gaming company, and this makes revenue generation excrutiatingly noisy.  Relative error ranges from 10-30 (eek) percent, for the model as it currently is.  Most of this is inherent noisiness, or so I tell myself.  At the same time, relative error is a little misleading since the more you aggregate your data, the lower your error gets.    Embed Quote