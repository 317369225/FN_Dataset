★How do I derive normal distribution and why can it apply to so many things?I never understood why things would converge to normal distribution, and how people derived the complicated formula. Can anyone try to explain that?
As others have stated, it seems out of the Central Limit Theorem. There are a few versions of it, but the one that is "easiest" to prove goes as follows. (I'm going to try to express this in layman's terms) Suppose you have some population on which you can randomly  pick individuals and make measurements. Those measurements for selected individuals are distributed according to some probability distribution. Could be anything. Could be flips of coins with the distribution 50-50 for heads/tails. Could be genetic alleles and the particular version of a given gene a person has. Whatever. This distribution will have some mean, mu, and some standard deviation, sigma. Suppose you take a sample of size N of individuals from the population. The individuals should be selected randomly, and each section should be independent of the others. Fine print, the sample size should be small compared to the population, so that the population distribution is not appreciably affected by sampling. So each individual measurement {X1, X2,... XN} is from the same distribution, and each is independent. For this sample of N individuals, you can calculate a sample mean: SN=(X1+X2+...+XN)/N Just taking the average for the sample. Suppose you repeat this experiment many times. You take a sample, measure the N individuals, compute the mean. Now, these sample means will tend to cluster around the mean for the population of individuals, though it will vary from sample to sample. The standard deviation of the samples will also be close to the standard deviation of the population. To make it concrete, suppose the population mean is 10, the standard deviation 2. Then is you take 5 samples say of 20 individuals each, you might get sample means of 9, 9.5, 10.1, 10.4, 11.1. Close to 10, but there's variability, because it's random sample. Suppose you repeated this experiment countless times, and you kept track, and plotted the distribution of sample means. What the Central Limit Theorem says is the following. 1. The mean of this distribution of sample means will tend toward mu, the population mean. 2. The standard deviation of the distribution of sample means will tend toward sigma/square root(N) 3. And, the really key result. As N gets sufficiently large (N->infinity), the distribution of sample means converges to a normal distribution, with mean mu and standard deviation sigma/square root(N) Whatever you start off with as a population distribution, if you take sufficiently large samples, the sample means will be distributed (approximately) according to a normal distribution. The larger the sample, the closer this will apply. And this happens surprisingly fast. Unless your initial distribution is really weird, a rule of thumb is that N=30 and you'll get something that basically is the normal distribution to within two decimal points. The same idea applies if you look not at the distribution of sample means, but of the sum of measurements over the sample. Distribution of sample sums goes to normal with mean mu*N and S. D. =sigma*square root(N). So, example. Height is determined by a bunch of different genes acting together, each with its distribution. Each adds toward contributing to height. The distribution of the net result if these factors adding together is expected to be normal, and it is. (In fact, the fact that height is normally distributed is a clue that height is a sum of various genes) Or say looking at coin flips. Score 1 for heads, 0 for tails. Look at samples of 30 coin flips. The number of heads will just be the sum of the scores for each coin. By the Central Limit Theorem, we expect the heads per sample of 30 coins to be approximately a normal distribution, centered on 15, which we see if we run a simulation on a computer. Anywhere we see a phenomenon in nature resulting from numerous random factors adding together, we get a normal distribution. That's why it's so common. We also see it pop up when you have a phenomenon that is the multiplication product of a bunch of positive, non zero factors. If we take the log of that variable, it will be equal to the log of the contributing factors. (Log of product = sum of logs) The logs of the contributing factors will have their own distributions. The log of the bigger phenomenon is therefore the sum of small factors with random distributions. So the log of the bigger variable will have a normal distribution. This is a log-normal distribution, another common one in nature. The central limit theorem is one of the most beautiful in math when you start to understand it. Along with some results in complex variables, it is one of those results where, for me, mathematics intersects with theology. Something profound and miraculous. As for how you prove it, it's kind of involved. The simplest proof uses something called a characteristic function. The proof can be tracked down with the help of professor Google.    Embed Quote