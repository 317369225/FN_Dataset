★What is an intuitive explanation for how the t-distribution, normal distribution, F-distribution and Chi-square distribution relate to each other?Why do all these different distributions exist and how do we use them in statistics? What influences the choice of distribution in statistical testing?
In standard logic, we rely on statements like A implies B, therefore ~B implies ~A. Unfortunately, this clean cut type of logic doesn't help us in many real world problems where it's theoretically possible for any result to happen, but some are way more likely to happen than others. So in hypothesis testing, we're trying to make an argument along the lines of, "If A is true, then B is our likely result. If we see ~B, ie. if we see something that is very unlikely given A, then we conclude that ~A is true." The problem is the step of "If A then B is our likely result", namely what is B, and how likely is it? This is where these distributions come in handy. The different distributions are well known equations to which we can fit large set of problems. We'd like to say something like, "If A is true, then we expect B to follow an F-distribution. Because our observed B is really rare under an F-distribution, we conclude that A is false." There are two key points to understand: 1. The central limit theorem lets us take an outcome whose distribution we don't know much about, and arrive at a well known distribution that we have the equation for, the normal distribution. For example, if you have a 20 sided weighted dice with a completely random set of probabilities that we don't know for each side, and you sum the result of a ton of rolls, the sum will have a normal distribution. 2. Once we are dealing with normal distributions, we can combine them to get different properties in our final test, like incorporating our confidence due to sample size or making the test result not change if we say, change units from feet to centimeters. All of the other distributions you're asking about are based on the normal distribution: Chi Squared Distribution: If you have a bunch of normally distributed variables with mean 0 and variance 1, then the sum of their squares has a chi squared distribution. T-Distribution: If you have a normally distributed variable with mean 0 and variance 1, and you divide it by the square root of a scaled Chi-Squared Distribution, it has a t-distribution. F-Distribution: If you have a chi-squared distributed variable and you divide it by another chi-squared distributed variable, it has an F-distribution. Why do we have all of these different combinations of the normal? To give our tests the properties we want. For example, if we want to see if average heights are the same, we'd like to take into account how confident we are of each sample average. So if we are checking if the heights are different in two samples of sizes five and six respectively, we take our difference in average height, which is normally distributed, and divide by how confident in sample averages from samples of size five and six, which has a chi squared distribution, and arrive at a t-distribution. This is the t-test. I won't get into all the reasons we use each of these distributions, as the specific reason you have to divide a chi squared by another chi squared, and thus use an F distribution is specific to each task. But hopefully this gives a bit of a start for tying these tests together.    Embed Quote