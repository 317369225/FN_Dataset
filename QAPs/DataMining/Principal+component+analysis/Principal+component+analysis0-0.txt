★How does one implement Principal Component Analysis?I am doing a minor project which involves reducing the dimensions of images using PCA.Therefore I need to know the algorithm used to implement PCA.
PCA is mainly  used  for dimensionality reduction.Its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high dimensional data space, PCA can supply the user with a lower-dimensional picture, a “shadow” of this object when viewed from its  most informative viewpoint. The main objective of PCA is to convert a set of observations of possibly correlated variables into a set of values of  linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance  and each succeeding component in turn has the highest variance possible under the constraint that it be orthogonal to (i.e., uncorrelated with) the preceding components.Mathematically,Given a data matrix X, where every sample is a m dimensional vector where m is the number of measurement types(variables), we  want to project X into lower dimensional space while preserving as much information as possible which can be achieved by  finding an orthogonal basis that spans the column space of  X. In particular, choose projection that minimizes squared error in reconstructing original data PCA makes one stringent but powerful assumption,linearity.Linearity vastly simplifies the problem by restricting the set of potential bases.With this assumption PCA is now limited to reexpressing data as a linear combination of its basis vectors. Let X be the original recorded data set and y is a new representation of the data set.Our goal is to find a linear transformation matrix P such as  PX = Y  s.t  rows of P are row of basis vectors for expressing the columns of X.these row vectors will then become the  principal components of  X. Now comes the question, what does “best express the data” mean .To extract valuable information from a signal measurement noise in any data should be low.A common measure for noise which quantifies it in terms of  signal is SNR(Signal to noise ratio) : SNR =   /    where is the variance.           A high SNR indicates a high precision measurement whereas a low SNR indicates very noisy data.Thus , by assumption the dynamics of interest exist along directions with largest variance and presumably highest SNR. Another key factor is the redundancy of data.For example,in a 2-dim case,for two arbitrary measurement types and ,  was it really necessary to record two variables or it would be more meaningful to just have recored a single variable,not both .Can we predict from or vice versa using the best fit line.This is the central idea behind dimensional reduction. Identifying the redundant cases  is easy for a 2-dim case (find the slope of the best-fit line and judge the quality of the fit).For higher dimensions this can be achieved by determining the covariances between the variables and defining the covariance matrix for X as where diagonal elements are the variance of particular measurement types,where large values denote interesting structure and off-diagonal terms are the covariance between  different measurement types where large values correspond to high redundancy.We have to manipulate our covariance matrix to say to achieve following two goals: to minimize redundancy which is meaured by the magnitude of the covariance and maximize the signal,measured by the variance.This can be achieved by diagonalizing   i.e to make all off-diagonal elements to be zero (Y is decorrelated) and by rank ordering each successive dimension in Y according to variance. While there are many ways for diagonalizing ,PCA selects the easiest method i.e to assume that all basis vectors { , ,…… } are orthonormal Therefore,P is an orthonormal matrix.The true benefit of this assumption is that there exists an efficient ,analytical solution to this problem. Given, the data set X(mxn matrix) where m is the number of measurement types and n is the number of samples.Mathematically ,the goal is to find some orthonormal matrix P in Y=PX such that is a diagonal matrix.The rows of P are the principal components of X.                      Let’s state some elementary linear algebra theorems which  will be helpful in formulating the above formula in a more meaningful way. The inverse of an orthogonal matrix is its transpose. For any matrix , , are symmetric. A matrix is symmetric if and only if it is orthogonally diagonalizable. A symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors i.e ( where D is the diagonal matrix and E is the matrix with eigenvectors of A arranged as columns)Now,we select matrix P s.t each row is an eigenvector of                               Therefore,the diagonal value of  is the variance of X along In practice,computing PCA of a data set X entails:subtracting off the mean of each measurement  type .(PCA doesn’t necessarily give a unique answer.For example if the data matrix is of Temperature,then it might give a different P for Celsius and Fahrenheit  to minimize this error ,mean is subtracted from each time series analysis we normally z square the data to achieve the same goal i.e subtrat the mean and divide by std. deviation) and computing the eigenvectors of    Embed Quote