★Why might Principal Components Analysis be bad to perform on a dataset prior to classification?
I don't think that it is bad in all cases, but I think that the reason it often hurts classification is that it removes variability without knowledge of the variables that are most useful in classification. PCA finds a lower dimensional representation of the data that minimizes the squared reconstruction error.  If you have irrelevant features (often the case in text classification), PCA counts errors in those with equal importance as errors in words that are important for your classification.  Taking an example like sentiment analysis, PCA's objective function weights errors on words like "July" and "baseball" as much as "good" "bad" and "excellent."  Since your classifier then is run on data that has a lot of the useful variation removed, it doesn't work as well. That being said, I think it does help in some cases.  It hasn't helped on anything I've done, but I think that it is standard practice when doing something like deep learning for image recognition.  In that case, it removes redundancy in the input representation, and the classifier is nonlinear and has the ability to use the compressed input successfully.    Embed Quote