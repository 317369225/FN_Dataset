★Principal Component Analysis: What is the intuitive meaning of a covariance matrix?If a matrix is a linear operator that transform space in a certain way, what transformation does a covariance matrix produce?
Matrices in general don't always have to deal directly with linear transformations. I think the best way to think about the covariance matrix is in terms of billinear forms. Roughly speaking, a billinear form is simply a function which takes in two inputs and returns an output, where the function is "linear" in both variables. Now, the function covariance is a bilinear form due to the identity: Now suppose we have random variables . Take the vector space of random variables spanned by these variables, as in random variables of the form: where the are real numbers. In general when you have a bilinear form on a vector space, given a basis for the space (in this case , unless some of your random variables are linearly dependent) then given vectors  , we can write: where the matrix encapsulates the pairwise inner products between your basis vectors through . Note that when our bilinear form is covariance, this matrix is precisely the covariance matrix! This is the main reason why we care about the covariance matrix. Now, onto the topic of linear transformations. Because the covariance of a random variable with itself is simply the variance of the variable and thus a non-negative real number, the covariance matrix must be a positive semi-definite matrix. If we agree to center all of our random variables to have mean , then the variance is zero if and only if you have the distribution and thus it is in fact a positive definite matrix. Furthermore as the covariance matrix is symmetric, it can be shown using the Spectral theorem that we can write: where is a non-singular matrix. This matrix is where linear transformations come into play. Essentially the matrix is what you get when you apply the Gram-Schmidt process to your random variables in order to make their variances all equal to . The matrix represents a linear transformation where you are moving from your basis of random variables to a new basis of random variables which have correlation between each other (though they are not necessarily independent) and all have variance .    Embed Quote