What's the best way to continuously scrape a large data source that is updated often?
The best way is to work with those companies and either get access to an API/data-dump or get their permission to scrape them. In most cases, scraping content for commercial purposes is no different (at least ethically) from redistributing copyrighted material. Additionally, by scraping a large site you might increase their workloads, resulting in worse response time (bad ux) and increased operational costs. There is a constant cold war between scrapers and large website operators, so your solution will likely break often.    Embed Quote 