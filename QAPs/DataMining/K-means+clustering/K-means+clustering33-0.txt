★What is an efficient algorithm for k-means clustering where k is 2 and the dimension is one, with or without weights?
This is an interesting special case where k-means can be optimized efficiently and in a globally optimal way.  Since the dimension is one, and you have two clusters, there will be a ``breakpoint'' at which all points to its left will belong to one cluster and all points to its right will belong to the other cluster. To find the optimal breakpoint, we can simply sort the data in increasing order, , then march through them one by one, deciding whether each point in the sequence is the optimal breakpoint. To compute this "breakpointiness score" for a given point , we need to figure out the sum of squared distances to the mean of all points to the left of and the sum of squared distances to the right of .  You'll notice that this is simply the score that ordinary k-means tries to minimize iteratively (sometimes called the "distortion"). At this point, you can elect to do this in the naive way, which would require linear time in the dataset size to compute a breakpointiness score per point, resulting in a final complexity of for k-means.  But, you can also notice that by caching running sums of the (and their squares), you can actually update the score in constant time per point, resulting in a final complexity of for finding the best breakpoint (though still in total due to the sort step). And then once you have these scores, you just return the clustering corresponding to the best (i.e. lowest) score, and that's all there is to it!  If you want an extra challenge, you can think about how to extend this "dynamic programming" trick to handle arbitrary numbers of clusters in 1-dimension.    Embed Quote