★How can we choose a "good" K for K-means clustering?Some datasets have an "obvious" set of clusters that would be found by K-means clustering with the right choice of K. How can we, given a set of data, choose such a K?
One approach that might be worth looking at is Bishop's variational Gaussian mixture model. In Chapter 10 of "Machine Learning" (specifically section 10.2.4), Christopher Bishop describes how the Bayesian formulation of a Gaussian mixture model (GMM) can be used to automatically determine the number of components needed to adequately "explain" the data. A GMM can be thought of as a soft clustering where each data point belongs to one or more clusters.  The amount that a data point belongs to each cluster is determined by a set of weights that are estimated using the GMM. The Bayesian formulation puts a prior distribution on the parameters of the model which enforce a tradeoff between the model complexity and the model fit. Bishop basically describes how the formulation can be used with "automatic relevance determination" to figure out which mixture components to keep at each training iteration.    Embed Quote