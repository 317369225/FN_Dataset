★What happens when you pass correlated variables to a k means clustering? Also, is there a way by which clustering can be used to group similar pattern observed for a variable over time?I have a data set in which there are different dimensions associated with a entity. Each dimension has a lifetime value, a duration value and metrics that capture its evolution over time. the goal is to cluster similar entities. 'Similar' meaning same pattern over time, same range of lifetime value and duration
Before I dive into this, I should probably warn that my experience with clustering is all on the job.  I've used k-means on a number of projects and have done a fair amount of reading about the algorithm's mechanisms, but I am not well versed in the academics and research in this area - so someone with a PhD might be able to give a more detailed and strictly accurate answer.  That said, I've encountered the problems you're asking about in practical applications, and have answered here based on my experience with k-means.  Here goes...! To answer the first part of your question: Whether passing correlated variables into a clustering algorithm affects your final clusters will depends on how correlated the variables are.  If the correlation coefficient is statistically significant, but of a small magnitude (say, less than 0.4), then each variable will likely exert some amount of individual affect on the final cluster arrangements.  If, however, the variables are highly correlated, you could probably drop one from the analysis - since the variables behave in a very similar manner, as far as the clustering algorithm is concerned using one variable will most likely be much the same as using the other.  To take an extreme case as an illustration, if you clustered your data using two variables that were perfectly correlated, then you would get the same output whether you used one or both of these variables.  Since the variables behave in exactly the same way, the vector "distances" that k-means will use to cluster the data with respect to each variable will be identical.  The less correlated the variables are, however, the more each will exert its own influence on these distances, and so influence the final clustering output. If your variables are only somewhat correlated, it might make sense to attempt three clustering solutions: the first with only one of the correlated variables, the second with only the other, and the third with both.  If the clusters you get are not very different, then you can eliminate one of the variables.  If they are, my personal preference in such a case would be to keep both variables, as each is likely exerting some amount of unique influence on the final clusters. One thing to keep in mind is that unlike a regression, a clustering algorithm is not estimating parameters to obtain a single, optimal "fit". Instead, it is essentially cutting up the data based on relative distances between coordinates in a hyperdimensional space, as well as the number of clusters that the analyst has identified as optimal.  The reason this distinction is so important is that a regression can be used to make inferences about the data.  Each parameter estimate in a regression potentially tell you something about the extent to which changes in an input variable are statistically associated with changes in the variable you are modeling.  If you include input variables that are highly correlated, it becomes harder to determine which (if either) is "truly" impacting the output variable (I use quotes because you'll never know for sure unless the data is based on a controlled experiment...and sometimes even then!).  In addition, including highly correlated variables can actually lead to a problem called variance inflation, in which the parameter estimates for correlated variables might be reported by the model as not statistically significant when, in fact, one of the estimates might be significant, if only one of the correlated variables had been included. Clustering, by contrast, if not affected by these problems, as it is not attempting to estimate parameters.  Rather than lead to variance inflation or similar difficulties, using correlated variables simply means the algorithm will have to navigate additional computational complexity.  If computational resources are not an issue, my experience is that it won't necessarily confound the clustering output to include more variables.  Though of course, you'll always want to evaluate the clusters to determine whether they make sense in the context of your data - especially since there are no hypotheses tests to tell you whether one clustering solution is more statistically significant than the other (though most software will give you one or two pseudo-statistics as a sort of guide). This is actually why you'll often hear clustering talked about as not really being Statistical Science in a strict sense - because there are no hypotheses to be proven or disproven using methods of statistical inference.  It is also why it is so important for you to know the data very well before deciding on a number of clusters - and why it is usually better to include fewer variables in general (since the clusters will be easier to visualize and evaluate). To answer the second part of your question: The answer to the second question is yes, this is definitely possible.  The first step is to nail down what you consider to be a "similar" pattern - specifically whether you're looking to cluster patterns based on normalized, directional changes (such as indices or percentage change) or on changes in raw volume - or both.  In the past, when comparing time-series data from two sources and trying to cluster patterns, I've indexed the data so that k-means is grouping based on directional changes relative to the mean of each series.  In this way, the clusters are based on the relative moment described by the variables, as opposed to volumes - particularly useful if the data you're evaluating represents, say, sales patterns in different states. Once you've normalized the data (if you've decided to do this), you'll need to create one input variable for each unit of time.  For example, if you have data that is monthly, you'd need to create one variable for each month.  These variables would then be passed into the clustering algorithm and used to create the final clusters. To illustrate, say you were trying to cluster 10 objects based on their monthly movement over the course of 24 months.  Then, you would structure your objects' monthly position data in such a way that each row in your dataset represents one object, and each column represent one month (with, of course, one column containing the object names).  You would then have 24 variables, each representing one month, which would be passed into the clustering algorithm.  K-means would then group together the objects whose 24 monthly values displayed the most similarities. Again, it makes sense to try different approaches - different variables, different #'s of clusters, etc.  The key here to create an input variable for every unit of time.  After that, it's essentially up to your discretion which of the clustering outputs you think makes the most sense.    Embed Quote