★While doing spectral clustering, why do we find the eigenvector of the graph Laplacian? Why not use adjacency matrix directly?
Aha I would like to answer this one since it confused me so much a few months ago. Absolutely the answer is "YES"! But using the adjacency matrix directly prefers to dealing with different scenarios compared with using the Graph Laplacian. An old paper "Normalized cuts and image segmentation"  on Page on princeton.edu included some discussion and based on the conclusions and figures I provide my answer. But firstly we should agree that instead of finding the smallest k eigenvectors as we do for Graph Laplacian, we should extract the largest k eigenvectors. You could simply scroll down to Page 39 in Section 6.1 and you saw the table like this: (notice that the definition of asso(A,B) and cut(A,B) is inherently the same, that is ) The table summarize the main information about 3 different  methods. The upper row shows their objective function (discrete) and the row below shows the eigenvalue system for computing the corresponding solutions. See? The equation from left down told you that you could used the (weighted) adjacency matrix (say W)  exactly and this is another kind of spectral clustering. One thing you should notice is the comparison between average association and average cut: average association:      (1) average cut:         (2)Obviously the only differences between  the objective functions from these 2 measures are the molecular.  Notice there are no guarantees that Intuitively, both the normalized cut and average cut algorithm are trying to find a "balanced partition" of a weighted graph, while on the other hand, the normalized association and the average association are trying to find "tight" clusters in the graph. The paper stimulate some scenarios to verify the three kinds of methods two and there's no need to copy the details here. One thing I didnot mention is that the way we do clustering is to use classical clustering algorithms (usually k-means) to cluster the eigenvectors computed from the eigenvalue equations and these are just approximation. And personally I think there are 2 main reasons that associate-based spectral clustering algorithms are not widely used as the cut-based ones: The approximation are not tight; The adjacency matrix are not always positive semi-definite which may cause the eigenvalues to be not real numbers. At last spectral clustering is not just a part including minimum cut or normalized cut. Indeed, all methods which  utilizes spectrums (eigenvalues, eigenvectors) are in the scope of this kind of algorithms but remember their performances depend on the scenarios to a great extent.    Embed Quote