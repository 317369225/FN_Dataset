★What is Gradient Boosting Models and Random Forests using layman terms?
We keep hearing three terms in this context and they are slightly different from each other but each is about building better learners by combining  individual weak learners.  Here is my understanding of the three: 1) Bagging or Bootstrap Aggregating:  If our training set is n x (m+1) with n rows and m features and 1 dependent variables, bagging will train multiple (weak) learners by training them on distinct random subsets of the n rows and then combine these learners to produce a final stronger learner.  The sampling to create training subsets can be with or without repetition.  So if I have five rows (n = 4) A, B, C, D, E I can train one learner using A, B, D and another C, C, E etc.  (essentially, overwhelm me less by using fewer rows at a time) 2. Random Forests:  Random forests take subsets of the columns and not rows.  So I can create my weak learners by ignoring some features,  If I have 7 features f1, f2, f3, f4, .. f7 (m = 7), I can train a learner using just f1, f2, f3, and f4 and another using f3, f4, f7, f1 and so on.  (essentially, overwhelm me less by using fewer columns at a time) In either case to create  the final stronger learner, I can a majority vote among the weak learners.  3. Gradient Boosting is substantially different from the above two.  While the above two methods, arguably, generate all weak learners in one shot, GBM is an iterative method.  Suppose after i iterations you have created your i^th learner F_i.  When you test your learner on your training set, the predicted value is y' instead of y.  Ideally, you would have liked y' to be same as y (and notw that y' and y are vectors of length n (one output for each row).  How about training a new learner h with attempts to predict (y - y') using the same training set.  If we could create h, then F_i + h will predict the original y correctly.  So i+ith iteration attempts to learn h and designates F_i+1 = F_i - h.  The process can go for ever or till you choose to stop. (essentially, train me better by focusing on the mistakes that I make (y' - y)) What makes the things confusing is that all three methods can be applied together.    Embed Quote