★What is setting random forests apart from other ensemble methods in Data Science?
My answer has to do with "At first glance, this appears as a run-of-the-mill ensemble application, using only standard techniques." and "the lack of algorithmic diversity". The reason why in RF are used the same tree model and not different diverse models is a statistical one. One can see Random Forests as a way to trade bias for variation. What I mean is that according with Bias–variance dilemma one can trade bias for variance. Decision trees are known to have small bias but large variance. What RF does is to reduce variance by allowing some bias by averaging. You can see how this works by understanding things like Law of large numbers. In order that averaging to work as expected, the models has to have the same distribution and to be independent. Independence is created by input variable randomization. The same distribution is induced by the fact that it is the same model. Of course is possible to "advance to the next step which could be algorithmic diversity" or "model diversity", however I believe that these models has at least to stay in the same family. But this is just my insight.    Embed Quote