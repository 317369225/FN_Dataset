★When are random forests better than linear regression?
Aside from performance issues, it is important to note that nonlinearity can manifest in many more ways than you seem to have in mind. A feature may be related to the output in a nonlinear way, but if the relationship is monotonic — e.g. something like , or with positive — a linear model can sometimes still give decent results. However, particularly if the relationship is not monotonic — e.g. something like or or with positive — then a linear model is very unlikely to give good results, and tree-based models will probably produce better results. My examples so far illustrate how tree-based models can fare better than linear regression when you only have one feature. When you have more features, then a simple example would be a case with two features f1 and f2: when f1 is small, the output is proportional to f2, but when f1 is big, the output is inversely proportional to f2. That is to say, the relationship between a feature and the output is conditional upon the values of other features. A tree-based model would be able to capture such a conditionality, but linear models simply cannot. In general: Tree-based models in principle can approximate functions with any "shape", whereas linear models can only produce functions with a linear "shape" with respect to a chosen set of features.    Embed Quote