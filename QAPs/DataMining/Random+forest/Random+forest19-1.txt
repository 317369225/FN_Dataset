★What is setting random forests apart from other ensemble methods in Data Science?
Since the basics have been covered in other answers, let me add a different analogy. To me, random decision forests are to classification / regression kind of what merge sort is to sorting. In merge sort, just about all of the action is in the merge. Though it divides up the input, the sorting problem in the base case is trivial. You could say random decision forests are all ensemble on top of trivial classification/regression. And strangely that is pretty strong: lots of weak learners add up to a strong learner. Simplicity lets you scale up the ensemble-ness extremely. That said, it need not be so. There is no good reason you have to use dumb learners in the leaves. For example, for regression, no need to use a point estimate in the leaves that is the mean value of the target; you could learn a little regression model in there. Or: decision boundaries don't have to be axis-aligned. Once you pump a little smarts back in to the trivial part, it gets even stronger. This is why RDF does unnaturally well.    Embed Quote