★What are reasons explaining Random Forest perform better than non-ensemble methods?
I wouldn't say that smaller samples of the training data are somehow less noisy, and I'm not sure what you mean about optimistic subsets (?) It's true of any ensemble method that you're building many independent classifiers and then choosing or favoring the one that happens to do best. So I agree with #3. One particular strength of random decision forests is that it is really doing feature selection for you, because it will by chance build trees on good features, and favor these over trees that were built on noise features. The decision rules at the leaves are not sensitive to outliers or small noise. I suppose the reason is somewhat the same reason that SVMs are robust to outliers. Only the values near the decision point really affect the decision point. For example the decision point for a numeric feature is typically half way between two actual values in the sample, and the rest of the values don't matter per se, only in that they are on one side or the other. The price of all this is that it achieves this through a lot of dumb brute force, building hundreds or thousands of trees. But processing is cheap today, if you can parallelize, and this is very parallel, so it ends up being a great way to throw cheap processing at the problem.    Embed Quote