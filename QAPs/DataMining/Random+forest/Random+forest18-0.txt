★Why is (scikit-learn's) random forest using so much memory?
I've run into a similar issue in the past. Are you attempting to use scikit-learn's parallelism by modifying the 'n_jobs' argument to the classifier (see the description in the documentation)? If so, you should be aware that your input matrix will be copied to every core on which you run it. So if you run 4 jobs, you're actually training on 4 full copies of the input data, which in your case will immediately eat up all available RAM. Something even worse occurs if you're trying to use grid search to fit your hyperparameters--your data is copied once per grid-point per job! Note that the 'pre_dispatch' argument complicates things even more (from that same documentation page): If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set pre_dispatch. Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs. Unfortunately, if you want to use python, I don't know of any other solid open-source implementations of random forests. There exist ML-as-a-service companies like Wise.io | Machine Learning as a Service & Big Data Analytics that will let you push data, train models, and pull results using python, and they might be free for academics (Joshua Bloom's answer to Classification (Machine Learning): What are good libraries for random forest?). If you're not tied to python, there are lots of great implementations of random forests in many other languages: I'd recommend Weka, which is a Java library (and I believe there's an R binding for it as well).    Embed Quote