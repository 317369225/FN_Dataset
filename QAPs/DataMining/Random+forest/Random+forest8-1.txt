★How do random forests and boosted decision trees compare?
I'll let Rich Caruana's & Alexandru Niculescu-Mizil's exhaustive & well-cited benchmark paper speak for me [1]: "With excellent performance on all eight metrics, calibrated boosted trees were the best learning algorithm overall. Random forests are close second, followed by uncalibrated bagged trees, calibrated SVMs, and uncalibrated neural nets. The models that performed poorest were naive bayes, logistic regression, decision trees, and boosted stumps. Although some methods clearly perform better or worse than other methods on average, there is significant variability across the problems and metrics. Even the best models sometimes perform poorly, and models with poor average performance occasionally perform exceptionally well." Calibration here refers to Platt's method or Isotonic regression, but earlier in the paper the authors point out that Random Forests don't really need calibration! So, while (calibrated) bosted decision trees are the best on average, I tend to use Random Forests (if feeling too lazy to validate more than one model) because they are straightforward, robust, and output good probabilities right out of the box. Edit: oh and another thing.  Beware, beware of boosting!  It can get pretty messed up by outliers or noise. [1] An Empirical Comparison of Supervised Learning Algorithms    Embed Quote