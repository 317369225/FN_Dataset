★What is setting random forests apart from other ensemble methods in Data Science?
Good question. Theoretically, it makes more sense to employ a lot of algorithms and average them together. However, there are a few practical reasons why Random Forests have become so popular in the last few years: Preprocessing is trivial. Decision Trees don't care whether your data is discrete, continuous, or even contains character values. They don't require you to scale all the variables to have a mean of 0 and a standard deviation of 1. You can literally write 10 lines of Python code to come up with a classifier that's 97% accurate on MNIST using Random Forests. Regression and classification are both easy. Often times it's not entirely clear when you want to use regression or classification, particularly with a 0/1 problem like detecting illness. Luckily, with Random Forests you don't have to choose-the same dataset can be fed into a Regressor and a Classifier. Forests are easily configurable. Too much overfitting? Just use some extremely randomized trees. Want to know variable importances? Surprisingly easy. Forests are extremely parallel. Bootstrapping in general is easy to parallelize, and there are plenty of modules where you can literally set the # of cores you want your forest to use as a parameter. In contrast, it can be significantly harder to parallelize an SVM or a Boosting-based method. Forests perform really well. From practice, I've found that a random forest thrown together in 5 minutes often outclasses other, painstakingly tuned algorithms. That's because bootstrapping works best with very unstable classifiers that overfit easily. Decision Trees fit this to a T, while a lot of other methods (linear regression, k-NN, SVMs) don't benefit nearly as much. In short, the combination of ease-of-use and high performance makes Random Forests the ultimate 80/20 solution: you'll get rather high-quality models with very little work.    Embed Quote