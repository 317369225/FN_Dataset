★When would one use Random Forests over Gradient Boosted Machines (GBMs)?
Different machine learning problems have different characteristics. Boosted algorithms in particular are sensitive to overfitting if the data is noisy -- boosting exhibits higher variance (in the sense of the bias-variance tradeoff discussed here: http://en.wikipedia.org/wiki/Sup...), but when it does work, it works well. Random forests, on the other hand, show very low variance, because the ensembles are not built on the residuals the way Gradient-boosted Machines does. In short: you have to try both out on your problem domain; sometimes random forests will match the data's biases better and sometimes GBM will.    Embed Quote