★How does random forest work for regression?
I think the first step would be to understand how decision trees work in a regression problem. You might be aware of CART - Classification and Regression Trees. When dealing with regression problem you try to predict real valued numbers at the leaf nodes which would look something like this for singular scale feature: Now the question comes how they work. Well in a standard classification tree, the idea is to split the dataset based on homogeneity of data. A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). On the other hand, in a regression tree, since the target variable is a real valued number, we fit a regression model to the target variable using each of the independent variables. Then for each independent variable, the data is split at several split points. We calculate Sum of Squared Error(SSE) at each split point between the predicted value and the actual values. The variable resulting in minimum SSE is selected for the node. Then this process is recursively continued till the entire data is covered. When you try boosted regression trees you get something like this Random forests can be used for regression analysis and are in fact called Regression Forests. They are an ensemble of different regression trees and are used for nonlinear multiple regression. Each leaf contains a distribution for the continuous output variable/s. Good resources: Decision Tree Regression    Embed Quote