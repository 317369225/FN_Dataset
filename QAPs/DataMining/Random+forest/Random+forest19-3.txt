★What is setting random forests apart from other ensemble methods in Data Science?
Random Forest is a method that  basically runs decision tree's on a collection of small random samples. Small random samples, while less representative of the population as a whole , can bring out the extreme cases of  the dataset to light better than large datasets. When several of the small random samples are combined they can also isolate the signal from the noise better. This is why I believe that Random Forests methods perform better. The central limit theorem and large samples are good at finding symmetry's in the dataset , but , if you are interested in finding asymmetries you really need to tap into smaller datasets. Lets say you have an urn of 100 coins. Now the probability of getting all heads is significantly less when you draw from the urn of 100 coins than it is if you draw from two urns of 50 coins each. i.e Extreme outcomes are more likely with smaller numbers. The book , Thinking, Fast and Slow: Daniel Kahneman: , explains the law of small numbers quite well. In the book there is an example where the gates foundation found that small schools perform better than large school. But , small schools also perform much worse than large schools. So - if your interest is to find the factors that "move the needle" on the margin , you need to listen to small datasets. Here is another link that explains the same problem - BELIEF IN THE LAW OF SMALL NUMBERS    Embed Quote