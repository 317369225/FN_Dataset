★What are the reasons of overfitting while using the Random Forest algorithm?
Broadly the reason is the same as for any algorithm: fitting noise instead of signal. In decision trees, this happens when the trees are too deep. If you just kept going, eventually the tree would have a node for every distinct point and it turns into a form of 1-nearest-neighbor classifier. This fits the training data too closely and is unlikely to generalize. This is why there is usually a stopping or pruning criteria. It could be a minimum node size, such that nodes with <= N examples are not split further, or a minimum information gain, such that nodes that have no decision that decreases entropy more than a trivial amount are not split.    Embed Quote