★How do I find variable importance in random forest?
Are you asking how this is computed? As far as I know, the R randomForest package (CRAN - Package randomForest) follows Breiman's original paper (Random Forests). There, you compute variable importance by computing out-of-bag error in the normal way. Then, you randomly mix the values of one feature across all the test set examples -- basically scrambling the values so that they should be no more meaningful than random values (although retaining the distribution of the values since it's just a permutation). Then you compute out-of-bag error again. If error increases a lot, that feature was important; the real values in the right place lead to better predictions than if they were meaningless. The change is the measure of feature importance. scikit-learn does something simpler and more efficient, although I'm not aware of the theoretical basis for it. It certainly has intuitive appeal. It pushes the entire data set down the trees and counts the number of times a data set passes through a node whose decision is based on a given feature. Features that appear often and high up the tree end up with high counts.  And that suggests the features were frequently chosen as providing the best split. At the least, it tells you that, empirically, the trees you generated consult that feature frequently for better or worse. This latter approach makes most sense to me if you are evaluating many features every time you choose a split. It also seems to make more sense if weighted by the info gain at the node. This is what I have implemented in for example oryx.    Embed Quote