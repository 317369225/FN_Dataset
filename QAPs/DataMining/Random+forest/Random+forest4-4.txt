★When is a random forest a poor choice relative to other algorithms?
Random Forests, like all decision trees, provide a discrete output because there a finite number of leaf nodes. Extrapolating between the discrete values takes additional effort. For example, if you have a continuous response variable, the Random Forest may only give you 50 distinct values. In contrast, linear models typically provide an equation where you have a continuous range of values. Personally speaking, I haven't found this drawback to be a problem. Another disadvantage, which again is true of all decision trees, is the danger of overfitting when you have many high-cardinality categorical variables. For example, let's say you have two categorical input variables, one with 50 distinct values and another with 80 distinct values. Well 50x80 = 4000 unique combinations. Chances are, a few of those combinations will look really good in the model because it's fitting a few specific cases.    Embed Quote