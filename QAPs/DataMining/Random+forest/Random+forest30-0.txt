★What is Gradient Boosting Models and Random Forests using layman terms?
Gradient Boosting Model (GBM): Imagine  20 teams (trees). A boss at the top, then subordinates, then more  subordinates, and so on. Team members are explanatory variables. Assume,  Trees = 20 and Depth (number of members in each team) = 5. So each team  will have 5 members, and total members = 100. We give them a book to  read, and then they will have to answer 20 questions (Number of  observations in our data). Assume they have binary answers: Yes or No.  Now, we start the process. The aim of the process is to maximum correct  answers by building 20 teams having 5 members each. Any member can be a  part of more than 1 team, and any member can have more than 1 more than 1  role in same team. The member which have maximum roles is the most  important variable of our model. The  process starts with a random guess of answers. Then it calculates error  ( = Actual - Predicted Answer). Next step, it build a team of 5  members, which reduces the error by maximum. Again, it calculates the  error. The second team (tree) has to reduce it further. But next team  doesn't trust its previous partner fully, so it assume that answers are  correct with x probability (learning rate). This process go on till 20  teams are build. So in the process, we have to decide, how many teams to  build (trees), members in each team(depth) and learning team, so that  error in the end is minimum. This can only be done by trial and error  method. Random Forest: Recall the show "Who Wants to Be a Millionaire". We observe that the accuracy of answers in "Phone a Friend" is less than that of "Audience Poll". This explains the rationale behind the random forest. Audience Poll - It averages the answers of weak learner (each learner being a tree), and that gives you a learner which is stronger than a sing learner "Phone a friend" - supposedly a strong learner. Here, each team is independent. We have to decide number of teams. Each team learns by trying to answer a sample of questions, then generalizes its learning over all questions. Also, all members are not considered while building a team. A Random Sample of members are considered. then we take the average of answers by all teams to get final answer. This method can only be used when the answer is binary. GBM can be used questions having more than 2 answers. Also look into this video: Vinod Gattani's answer to Random Forests: How do random forests work in layman's terms?.    Embed Quote