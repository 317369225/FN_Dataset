★Random Forests: How do random forests work in layman's terms?I've read the wikipedia article, and the bioinformatics article on the Random Jungle implementation of random forests is informative, but still doesn't give a decent, less-technical explanation of what RFs are actually doing. So how would you explain random forests to a non-computer scientist? I'm specifically interested in how RFs do regression problems.
Unfortunately, I've seen some of these comments describe Random Forests as building 'stumps'. They do not; this is the domain of boosting and stochastic gradient boosting for trees. Random Forests are ensembles of decision trees that do two things to help them find patterns in data: 1) random record selection each tree is built from a separate random sample of the data using bootstrap sampling. So you can build hundreds of trees each from a separate bootstrap sample of the data. So we gain diversity from examining different examples in the data. By convention we overfit the data extensively. 2) random variable selection in a standard tree, each split is created after examining every variable and picking the best split from all the variables. Random Forests selects a random subset of variables for *every split*. By convention, the number of variables to consider is sqrt(M) where M is the number of candidate inputs. So if you have 100 input variables, you select a random 10 of those for every single split. This forces the trees to find alternative ways to predict the target variable. By building trees from random records and random variables, the greedy search aspect of trees can be overcome and obviously is overcome because RF works very well on a wide variety of data. This doesn't answer the question posed in this thread though. I'll think about that and maybe answer that later.    Embed Quote