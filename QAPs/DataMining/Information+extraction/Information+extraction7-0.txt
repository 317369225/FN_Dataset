★Why do we need deep learning for open information extraction?
a) Because, real problems are not linear and even more intricate to be solved with simple linear approximations or optimizations. What DL does is  unfolding, (affine transformations) instance space by each layer of units as much as the different classes are being linearly separable.  KEEP THIS ARGUMENT FOR YOUR SECOND QUESTION.  As you pointed out, there is also unsupervised NNs. The idea of unsupervised NNs is to learn compressed (lower units at the layers than the number of inputs) or elongated (higher units at the layers) but sparse representations that gives the best possible reconstruction of the given entity.  In both cases, we try to learn new set of representations out of the given data, that is sensitive to different commonalities. For being an metaphorical example,  given a set of car and cat images, some set of units are activated by the car's tires or doors and another distinct set of units are activated by the limbs of cats. Hence, if we intend to classify these two categories at the later stage, this unsupervisely learned representations are better start points compared to the random weights assigned for the supervised fine-tuning phase. Incidentally, these representations can also be used for out of the box tasks to extract features from totally different set of images since even the images are from different sources, structurally car is the same car and cat is the same cat. However, one caveat is the generalization constraint of the learned network but it is beyond the scope of your question. from Neural Networks, Manifolds, and Topology b) Backpropagation is the state of art training methodology of supervised NNs. Recall the note from the first question. Backpropagation is the way of guiding each unit to change weight values to untangle the instance space so as to make different classes separable. It is basically an application of "Chain Rule" in Derivational Calculus. You compute the prediction loss at the output units, compute the gradients and flow them back to preceding units (Neural networks and deep learning). c) I am not sure  what you mean by open information extraction but from the remaining details, I understand that you try to do knowledge discovery like the Google's Knowledge Graphs. This is one of the new trend in DL based on Tensor Networks in which basically performs Matrix Factorization.  Tensors are the generalizations of the matrices to higher dimensions. Then this makes as able to discover relations between entities by cascading input matrices. For instance, if we have a instance matrix which includes the words as rows and columns as the representation, then we can add more matrices successively and relate each item in one matrix to other items in the other layers. For more detail : Page on stanford.edu    Embed Quote