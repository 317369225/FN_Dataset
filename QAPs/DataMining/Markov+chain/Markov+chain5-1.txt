★What is an intuitive explanation of Markov Chains?
An example of a Markov model in language processing is the concept of the n-gram.  Briefly, suppose that you'd like to predict the most probable next word in a sentence.  You can gather huge amounts of statistics from text.  The most straightforward way to make such a prediction is to use the previous words in the sentence. For example, given the sentence, "The cat chased the ____", suppose you'd like to predict the next word.  Well, you could consider all of the words that have appeared up until that point in the sentence, and chose the most likely next word.  Or you could just look at the last one, ignoring everything that came before it.  The fact that you're ignoring events before a certain point in the past gives such a model the Markov property. A Markov chain is a sequence of (probabilistically reached) states that obey the Markov property.  There are a number of different kinds of Markov chains, but I'm going to focus on the most commonly known. Again, suppose that you'd like to predict the weather on day n.  A Markovian approach would consider only day n-1 in order to make such a prediction. This is typically described as being memoryless.  Markov chains don't have to be memoryless.  An mth order Markov chain only considers ("remembers") the last m states.  As such, a memoryless Markov chain is an mth order Markov chain where m = 1.  There are a several ways of formalizing Markov chains, the most popular being probabilistic finite state machines and matrices. If you're familiar with FSTs, you can think of each state as a possibility in the universe.  If A and B are states in the FST, there's an arc (transition) from A to B.  Unlike with traditional FSTs, these arcs have probabilities associated with them.  These probabilities represent the probability that A transitions to B.  (If the probability is 0, we usually don't draw the arc.)  Therefore, all of the arcs from A to every other state must sum to 1.  (In a traditional deterministic FST, one can think of every drawn arc as simply having a probability of 1.) So, you can imagine moving through this finite state machine.  Every time you move to a new state, you flip a biased die (based on the transition probabilities).  Based on the result, you move to the next state.  Since each state only depends on the previous state -- as FSTs are memoryless -- you are moving through a Markov chain. In the language modeling example, the chain is the sequence of events that led to the production of each word in the sentence.  The word "The" produced "cat", which produced "chased", which produced, "the."  This is moving through a Markov chain.  You can also compute the probability of the entire sentence by multiplying each of the individual probabilities:  So, you compute the probability the "chased" produced "the", the probability that "cat" produced "chased", etc., and multiply them all together.  This is the probability of the entire chain of events  (where the "generation" of each word is an "event".)  Even though we don't usually think of each word in a sentence as occurring at a discrete point in time, this is the nature of a discrete stochastic process (of which the Markov processes are a subset.)    Embed Quote