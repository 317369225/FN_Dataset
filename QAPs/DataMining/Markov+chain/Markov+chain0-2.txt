★What is a Markov chain?
The Markov chain seeks to model probabilities of state transitions over time.   The ink drop in a glass of water example: Fill a clear glass half-full with pure water. Let it sit for a while...  Then drop one drop of India ink into the water. You'll observe the ink gradually disperse throughout the glass. At some point in time, the liquid in the glass is a uniform color, representing an equal distribution of ink particles throughout the glass. That's the point at which the Markov chain has resolved from its initial state into a steady state. Some Markov chains don't resolve. You have Brownian motion at the molecular level, which is modeled as a Random Walk process, and the interior walls of the glass, which represent boundary conditions. The Roulett Wheel example: If a Roulette wheel has 37 pockets that the ball can fall into, then the Markov chain represents 37 different state transitions. For clarity, visualize the Markov chain as a square matrix, with 37 rows and 37 columns. The intersection of each row and column represents a state transition. In a normalized Markov matrix, the values in each row sum to 1.0. There's a special case where the values in each column also sum to 1.0, which turns out to be quite useful when modeling certain classes of real-world events. If the Roulette wheel is a truly random game, then each state transition has the same probability (1/37), such that the probabilities of all possible state transitions sum to 1.0. In this case, you can't beat the wheel. However, if observation/history detects a slight bias, such that the state transition from Red-1 to Black-33 has a slightly higher probability than from Red-1 to Red-3, then it is possible that your Markov model may resolve, and that you could "beat the odds".    Embed Quote