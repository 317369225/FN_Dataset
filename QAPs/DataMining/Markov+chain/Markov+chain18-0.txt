★How do you prove that a Markov chain is ergodic?Note that ergodic Markov Chain = irreducible Markov chain (with no transient states)
Here are three approaches: On a Markov chain that is simple enough to reason about, you can just argue that it's possible to get from any state to any other state. This will mean that all states of the Markov chain are recurrent and thus the chain is irreducible / ergodic. On a more complicated aperiodic Markov chain given the transition matrix, you can find a power of the matrix (let's call this n) such that all entries are non-zero. This means that in n transitions, every state is reachable with positive probability from every node. If the Markov chain is periodic with period m, you'll can still do the same approach above but you'll need to look at m consecutive powers. (if you're interested in a general reference on Markov Chains, check out The Only Probability Cheatsheet You'll Ever Need)    Embed Quote