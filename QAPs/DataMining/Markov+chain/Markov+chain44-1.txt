★Has anyone used Markov Chain Monte-Carlo methods to generate art in the style of particular modern painters?
Michael's answer is a pretty good summary. What follows is another way to think about this and Markov Chain Monte Carlo (MCMC) in general. Generating something coherent out of some flavor of MCMC requires that whatever unit forms the basis of your reconstruction be sufficiently high-level. The reason for this is clear when we think about what MCMC is really doing. When generating text, Monte Carlo methods usually do something similar to this: Pick a starting "token" somehow. Until the thing we're generating is "done", pick a new token based on all of the tokens we've generated thus far.When generating writing in the style of an author, the "tokens" are words, and we usually just generate a fixed number of words or sentences. (That is, we define "done" to mean "having the number of words we want.") When the writing is based on that of a certain author, we match his or her style by using what we know about it to do step 1 and 2 the way he or she would. For example, we might choose the starting word based on the distribution of starting words the author used. (E.g., if the author used "The" as the first word in 45% of his or her books, we'll do so with that probability, and so forth.) Then, when generating each word, we're trying to answer the question, "what would the author write next, having written what he or she wrote thus far?" Markov Chain Monte Carlo is a special case in which we pick the thing to write next by looking at the last N words, for some N, and picking based on the distribution of next words the author chose when in that situation. For example, with N=2, if the last two words we wrote were "the book", we'd look at every time the author's literature contained the two word phrase "the book", pick one at (uniform) random, and choose the following word as our next word. Let's ignore for a second that choosing words like this doesn't give your sentences any kind of grammatical structure. The next big decision you have to make is what N to use. When N is larger, each word you choose has more context, but you're choosing from a dramatically smaller pool every time. (To color your intuition, let's say the author uses K different words at roughly the same frequency. This isn't realistic, but this is just for intuition's sake. The percentage of the author's N-word phrases which match the one you're looking for, each time, should be about , which decreases exponentially in N.) To give you an idea, Google released the data on a quadrillion-token dataset, and there are a number of relatively common three- and four-word phrases that appear a shockingly small number of times [1]. If you started looking for five word phrases (i.e. N = 5), you'd find yourself looking at a pool of 10 or fewer ocurrences of your previous 5-word phrase almost every time. The problem with this is that your sets of 5 words will start chaining together. That is, you'll hit a point in which a certain 5-word phrase appears in only one place, so you end up taking the sixth word in that chunk of text, and the second-through-sixth words appear only there, so you take the seventh, and so forth. When this happens, you aren't generating a new text in the voice of an author; you're really just printing out big chunks of his or her work. (We call this the "sparsity problem," because the distribution of 6th words for a given 5 word phrase is zero for almost every 6th word, or is very "sparse." There is a whole bundle of fascinating research on how to smooth this distribution. [2]) When N is this low, though, you're choosing words with so little context that the phrases you get make basically no sense. Imagine trying to write a book with a memory so bad that you could only remember the previous two words and had absolutely no other sense of context. If we tried to do this with letters instead of words, we'd need N to be a little higher, or else even the "words" we generated would be gibberish, but if N is too high, we'll start to run into the same sparsity problem and we'll end up taking big chunks of text directly from the author. We can make text that at least vaguely feels like that of an author, though, because 2 or 3 words is enough context that the phrases we generate seem vaguely like the author's and the words we choose are similar in frequency to the author's. Adapting this to painting breaks down in two important ways. First, there is no obvious unit that we can repeatedly generate based on the work of our artist. Secondly, there is no obvious order in which we should generate that unit. Using something like paintbrush strokes is akin to imitating an author by generating letters instead of words; if N is too small, we'll get a bunch of strokes with very little context, and the resulting painting won't look like anything, and if N is too large, we'll start copying paintings stroke-for-stroke, and it probably happens that any N is either too large or two small when you use strokes. As for the second limitation, there isn't an obvious order for the strokes. Generating text, we can just go word by word, but "stroke-by-stroke" doesn't work quite the same way. (Do we go from top left to bottom right? Can the strokes overlap?) If we could extract something a little more high-level, like objects or settings or some other sort of feature of paintings, we could make more interesting imitations, or at least ones that didn't look like visual gibberish, but we'd be cutting objects verbatim from the source pictures, so the MCMC abstraction doesn't apply particularly well in this case, either. [1] See: http://googleresearch.blogspot.c... [2] See the language modeling portion of: http://www.nlp-class.org/    Embed Quote