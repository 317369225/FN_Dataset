★Which feature selection approach typically yields a more accurate (in out of sample performance) model: black box or intuition (more detailed explanation of question inside)?
(1) is going to be closer to the truth, but don't think that people don't try (2) either. I once had a professor who called "data mining" "data criming". You asked for the pros and cons of each, though, so here goes. (1) Intution Cons: Go find me this team of experts. I'll be waiting. Sometimes, you are the expert in your field. Lots of people/research groups/companies are using machine learning to tackle problems that people literally haven't tried solving before. You're painting this picture as being a little bit chained to the will of the experts, and only being able to test models that include variables which the experts "approve" of. I'm not sure I agree with your assessment, but it certainly goes in this column. Pros: Explaining why your model is correct/viable should be relatively easy. And if it isn't, well you've got some experts on hand to explain it to you. Typically your model is also going to be small, which lends itself to Occam's Razor. You trade away some degree of predictive accuracy and gain credibility. It's ok that the precipitation outside and the day of the week doesn't predict the temperature 100% of the time. You just haven't added "Month" to your model yet, and that's OK. Someone else will, and they'll get better results, and they'll publish that, and we'll all be better for it. (2) Black Box Cons: There's a lot of data out there. Literally infinite amounts of data, and people are recording gobs of it faster than ever. Does Facebook think that your buying habits might be influenced by the sexual orientation of your siblings, the day of the week you were born on, and whether or not your favorite artists smoked cigarettes? Who knows what that model would show, but by God they've got the data to try it out. As a general rule, any two variables that don't have a perfect 0 correlation coefficient will have some predictive power on each other. If I conflate enough of these together, I can make completely unrelated variables make predictions that just tend to be correct sometimes, when we know that it is simply pure dumb luck and a little bit of criminal statistical negligence. As another general rule, if the number of parameters (variables) in your black box model exceeds the number of data points you get this hilarious thing called overfitting. For a visual demonstration, whip out your TI-84 calculator and look at the difference between plotting a linear regression of 4-5 points and then plotting the polynomial regression. I leave this as an exercise to the reader: tell me which one tends to predict the trend better, and which one is just a wavy bunch of curves that ends up diverging in the wrong direction 50% of the time? Pros: When you have the right training set, the perfect test set, the planets align and the Mayans were right about 2012, you'll get perfectly accurate results. Here's what I call Option #3. Your boss says "Hey, I've got a problem that I need you to solve with a machine learning algorithm. Your target is [insert type of problem here] and I've cooked up a dataset that you can use." I suppose you can count yourself lucky if you get the data handed to you, and don't need to scrape it yourself. Now, the part that gets inserted into the brackets depends on what you're trying to do. Clustering data points into groups, regressing data points to make predictions, etc. I'm a fan of classification algorithms myself. Let's take classification as the problem and the dataset is weather, kind of like the problem I mentioned above. I'm going to try to predict the weather outside, and it's going to be one of {Sunny, Snowy, Cloudy, Rainy, Windy}. If I just randomly guess the weather, I should be able to get a baseline 20% accuracy, but let's aim for 80% accuracy just for kicks. Now, let's get down to business. My boss hands me a dataset. He called some favors and pulled some strings. It's the local weather report, recorded in 200 U.S. cities, every hour (on the hour), every day, 365 days a year, for 6 years. This dataset has 10 million rows in it. Now here's the question - did my boss get me a good dataset, or a painful one? A good dataset would have some nice, relevant columns, like temperature, dew point, relative humidity, wind speed, maybe even wind direction, and preferably the pressure. However, you and I both know that datasets aren't going to limit themselves to that set of good variables I can take home and introduce to my parents. Instead, it's going to start with heat index. Ok, that's acceptable for my dataset. Then comes wind chill. Next is "temperature in centigrade". Wait, isn't that one just a conversion of "temperature"? Yes! It is! Now our dataset isn't linearly independent, hooray! And the columns just keep piling on. Then there's height above sea level, and whether or not the temperature station is an Air Force Base (because that's important), and whether or not it's a naval station (those may or may not be mutually exclusive). Then there's the number of hurricanes in the Atlantic Ocean minus the number of typhoons in the Indian Ocean. Why is that a column, I DO NOT KNOW. Then there's the number of degrees the Earth's axis is tilted. The entire column is just 23.4 repeated 10 million times. Then there's the number of earthquakes recorded in the previous day, and sales of Target-brand sunglasses. Now I'm not even sure if this set was taken from the National Weather Service. Do you see why the "black box" method you described is downright dangerous for statistical inference? You say that the machine can do proper feature extraction but do you really trust it? I once did an analysis where the starting data set had more than 250 variables. Half of them were binary. Will you really trust the machine to make the right decision there? Just because it's right more often doesn't mean it's more accurate, or a better predictor. You might just be feeding it shoddy data. I'll leave this link (@Neural Network Follies by Neil Frasier) as a prime example of how an unrelated coincidence in your data can completely ruin your machine learning experiment.    Embed Quote