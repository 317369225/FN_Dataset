★How do I perform feature selection?
Oliver and Shameek have already given rather comprehensive answers so I will just do a high level overview of feature selection The machine learning community classifies feature selection into 3 different categories: Filter methods, Wrapper based methods and embedded methods. Filter methods These include simple statistical test to determine if a feature is statistically significant foe example the p value for a t test to determine if the null hypothesis should be accepted and the feature rejected. This does not take into account feature interactions and is generally not a very recommended way of doing feature selection as it can lead to lost in information. Wrapper based methods This involves using a learning algorithm to report the optimal subset of features. An example would be how RandomForest is widely used by the competitive data science community to determine the importance of features by looking at the information gain. This can give a rather quickly and dirty overview of which features are important which can help provide some informal validation of engineered features. Tree based models like RandomForest are also robust against issues like multi-collinearity, missing values, outliers etc as well as being able to discover some interactions between features. However this can be rather computationally expensive. Embedded Methods This involves carrying out feature selection and model tuning at the same time . Some methods include greedy algorithms like forward and backwards selection as well as Lasso(L1) and Elastic Net(L1+L2) based models . This will probably require some experience to know where to stop for backwards and forward selection as well as tuning the parameters for the regularization based models.    Embed Quote