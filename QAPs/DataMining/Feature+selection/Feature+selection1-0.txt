★How do I perform feature selection?
The simplest method is probably Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores; see the scikit-learn documentation for the list of commonly used statistical tests:     http://scikit-learn.org/modules/... A more robust but more "brute-force" method where the impact of combined features are evaluated together is named "Recursive Feature Elimination": First, train a model with all the feature and evaluate its performance on held out data. Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining features. Iterate until you observe a sharp drop in the predictive accuracy of the model. Here his an example, again using scikit-learn:     http://scikit-learn.org/auto_exa... And here is the implementation:     https://github.com/scikit-learn/... If you want to train a (generalized) model be it for classifications (e.g. logistic regression) or regression (e.g. ridge regression a.k.a. l2 penalized least square), you should consider adding a L1 regularizer that will promote feature selection *while* learning: coefficient for the weakest features are set to zero by the learning algorithm itself. The usual name for L1 penalized linear method is "the Lasso":     http://scikit-learn.org/modules/... The optimal value of the strength of the regularizer can be found by cross validation. To do this efficiently you can either use the LARS method as described here:      http://scikit-learn.org/modules/... LARS allows to compute the values of all the coefficient for different values of the penalization strength (a.k.a. the regularization path) very efficiently:       http://scikit-learn.org/auto_exa...  It is also possible to compute a regularization path efficiently with other algorithms such as Coordinate Descent (and maybe also Stochastic Gradient Descent) using the "warm restarts" trick:       http://scikit-learn.org/auto_exa... To account for the instability of Lasso when dealing with highly correlated features, you should either consider combining the L1 penalty with L2 (the compound penalty is called Elastic Net) which will globally squash the coefficients but avoid randomly zeroing one out of 2 highly correlated relevant features:     http://scikit-learn.org/auto_exa... Stability in feature selection models for the pure Lasso can also be achieved by bootstrapping several Lasso models on dataset folds and selecting the intersection (or union, I am not sure) of the non zero-ed features. This method is called BoLasso and is not yet implemented in the scikit (pull request always appreciated): the reference paper is:     http://www.di.ens.fr/~fbach/fbac... Other ways of "fixing the Lasso" (taken from F. Bach NIPS 2009 tutorial): adaptive Lasso (Zou, 2006), relaxed Lasso (Meinshausen, 2008), thresholding (Lounici, 2008), stability selection (Meinshausen Buhlmann, 2008), Wasserman and Roeder (2009) Edit: stability selection has been implemented in scikit-learn more recently. Finally if you are in a Bayesian mood, smart priors such as the one used in Automatic Relevance Determination (ARD) should give you similar results:      http://scikit-learn.org/modules/...    Embed Quote