★How effective is it to use simple measures of correlation in feature selection?
The answer to this question greatly depends on the characteristics of the data, but more often than not, if your feature set is small (20-30 is quite small), from my experience it's rarely worth the trouble to perform any feature selection... at least as a first pass analysis. But to answer your first questions, using correlation or (linear) regression coefficients to identify significant predictors for your outcome variable is an acceptable and common practice for feature selection. The main difference between the two is that you'll probably end up with more predictive features using correlation, since it effectively standardizes your variables (and thus you lose information). You should also keep in mind that if you suspect interdependence between your features, you should model that as well by including some product terms. Indeed, variable ranking is then the standard next-step method for detecting outliers that probably won't contribute enough to classification. As an aside, it's often generally useful to perform that regardless, in order to get an initial idea of what results to expect. However, there are some criticisms for variable ranking, as it leads to selecting redundant features and it can also exclude individual variables than might actually be useful contributors to patterns in multivariate classification. Thus, it may be better to use a method that can select subsets of variables that together have good predictive power rather than focusing on the ranking of predictive power at the level of individual variables. Wrappers and embedded methods are some approaches used to address this problem.    Embed Quote