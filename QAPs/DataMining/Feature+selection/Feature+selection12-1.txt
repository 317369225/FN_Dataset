★What are some general tips on feature selection and engineering that every data scientist should know?In most of Kaggle competitions the participants talk about selecting what they call Golden features and how it is more important than selecting the classification algorithm. So how can I derive such good features and what are some good measures for assessing them?
You could look at each individual feature and see how well they correlate with the classes independently using some ranking metric. For instance, one particular metric could be Pearson's correlation: Where x are the values that one feature takes on for multiple observations and y are the corresponding classes. 1 is high correlation, 0 is none. If you apply this metric to all your features it then ranks them, and then you can select a subset of features (i.e. the top 10% ranked features) and look at the performance. You could plot the accuracy of different subsets to find the ideal amount of features to keep. This is done when you have several features (time series, DNA) You could also do this ranking with the classifier itself, since it finds weights and those weights could be considered a ranking. You can create new features by using principal component analysis (PCA) on your design matrix.    Embed Quote