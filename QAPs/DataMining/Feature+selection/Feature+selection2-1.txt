★What is the difference between principal component analysis (PCA) and feature selection in machine learning? Is PCA a means of feature selection?
PCA: It is an unsupervised method, used for dimensionality reduction. If your input space is of dimension D, then you would ideally like to pick K < D dimensions such that the euclidean distance between the data points in the K dimensional space is a close approximation to the euclidean distance in original D dimensional space. Another way to say the same thing, is that you want to preserve the variance of data as much as possible. Before selecting the K dimensions using PCA, one has to apply an affine transformation to decorrelate the axis. Why is this important? In the above example you have to pick one dimension out of the two, in the left image, picking either X or Y axis leads to a loss of significant amount of information ( Project data on either X and Y, and you should see that it alters the shape and pairwise distance in distributions ). One the contrary, in the right image, we have aligned our axis to be decorrelated. One can clearly see that P2 axis has a small amount of variance, and hence picking P1, there is a minor change in the pairwise distances or variance of the system. To summarize PCA, 1) Normalize the data, by having a zero mean and decorrelating the axis. 2) Pick K dimensions having the highest amount of variance. Feature Selection: In the example above one can argue that PCA is a means of feature selection, as essentially you pick few features out of the given pool. However it is important to note that the criteria for doing so is completely unsupervised. You are aiming to preserve the variance of the data and that's it. In feature selection you can define an objective, for example classification of data and the algorithm is going to pick dimensions which lead to an increased classification score. To drive the point home, lets look again at the image we used for PCA.  Assume our task is bike classification ( which type of bike? ), where P1 is correlated with price infomation and P2 is  correlated with bike type . Since the price range for different types of bikes varies a lot, you have a high variance on P1 compared to P2. Using PCA, one would select P1 for your system, where as a feature selection algortihm would be able to see the noise induced by P1 for the classification task, and would pick P2 as the relevant feature dimension. One small tip, what if we wanted to apply feature selection directly on the left image? You can see for yourself, that the axis are correlated and picking either of them, you lose some information regarding the bike type. For this reason, even if you do not use PCA to perform dimensionality reduction, it is a good practice to decorrelate the dimensions of your input data. It makes the life of optimization algorithms easier :).    Embed Quote