How is the fuzzy search algorithm in Sublime Text designed?The fuzzy search functionality in Sublime Text is truly amazing. How is it designed or how would you design something similar? (Preferably in JavaScript)I do not know how it is implemented, because I have not seen the source code, but one of ideas I would use is to first preprocess the list of strings in a way which would make it possible answering such queries in sublinear time. One particular trick I know of is to compute a "histogram" for each string, which for each letter tells the number of occurrences of this letter in this string. For example for "json.js" it gives j:2,n:1,o:1,s:2,.:1. You can build a histogram of your query as well. Clearly the histogram of the query needs to be "smaller" than histogram of any result, so you may build an index in a way which would allow to quickly skip all the strings which are "too small". I am not saying, I know how to do that, but I have some ideas. For example you could for each letter and each number of occurrences build a list of all strings which contain this particular letter with as many occurrences. This might mean that our string "json.js" is can be found in 5 lists : list for j2, list for n1, etc. Now, when you enter a query, you build a histogram of the query. For example if I type in "jjs" the histogram is j:2,s:1. Now, I have two options: either traverse a list for j2 or a list for s1. Actually in the first case I have to traverse j3,j4,j5..etc as well (same goes for s2,s3,...). Given lengths of all lists, you can quickly determine which of these options is faster. Say, there is less strings with at least 2j's then there is words with at least one 's' -- then you should use the first option. Now, this method used only one letter (either j or s) to narrow the search. You could build similar lists for all 256x256 combinations of letters, hoping that it will make lists even shorter. A promising way to optimize it is to leave only these combinations for which there is a significant (say 50%) gain. For example if there is 1000 strings with two 'j', 2000 strings with 's', but only '254' strings with two 'j' and one 's', then it might be worth creating a separate list for this case. To make it more serious I would consider not only stats of the data set but also of queries -- how likely is the user to type in "js" ? This all so far did concentrated on histograms, which obviously misses an important piece of information: the order of letters. Perhaps there is a significant gain from treating "jjs" and "jsj" as two separate queries, as the later sequence is less probable? So in the end I would suggest this algorithm for building an index: start from building indexes for single letters. That is build a map in which a single letter is a key, and list of strings which contain this letter is a value. Then recursively try to extend each key with one more letter and see if this leads to 50% reduction of the list length. If so, then add the new key and the list to the index. This way the size of the index is bound (as at each level lists are 50% shorter than on the previous one), and speed is probably good enough. This is obviously a heuristic, and the "50%" is a made up rule. You could try 20% or try to "give a chance" to a key even if it did not reduce the size significantly, because maybe after adding one more letter it will drop by 75% or something. I am curious what is the actual algorithm, too! 