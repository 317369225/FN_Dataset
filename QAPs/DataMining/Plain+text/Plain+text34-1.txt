★How many (standard 8.5"x11") pages would it take to store 100TB of plain text data?This is a follow up to: Upthere (company): How would you store 100 TB so that it is available for the next 100 years?
Well, if you look at this from the most basic perspective, take the DPI of your printer, square it, and then multiply that by the number of square inches on a page.  Maybe if you're feeling really exciting, multiply this by the number of colors in your laser printer, let's call it four for fun (CYMK).  Assuming margins of .25 on each side, this gives you a maximum ceiling of about 120M dots per page.  Assuming that is the theoretical maximum of bits that you can get out of a single page, divide by (1024 * 1024 * 8) to get to megabytes and you can argue that your absolute maximum uncompressed data limit per page is around  14.1 MB. Edit: The above makes the assumption that your printer is printing at 600DPI.  Your mileage may vary. Realistically, though, this isn't going to work.  You're probably not going to want "bits" touching each other since you're going to need to go through an analog -> digital process (scanning) to interpret this data, and since your scanner is going to have to be able to capture the data back in a manner that can be quantized into discrete bits, you're probably not going to utilize the full resolution of your printer out of this process.  Perhaps (and this is probably a long shot) you can come up with some method to cluster bits with as much density as possible by making sure only similar colored dots don't touch, but you're going to take a massive hit to potential density either way.  When you factor in the need for error correction, you're not going to get anywhere near 14MB per page. Once again, we're taking data through a digital -> analog -> digital workflow here, in a process that uses a lot of electric motors and moving paper and cheap A/D or D/A circuits, there's going to be errors that need to be accounted for in the encoding process if you want to be able to decode it later. But that's just the theoretical limits, let's look at reality: Paperdisk seems to be able to reliably encode about 1MB per page with properly matched printer / scanner combinations at 600dpi.  That would make your exercise require somewhere about 104,857,600 pages.  Better start cleaning out your local Staples or Office Depot now! As far as I can tell from their paper on the subject (http://www.paperdisk.com/ibippap...) , while the paper itself (and the benchmarks within it) is old, the data density limitations are bounded by printer / scanner resolution and not processing power of the computer being used.  That said, there haven't exactly been leaps or bounds in laser printing resolution over the last 20 years or so, but it may be interesting to see if consistency has increased (which would, hypothetically, at least, reduce the amount of error correction needed, thus giving you more usable space per page.) On a depressing note, even if you were to somehow get a 1,000x increase in data density, which I imagine is unthinkable, you would still be looking at 104,858 pages. There is the question of whether your 100TB is before or after compression and what kind of data it is (which will tell you what sort of compression ratios you can expect.)  If you've got 100TB of uncompressed ASCII data, you may be able to precompress that to a fairly high degree before trying to encode it. In conclusion, I invite people who might have a better understanding (either on the theoretical side by checking my assumptions, or in the real world side by having actual experience here) look at my answer here and add any insight they might have.  I have a bit of real world experience in building barcode encoding / decoding software, and have had to look at data density on paper before, but never at a high scale.  I think this is a fairly interesting problem though, and wouldn't mind hearing other perspectives on it.    Embed Quote