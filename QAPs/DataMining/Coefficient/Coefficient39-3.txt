Are the highest correlation coefficient indicators most helpful in prediction?
This is true if we just consider each variable individually. Let's generate 100 samples of Y coming from Normal(0,1), with Y=rnorm(100,0,1) Let:  x_high =  Y+rnorm(100,0,0.1). Cor(x_high,Y)=0.9947 x_med =  Y+rnorm(100,0,1). Cor(x_med,Y)=0.7474 x_low   =  Y+rnorm(100,0,5). Cor(x_low,Y)=0.2171 As you can see, we made three variables that each have a different level of correlation based on how much variance we add to the r.v. we are adding to Y. Finally, let's create the linear model. We're interested in the p-values of each variable. The closer to 0, the more confident we are that they aren't equal to 0 (in other words, that they're significant to our model). The regression output gives us the following: x_low: 0.540 x_med: 0.0978 x_high: 0.000.... Therefore, as we see, highly correlated variables do in fact help most with prediction. As for your question about non-linear models: If we use gradient boosting method's relative influence calculations, we get the following, using n.trees=10:  x_low  x_med  x_high      0           0          275.05 Maybe not the most accurate results, but this gives us an idea of how much more useful the most highly correlated variable is compared to the other two.    Embed Quote Updated 21 Jan. 81 views. Asked to answer by Quora User.