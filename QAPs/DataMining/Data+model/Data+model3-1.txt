★Why does deep learning require the construction of generative data models?
The ultimate goal when building a classification model is to get it to generalize well -- that is, accurately predict the label of test examples that it hasn't seen before. In order to get a model to generalize, you need to limit its model complexity.   If you don't, you'll overfit your data, which means you'll make great predictions on data that you've seen before, but you'll do poorly on test examples.  This is pretty unimpressive, because your model could have been to just remember all the data it has seen before, and it'd produce the same behavior.  For neural networks, typical strategies to limit model complexity are to use a small number of hidden units, to add simple regularization (weight decay), or to stop training early.  The latter two essentially amount to telling your model to keep its weights small, which roughly corresponds to making the model closer to linear. I'm not an expert in deep learning, but from what I understand, the unsupervised part can be seen as a smarter form of regularization, where rather than encouraging your weights to be small, you're encouraging your weights to be near a representation that represents the data well.  This lets your model be flexible in the dimensions that are important to representing the data, but it is still limiting the complexity so that the model will generalize. And to be fair, deep learning isn't always a great idea.  It makes the most sense when you have a lot of structure in your inputs (e.g., if your inputs are pixels, there's a lot of correlation between neighboring inputs) and when it is easy to get lots of unlabeled data (e.g., random images off the internet) but harder to get labeled data (e.g., images labeled by what objects are in them).    Embed Quote 