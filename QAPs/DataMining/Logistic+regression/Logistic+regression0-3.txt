★What is the difference between logistic regression and Naive Bayes?
Both naive bayes and logistic regression are log-linear models; that is, in both cases the probability of a document belonging to a class is proportional to exp(w·x), where w is a classifier parameter and x is a feature vector for the document. The main difference is that, in naive bayes, the model is specified so that both the data and the labels depend on w, while in logistic regression only the labels depend on w. Tom Minka has a more technical explanation  http://www.research.microsoft.co... , but the gist of it is that, by freeing the w parameter to only model the labels, you can overcome some shortcomings of naive bayes (for example, if you create two features for every word in a document, what you're doing is essentially squaring all naive bayes probability estimates, while this is not necessarily true of a logistic regression classifier), but your classifier becomes subject to other issues (it will probably overfit if you don't regularize, for example, by placing an explicit prior on w).    Embed Quote