★What is the difference between logistic regression and Naive Bayes?
As others have said, they both train feature weights for the linear decision function (decide true if above 0, false if below).  The difference is how you fit the weights from training data. In NB, you set each feature's weight independently, based on how much it correlates with the label.  (Weights come out to be the features' log-likelihood ratios for the different classes.) In logistic regression, by contrast, you set all the weights together such that the linear decision function tends to be high for positive classes and low for negative classes.  (Linear SVM's work the same, except for a technical tweak of what "tends to be high/low" means.) The difference between NB and LogReg happens when features are correlated.  Say you have two features which are useful predictors -- they correlate with the labels -- but they themselves are repetitive, having extra correlation with each other as well.  NB will give both of them strong weights, so their influence is double-counted.  But logistic regression will compensate by weighting them lower. This is a way to view the probabilistic assumptions of the models; namely, Naive Bayes makes a conditional independence assumption, which is violated when you have correlated/repetitive features. One nice thing about NB is that training has no optimization step.  You just calculate a count table for each feature and you're done with it -- it's single pass and trivially parallelizable every which way. One nice thing about LR is that you can be sloppy with feature engineering.  You can throw in multiple variations of a feature without hurting the overall model (provided you're regularizing appropriately); but in NB this can be problematic. Ng and Jordan (2001) specifically address NB vs LR.  One interesting finding is that NB can perform better when there's a low amount of training data.  But LR should always outperform given enough data.  http://ai.stanford.edu/~ang/pape... .  Also, see the very nice exposition in Mitchell (2005): http://www.cs.cmu.edu/~tom/mlboo...    Embed Quote