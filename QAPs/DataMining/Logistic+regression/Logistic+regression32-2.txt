★Why is Support vector Machine hard to code from scratch where Logistic Regression is not?In SVM, each of the feature gets modified by application of kernel function on it, the cost function remains the same as in logistic regression but with modified features. Whereas, logistic regression is SVM with linear(or no) kernel , still why it is said that SVM is hard to implement and an already implemented library like libsvm should be used.
Logistic regression is a smooth unconstrained convex problem. Gradient descent would work well, Newton's method will work even better. Both these methods are very simple to code up, and the latter is blazingly fast. SVMs can be written as an unconstrained non-smooth convex problem (because of the hinge loss, which is not smooth), or as a constrained smooth convex problem. In either case, Newton's method won't work without modification, and gradient descent is a bit tricky too. It is easy to write optimization code that is correct but takes a long time to converge. SVM optimization packages like LibSVM use specialized algorithms for solving SVMs (LibSVM uses Sequential minimal optimization ) that are much faster.    Embed Quote