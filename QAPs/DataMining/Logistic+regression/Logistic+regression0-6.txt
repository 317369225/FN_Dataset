★What is the difference between logistic regression and Naive Bayes?
Excellent answer by Brendan! Let me add one more interesting difference between Naive Bayes and Logistic Regression. Let's for simplicity pick a scenario like document classification where documents are represented by d, terms (features) by t and classes by c.   You can assume the same generative model for both Naive Bayes and Logistic Regression wherein, you pick a class with some probability, pick a document length from some distribution and then simply roll an n sided die corresponding to this class c to repeatedly generate terms of the document. The only difference is that for parameter estimation, you will optimize joint likelihood in Naive Bayes, which is: \product_{all (d,c) pairs in corpus} P(d,c) P(d,c) = P(d|c) P(c) and P(d|c) in turn is written as proportional to \product_{t} P(t|c)^count(d,t) In case of logistic regression, you'll instead estimate parameters by optimizing conditional likelihood. \product_{all (d,c) pairs in corpus} P(c|d) P(c|d) = P(d|c) P(c)/P(d) where P(d) = \sum_{c} P(d|c) P(c) It turns out that as a result of this "normalizing factor", P(d), you cannot obtain the maximum likelihood estimate of the parameters in a closed form any more. However, the function is convex and the parameters can be estimated by means of a gradient ascent like method. Optimizing conditional likelihood is also called discriminative training and helps offset the problems that come with assumption of conditional independence. It is an interesting exercise to assume the creation of a duplicate feature in both NB and LR and see the difference. You will find the LR objective function (conditional likelihood) being very insensitive to such redundancy.    Embed Quote