★Why is the output of logistic regression interpreted as a probability?
I can only think of one thing to add that might help. In the binary case (2 class problem), you convert a log likelihood ratio to a posterior probability using the sigmoid function without any scaling or offset. In the multiclass case, you can convert the set of log likehoods for all classes to a set of posteriors using the soft-max function also without any scaling or offset. These transformations take you from log p(data|class) to p(class|data). So in the odd and rare case where the input to the logistic regression is a scaled and shifted version of a true log likehood, the optimal logistic regression parameters would simply undo the shifting and scaling and produce the true class posterior. In all other cases, the logistic regression still tries to find the linear transformation that convert the input to class posteriors.  This is reflected in the criteria function which attempts to maximize the posterior of the true class of each data point over all the training data.    Embed Quote