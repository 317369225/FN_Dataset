★Why is Support vector Machine hard to code from scratch where Logistic Regression is not?In SVM, each of the feature gets modified by application of kernel function on it, the cost function remains the same as in logistic regression but with modified features. Whereas, logistic regression is SVM with linear(or no) kernel , still why it is said that SVM is hard to implement and an already implemented library like libsvm should be used.
It's not true that logistic regression is the same as SVM with a linear kernel.  And in fact you can have a kernelized logistic regression if you want.  The difference is that SVMs and Logistic regression optimize different loss functions (i.e. you pay a different cost for making mistakes in classification).  In the SVM, you pay a hinge loss for misclassifying points, and in logistic regression you pay a log-loss.  To answer your question, it's also not necessarily true that SVMs are hard to code from scratch.  There are multiple ways to solve for the SVM solution, some easier than others.  For example, with big datasets, people commonly use stochastic (sub)gradient descent methods on the primal, which are typically intuitive and straight-forward to implement.    Embed Quote