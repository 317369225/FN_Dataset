★Why does logistic regression not converge when the data is completely separable?
When the data is separable, there is an incentive for to get bigger and bigger, to "emphasize" more and more the difference between the two classes.  For example, if you double the size of beta, then elements in class 1 get bigger log odds, and elements in class 0 get smaller log odds.  So, from the perspective of minimizing the loss function, it is always better to make beta bigger and bigger. In this way, beta won't converge.  But, the direction that beta points in will converge.  In particular, converges.    Embed Quote