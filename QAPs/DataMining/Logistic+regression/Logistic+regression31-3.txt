★What exactly is a logistic regression algorithm in machine learning. What are its applications?Want to know what is logistic regression and what are its applications.
In general when we make a machine learning based program, we are trying to come up with a function that can predict for future inputs based on the experience it has gained through the past inputs and their outputs (training set). Logistic Regression is - coming up with a probability function that can give us 'the chance, for an input to belong to any one of the various classes' we have (classification). Consider the following hypothetical data. This is our TRAINING SET. I will refer it multiple times in the explanation. This is a binary classification problem as each tumor is either Malignant (y = 1) or Benign (y = 0). These are the 2 classes here. Our aim is to come up with a probability function that takes in an input X (size of tumor) and return 'what is the probability of this tumor to be malignant'. This probability function is the 'Sigmoid Function' which is : The graph for  above function looks like : Since, probability of any event to happen is [0,1] ( between 0 and 1, including both), this function definitely seems fit to be used as a probability function for logistic regression. Now various question arise - 1) what is z (input to sigmoid function) ? ans)           z = transpose(theta) * X 2) what is X here ? ans)      X is the 'size of tumor'. It is a tumor size that is not in inputs of our training set but we are trying to predict the probability for it to be malignant. Say X = 3.2 3) What is theta ? ans ) Before I answer this, let me explain what is Cost/error function. Suppose you were able to come up with a probability function. You fed it X(size of tumor) = 0.9 and it gave probability for it to be malignant = 0.3, which means it has more chance of being benign. But clearly from our training set this definitely is wrong as for X = 0.9, Y = 1 i.e malignant. So this is an error. In general the error function in logistic regression is given by : Don't let this function scare you. Here m = no of elements in training set i.e. 7 for us(refer training set above), y is simply 1 or 0 and h(x) is nothing but the 'Sigmoid function' we talked about above. Since sigmoid is a function of theta(refer what is z above), hence J is a function of theta. So, clearly J i.e. how much error we make s a function of theta. Now we minimize J over theta and find out those values of theta for which our error function in minimized. This minimization problem is a whole another concept to study and you can find how it is done by studying the Gradient Descent algorithm - http://en.wikipedia.org/wiki/Gra... Once we have theta, our probability function (sigmoid) is ready and we can feed it any size of tumor and it will give us its probability of being malignant. This theory is applied to many industry problems. - whether an email is SPAM(y=1) OR NOT(y=0) - whether a person will vote or not in upcoming elections - classifying a set of words as nouns, pronouns, adjectives etc. - application in Information extraction - in speech recognition systems wherever a classification has to be done, logistic regression might be used. Also check out multinomial logistic regression. - Multinomial logistic regression    Embed Quote