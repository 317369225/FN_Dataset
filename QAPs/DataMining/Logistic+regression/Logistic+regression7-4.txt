★Why is logistic regression used so often in data science?
There are a bunch of good examples of needing to predict a binary outcome.  Let's put that aside. The question is: why logistic regression over any number of other methods: support vector machines, neural networks, naive bayes, etc.? First, I will point out that if one specifies the correct transformation of X, the set of predictors, then logistic regression will be as good as or better than any competiting method.  Now, one still has to figure out the correct transformation of X.  More on that later. Second, if the statistical model built is to be put into production, logistic regression models offer a way to put them into computer code using straight forward functions like +, *, log, etc.  So a data scientist can put the math equation into the system with relative ease.  Now, it ends up that getting the correct transformation of X can be accomplished more easily using alternative modeling techniques, like neural networks, boosting or random forests.  For me, when I need to put a model into production quickly with just straight forward computer code, I will first build the best model outside of logistic regression, figure out the transformation of X, transform X and then build my final model in logistic regression. As a final note, logistic regression is sometimes used because it was the first modeling technique on the block and people > 10 years ago got used to that technique.  I know this is very true in consumer finance where decison makers use it because they do not feel comfortable with (i.e. they are ignorant of) other techniques.    Embed Quote