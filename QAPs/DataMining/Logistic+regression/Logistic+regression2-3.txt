★What are the advantages of logistic regression over decision trees?Are there any cases where it's better to use logistic regression instead of decision trees?
Both Logistic regression and Decision trees are used for classification purpose such as 1.       Predicting whether a particular user will click an ad in shown in the webpage. 2.       Whether a customer will take a loan from bank or not 3.       Identifying whether a document was written Author –A or Author-B Decision trees will generate the output as rules along with metrics such as Support, Confidence and Lift, while logistic regression analysis is based on calculating the odds of the outcome as the ratio of the probability of having the outcome divided by the probability of not having it. Let us understand this better by looking at the outputs generated by these algorithms for above the case study-1. Decision Tree outputs rules like: If the ad is shown on the first page right side, the user will click the ad - Support 0.9, Confidence 0.95 and Lift 3.345 and logistic regression generates Odds ratio of the user clicking on the ad is 0.785. 1.      Assumptions The decision tree assumes the splits are axis parallel and will become more complex with the increase in number of features and multiple decision boundaries are possible.  On the other hand, Logistic regression assumes there is only one decision boundary that is smooth and non-linear.  2.      How the decision boundaries are constructed? Below are two basic functions of Decision trees and Logistic regression. A.    Decision Tree a.       Selecting the best attribute/feature to divide a set at each branch, and b.       Deciding whether each branch is justified adequately. The different decision-tree programs differ in how these are accomplished. B.     Logistic Regression a.       Stepwise selections of the variables and the corresponding coefficients computed b.       The maximum-likelihood ratio is used to determine the statistical significance of the variables which will be part of the Logistic Regression equation.   3.      Limitations Complex decision trees may over fit the data and trees will become unstable. You could prune the tree to solve this. You could use L1 regularization to solve the problem of unreasonable coefficients for the independent variables.    Embed Quote