★Why is the output of logistic regression interpreted as a probability?
The logistic function by itself doesn't imply that good probabilities are obtained. All it guarantees is that the outputs are bounded between 0 and 1. These can be interpreted as probabilities because they are squashed to a restricted range, but is that really enough? Logistic regression does provide guarantees that mean the outputs can be interpreted as probabilities, but for deeper reasons than "the output is in the right range". When the model has converged (learning has ceased) the model probability of the training data (ie the output of the model) will equal the empirical probability of the training data (ie percentage of instances of class 1 at a particular point). This is enforced as Lagrangian constraint in the the construction of maximum entropy (which is basically synonymous for logistic regression) modelling. You can arrive at the same conclusion if you look at the derivative of the Bernoulli log conditional likelihood (LCL) of the training data with respect to the weights (my equations are from memory so probably slightly erroneous; comments for fixes appreciated). But remember this comes from the formulation of the problem, it isn't an accident. are the weights to be learnt is the bias term is the i-th input term, length i is the model probability of the i-th example (ie ) The gradient of this (I have left out all of the intermediate maths) is y is either 0 or 1 for binary classification, so when the gradient is zero, the sum of the 's equals the sum of the 's, ie the model output equals the empirical probability, so the model output can indeed be interpreted as a probability as it matches the training distribution.    Embed Quote