★Why is logistic regression used so often in data science?
A few advantages:   1.  It tends to give calibrated predictions.  So if you take all of the examples where P(Y = 1 | X) > 0.95, then approximately 95% of them will be Y = 1.  Naive Bayes is usually not calibrated.    2.  Since the gradient is sparse (or has a sparse approximation in the regularized case), it is a good fit for domains with sparse features, like NLP.    3.  Stochastic gradient descent scales nicely for medium sized datasets, though multiple-machine scaling is difficult.  In general I think that neural networks with a softmax activation are better than logistic regression and have all of the same nice properties.    Embed Quote