★What is the difference between logistic regression and Naive Bayes?
A quick comment to complement the very informative other answers. As others have mentioned, Naive Bayes fits feature weights independently while logistic regression accounts for correlations amongst features. As a result, Naive Bayes classifiers are often poorly calibrated, meaning that the predicted probabilities from Naive Bayes can be a poor fit for the empirical frequencies of outcomes. (In other words, if you take all of the examples for which your classifier outputs a score of 0.8, about 80% of those examples should have positive labels and 20% should have negative labels. A poorly calibrated classifier won't have this property across the range of predicted probabilities.) For example, let's say you're a search engine and that you're predicting the probability that a user clicks on an ad so you can estimate how much money you expect to make by showing it. This is given by the probability of a click times the amount the advertiser would pay for having the ad shown. A classifier trained with Naive Bayes might learn that there's a high probability that someone who's old will click the ad. It might separately learn that there's a high probability that someone who's clicked a lot in the past will click it. Now you see a user who's old and has clicked a lot of ads in the past, and ask your Naive Bayes classifier for a prediction. When these probabilities get combined, the classifier will like overestimate the odds of the user clicking, giving you an overly optimistic estimate of your revenue. Logistic regression won't suffer from this problem as much, as it accounts for the correlations and implicitly aims to make calibrated predictions. So if you care about the actual probabilities (instead of just relative rankings), it's a good idea to either explicitly calibrate Naive Bayes or use logistic regression, if possible. Rich Caruna has a nice paper (pdf) on the topic. I have some additional notes on Naive Bayes and Logistic Regression here:   Data-driven modeling: Lecture 2   Data-driven modeling: Lecture 5    Embed Quote