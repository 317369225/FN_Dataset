★What special considerations do you need to take into account when using machine learning and text analytics methods with Chinese, Japanese, and Korean texts?Theoretically, a machine learning algorithm such as a Bayesian or SVM classifier will work with any set of texts, but my understanding is that tokenising the ideogrammatic characters in these languages may be difficult. If anyone has any experience or pointers to papers on this subject, that would be great.
Charles H Martin has well explained the challenge, which is referred to as Word Segmentation [1] in research literature on Chinese/Japanese language processing. I'll add some details about Word Segmentation from the perspective of Chinese language processing (the issue discussed is shared by several Asian languages such as Japanese). 1. Background In Chinese, there is the concept of characters (the basic building blocks of the language) and words (which express meanings). Generally each word would contain more than one characters. And each sentence contains multiple words (thus many characters). 2. Task Definition Word Segmentation: given a string of consecutive characters, how would you split it into meaningful words (by adding additional white-spaces)? 3. Example The following phrase is literally translated as: Nanjing City Yangtze River Bridge, which is the name for the famous bridge over Yangtze River in the city of Nanjing: 南京市长江大桥 There are 7 Chinese characters (the square blocks) in this phrase. There exist many word segmenting choices for this phrase disregard semantics. The two most sensible ones are: (1) 南京市          长江               大桥      Nanjing City, Yangtze River, Bridge   (2) 南京       市长      江大桥      Nanjing, Mayor,   Jiang Daqiao (Jiang Daqiao in this case is a named entity) As we see above, depending on your splitting scheme you'll end up with different sets of words for even one single sentence using NLP programs. So the challenge is how to teach a computer to come up with the most sensible splitting (the scheme that general Chinese people could naturally come up with). 4. Current Status Word Segmentation is a very important step in Chinese language processing. Once you segment and tokenize the language, the rest of processing would be similar to standard NLP techniques. Word Segmentation is still an ongoing (and hot) research topic in Chinese language processing. Every year there're publications on related topics in top conferences such as ACL [2]. Current approaches are mainly corpus-based (you build a large dictionary of popular Chinese words and use it to guide your segmentation). Word Segmentation has wide application in industry as well -- all major Chinese search engines such as Baidu.com and sogou.com use such techniques. There are several open-source libraries dealing with the segmenting issue as well. Word Segmentation is regarded as a major bottleneck in bridging gaps between Chinese NLP and standard NLP (for Roman languages), as the accuracy of segmentation has great effect on latter processing. People have very recently proposed a project called Ultra-scale Word Segmentation (海量分词) in hope of aggregating the efforts in related research by providing a public centralized API-based segmentation service. Links: [1] Text Segmentation: http://en.wikipedia.org/wiki/Tex... [2] Chinese Word Segmentation publications: http://academic.research.microso...    Embed Quote