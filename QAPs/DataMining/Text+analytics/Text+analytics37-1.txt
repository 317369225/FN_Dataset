★What special considerations do you need to take into account when using machine learning and text analytics methods with Chinese, Japanese, and Korean texts?Theoretically, a machine learning algorithm such as a Bayesian or SVM classifier will work with any set of texts, but my understanding is that tokenising the ideogrammatic characters in these languages may be difficult. If anyone has any experience or pointers to papers on this subject, that would be great.
Here the point is understanding how to create the tokens and map the text into the vector space Modern Korean is a Roman language and has letters; it was created by a Korean king to help modernize Korea. Chinese and Japanese, however, are very old and use thousands of context-specific symbols.   Both are written without white space--so tokenizing the text is harder than English text. One approach is to guess the whitespace and see what you get Here is an simple example of how to Tokenize Chineese: http://alias-i.com/lingpipe/demo... Another approach is to simply use a large number of ngrams Getting higher quality results requires specialized techniques to )detect word and sentence boundaries) and to resolve context (similar in spirit to word sense disambiguation)    Embed Quote