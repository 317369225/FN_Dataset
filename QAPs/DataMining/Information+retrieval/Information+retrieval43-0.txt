★Information Retrieval: What are the relative advantages/disadvantages of various semantic similarity measures?
The LDA paper by Blei, Ng, Jordan has a good summary of IR techniques for dimensionality reduction (I assume that's what your goal is). I enlist some of them: Cosine-similarity: Easy to implement, not a good metric. The reduction is not substantial. tf-idf: Reduces a corpora of arbitrary length to a sequence of numbers, for each word we maintain tf and idf values. Very easy to computer query-document or document-document similarity. The reduction is not really substantial. Latent Semantic Indexing: Very good for document similarity and dimensionality reduction. Involves matrix factorization which can be very large. There are techniques to apply SVD/PCA on sparse matrices or in a distributed environment. Apache Mahout has implementation of these algorithms for sparce matrices that don't necessarily fit in a single machine's RAM: Dimensional Reduction LDA: An hierarchical Bayesian model that models documents as mixture of topics. This is very popular currently and greatly used in academia and industry. There are scalable versions of this algorithm, most notably Speeding up Latent Dirichlet Allocation from researchers in Yahoo!. There are improvements that improve upon bag-of-words assumption in the original LDA. Document similarity can be calculated effectively for two new documents (would require running few iterations of Gibbs sampler on two new test documents, so may not very fast).    Embed Quote