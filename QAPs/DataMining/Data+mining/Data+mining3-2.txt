What are the most useful data mining / analysis / science tools?
I'll try to round off some of the Python mentions. * Classic Python numerical/scientific combo - NumPy, SciPy, matplotlib, a bunch of the scikit stuff ( scikit.learn for machine learning stuff, scikit-image for image processing ) are great, no brainer for R&D (matlab-like conventions most of the time which can help esp. if you have people coming from academia that are more at home with that) * From there depends what you're doing:   - as so far as datasets - Before hitting the DFS/NoSQL (e.g HDFS, Mongo etc.) highway, you can for example use HDF5 data format for pretty fast serialization / random access to fairly large datasets in a local / development environment. h5py is a nice library for working with this format esp. in conjunction with numpy, check out their quick start guide. not sure how well maintained this format is but for prototyping etc., you wont need to deal with operational hassle of setting up clusters / daemons etc.   - if you're dealing with image processing / vision, OpenCV is obviously a good start, their latest releases (2.4.x) have the cv2 python library which works natively with numpy arrays as well. Lots of functionality inbuilt.  - NLP and text mining - nltk is very actively developed and already has alot of great functionality and easy to use datasets.  - one thing that wasn't mentioned has to do with working with graphs / graph algorithms - this is oviously a huge issue for most 'big data' applications where its alot about mining for relationships / correlations between different variables. NetworkX is a very complete Python package for most classic graph data structures and algorithms. Was also recently featured in a Pycon talk (google for 'pycon 2012 graph processing' ). My experience has been that its great for prototyping but not geared so much for bigger data sets (hundreds of thousands of nodes/edges at most, you might get abit farther with sparse graphs though).  - distributed processing - If you do end up finding yourself with most code in Python and wanting to move towards Hadoop / Map Reduce , There's a couple of python frameworks that will let you write MapReduce jobs and use hadoop streaming to run these on a hadoop cluster. Have a look at Dumbo, and more recently Hadoopy (never tried hadoopy myself but sounds promising, dumbo is great and has good documentation, but takes some tinkering to try and get a handle on w.r.t performance, 'whats under the hood'.. )   - In general, Once performance becomes an issue:    * have a look at Cython (lets you mix C strong typing inside python code  and optimizes certain operations by compiling into low level Python C API code, can get huge speed gains in numerical context, looping etc.),    * SWIG for wrapping small C/C++ modules in Python.    * One more pretty amazing project with respect to numpy and performance that I recently found out about is numexpr - http://code.google.com/p/numexpr/ . Have just played with it following their examples but seems like a huge win already.. My experience has been that with these technologies you can certainly run production-grade code in a timely manner (whatever the meaning for that might be in your case - be it real-time processing of some sort or an analytics backend..)    Embed Quote 