★What hadoop workflow infrastructure will you recommend?
I'm the author of Luigi, full disclaimer :) I wouldn't agree putting Luigi (or hamake, which I had never heard of until now) in the same category as Crunch and Cascading. They all help you run stuff that spans multiple Hadoop jobs, but the difference is: - Luigi (as well as Oozie, Azkaban, hamake etc) is a workflow engine. It doesn't care too much about what it runs. It doesn't help you write the data processing logic. Its purpose is to help you need to run many steps in a batch processing workflow, re-use steps, automate jobs, monitor failures, and so on. - Crunch and Cascading are mapreduce abstractions that let you use higher order data processing operations. I would put them into a category together with scripting languages such as Hive and Pig. These tools all compile down into a handful of Hadoop jobs, but it doesn't scale beyond that. I'm a big fan of these tools, and as abstractions to mapreduce they are great. We actally use Luigi to run Scalding jobs (Scala version of Cascading) and it works really well. Luigi is the "glue" that stitches together 1000s of tasks and provides automation and monitoring, where each task can be anything, like running a Hive query, running Cascading, inserting stuff into Cassandra, running mapreduce in Python, training machine learning models, etc. A nice thing about Luigi is that it's not tied to Hadoop in any way, although it comes with a lot of Hadoop batteries included and Hadoop obviously being a great tool for batch processing. It's also loosely based on the same ideas as Make    Embed Quote