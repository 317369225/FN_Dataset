★Why do we square instead of using the absolute value when calculating variance and standard deviation?
You've got technical answers. Among Michael Hochster's bullet points, one key one is the last. So, here's a different take: It's because Ronald Fisher came before Alan Turing. OK, I know, least squares came long before Ronald Fisher (see The History of Statistics: The Measurement of Uncertainty before 1900: Stephen M. Stigler: 9780674403413: Amazon.com: Books) but statistics really started to be used a lot more in the 20th century.  Once there are reasonably fast computers, alternate measures of spread (e.g median absolute deviation) become possible because we don't need to rely on an analytic solution. Indeed, had there been computers much earlier, I think it likely that we would all be doing permutation and randomization tests a lot more.    Embed Quote