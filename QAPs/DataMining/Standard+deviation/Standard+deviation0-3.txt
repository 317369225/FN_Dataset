★Is there an intuitive explanation for the difference between standard deviation and sample standard deviation?
Hi Bill! This is one statisticians love to argue about. As a preliminary note, as they say in "Numerical Recipes," if the difference between n and n-1 matters, you are probably up to no good! I agree with your definition of standard deviation -- it's the root-mean-square of a distribution minus its mean. And the variance is just the square of this. If we have perfect knowledge of a probability distribution (e.g. we know the p.d.f. or p.m.f.), calculating the standard deviation or the variance is easy. The question, then, is how to estimate the variance or standard deviation of a distribution when you only have partial information -- in this case, when you only have a finite sample drawn from the distribution. The naive technique would be: Estimate the mean of the underlying distribution by taking the mean of the sample Then estimate the variance of the underlying distribution by taking the variance of the sample, plugging in the mean estimate obtained above as the actual mean As you know, this produces a biased estimate of the variance of the source distribution. (Here's a pretty good explanation: http://en.wikipedia.org/wiki/Bia...) One colloquial reason why is that a crazy sample will just skew the mean's estimate a little bit away from the true mean, whereas you really want the mean where it should be and the craziness should affect the variance. Amazingly, substituting n-1 for n does produce an unbiased estimate of the variance. Additionally, if you have perfect knowledge of the mean (e.g. you know the distribution is zero-centered), then 1/n for variance OR standard deviation produces an unbiased estimate, after you fill in your known-true value of the mean. However, this doesn't mean that using 1/n for mean, and then 1/(n-1) for stddev produces an unbiased estimate of the underlying standard deviation! The square root makes things complicated. In fact, to the best of my knowledge there is no distribution-independent, unbiased estimator of standard deviation given a finite sample. (There is one formula that works on normal distributions, another formula for another shape, etc.) To state it another way: the square root of the unbiased estimate of the variance is NOT an unbiased estimate of the square root of the variance. This is why I cringe when you define the 1/(n-1) formula as the sample standard deviation. It is an estimate of the underlying standard deviation, given a partial sample, but it's a biased estimate -- just as 1/n is biased. So, summary: 1/n (to estimate the mean) and then 1/(n-1) (for variance) gives an unbiased estimate of the variance of the source distribution, given a finite sample. But taking the square root does not give an unbiased estimate of the standard deviation, and to the best of my knowledge there is no single formula that gives an unbiased estimate of the underlying stddev from a sample, without knowing the shape of the distribution. (Finally, many statisticians would question the elevation of unbiasedness above all other properties that might be desirable in an estimator, and I think Jaynes has suggested that the word "bias" be replaced with "the component of the estimator's error orthogonal to its variance." You can see why this may not have caught on, but I don't want you to think that bias is the be-all-and-end-all of performance characteristics for an estimator.)    Embed Quote