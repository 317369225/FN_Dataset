★Is there an intuitive explanation for the difference between standard deviation and sample standard deviation?
Some excellent technical answers here - I didn't see my personal favourite for the 'intuitive' answer. We want something that relates to the typical, or average, squared difference between a data point and the "rest of the data points". So to estimate that we need to sum up the squared differences, and divide by the number of differences. When you've got 1 number, you've got no differences When you've got 2 numbers, you've got one difference Each number you add into your sample gives you another independent difference So it makes sense that the true 'average squared difference' is the total squared differences divided by n-1 .... with this introduction, I then go on to the 'Sum squared is smallest around the mean of the set, so the sample mean will (almost) always give an underestimate of the contribution of the sample data to the population variance (because the sample mean is (almost) never precisely the population mean). .... and direct the students who are still curious / desirous of a proof that the expectation of SSe/(n-1) is the sample variance (e.g. wikipedia)... However - as others have said - all this should be about the variance, in order to avoid saying "the sample stdev is an unbiased estimator" (it isn't).    Embed Quote