★Is there an intuitive explanation for the difference between standard deviation and sample standard deviation?
Suppose you have a single datapoint x from a population P. What's the variance of P? Well, if the whole population P consists solely of the single datapoint x (i.e., P = {x}), then the population variance formula correctly matches our intuition that there is zero variance. But what if x is only a sample from P, i.e., P actually consists of many more unknown points? It's very unlikely that these other points are all x's, so if we used the population variance formula to estimate P's variance (giving an estimate of 0), we'd be underestimating the actual variance. Thus, we need to define a sample variance formula for when we don't have the entire population in hand. How should we define this formula? If we accept that the formula should have the form for some (which is natural, since it's the simplest form that ensures that the sample variance approaches the population variance in the limit), then the only choice of that makes the sample variance non-zero (admittedly, to make it undefined or "infinite" instead -- but then again, perhaps that's exactly what we want, since a single sample point gives us no information about variance) for the case of a single datapoint is .    Embed Quote