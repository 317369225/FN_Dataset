★Why is standard deviation the standard measure of uncertainty in quantum mechanics?Isn't there no better measure that is more appropriate for nature (i.e. one that fits nature better)? If not, is it just a coincidence that such a simple measure also makes the expressions of the laws of physics simplest, or is there something more to it (e.g. something to do with the mathematics of waves)?
There are many different measures you can use to measure uncertainty and many of these have their own uncertainty principle. It's just that mean square errors are very well behaved mathematically so it's easier to teach the usual Heisenberg principle Sometimes another uncertainty principle is more useful. For example, there is a popular argument that derives the lowest energy level of the hydrogen atom from the Heisenberg uncertainty principle. I think Feynman himself used it. Here is one version of it: The Uncertainty Principle. Although it appears in many quantum mechanics courses it has been known to be incorrect for many years. However, a version using a "Sobolev inequality" instead of the Heisenberg uncertainty principle is correct. You can read about some other uncertainty principles here: Page on caltech.edu. That article explicitly mentions that the Sobolev inequality is harder to prove.    Embed Quote