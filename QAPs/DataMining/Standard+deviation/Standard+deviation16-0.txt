★Why do we square instead of using the absolute value when calculating variance and standard deviation?
First I'll answer the mathematical question asked in the question details, which I'm going to restate because I think it is stated wrong: Why is it true that  ? The short answer is "Because of Jensen's inequality." See http://en.wikipedia.org/wiki/Jen... and the rest of the article for context. It says in particular that for a concave function , with equality only when the are all equal. In this case we take as the square root function and . What about the more general question, "Why variance?" I don't believe there is any compelling conceptual reason to use variance as a measure of spread. If forced to choose, my guess is that most people would say more robust measures like interquartile range or MAD better capture the concept of "spread" in most cases. But variance (and more generally "sum of squares") has some attractive properties, many of which flow from the Pythagorean theorem one way or another. Here some of them, without much math: We can decompose sums of squares into meaningful components like "between group variance" and "within-group variance." To generalize the above point, when a random variable is partly explained by another random variable there is a useful decomposition of the variance of into the part explained by and the unexplained part. (See http://en.wikipedia.org/wiki/Law...). If we think more broadly about mean squared error, this too can be decomposed into the sum of variance and squared bias. It is easy to interpret this total error as the sum of "systematic error" and "noise." Often we want to minimize our error. When the error is a sum of squares, we are minimizing something quadratic. This is easily accomplished by solving linear equations.So yes, variance and mean squared error are conveniences rather than conceptual necessities. But they are convenient conveniences.    Embed Quote