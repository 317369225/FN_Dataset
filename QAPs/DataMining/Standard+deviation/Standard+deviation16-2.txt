★Why do we square instead of using the absolute value when calculating variance and standard deviation?
You are right that squares have some nice mathematical advantages, especially simple analytic formulas for derivatives.  This is very handy when you work out formulas for regression coefficients.  E.g., you want to fit a set of data with a function and you have to determine the coefficients in the function that give the best fit.  Minimizing the root mean square error is simple.  I've never seen anyone work out formulas for coefficients that would minimize the average absolute error.  (Might be a fun problem for a first year calculus class.) The average of the absolute values of the differences from an average is generally different from the root mean square difference from the average .  Abs[x] = Sqrt[x*x].  But Sum[Abs[xi]] is not equal to Sqrt[Sum[xi*xi]] E.g., take the set {-2, 2, -1, 1, 0} The average value is zero.  The squared differences are {4,4,1,1,0} The absolute values of the differences are {2, 2, 1, 1, 0} The RMS difference is Sqrt[2] = 1.414... The average absolute value difference = 6/5 = 1.2 This example hints at an important difference between  minimizing absolute values of differences and minimizing squared differences.  The usual RMS fit is especially sensitive to large errors - they get squared in the process.  If the large errors are actually bad data, you have to remove them from the set before doing the fit.  Minimizing the sum of absolute values of differences would give a fit that was less sensitive to outliers.  Which is 'better' depends on what you know about your data.    Embed Quote