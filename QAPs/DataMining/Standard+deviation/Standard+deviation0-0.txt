★Is there an intuitive explanation for the difference between standard deviation and sample standard deviation?
Suppose you have a distribution like this blue line and the data comes out like these red x's: By random chance, the data is a little lopsided. In this case, the mean of the data is the red dotted line, and it's a little bit higher than the mean of the distribution (black line). The red line will be too high half the time and too low half the time, so it's called unbiased. It's still always off by at least a smidgen, though. Now move on and calculate the standard deviation of your sample. To do this, you need to use the mean. After all, standard deviation is just a way of saying how far away the points are from the mean, so the mean that you calculated will have to come into your formula for standard deviation. There's a problem, though - you're using the wrong mean! You're using the red line, when you'd prefer to use black. The red line is shifted towards the direction with more data points, so it's closer to your data on average than it should be. So when you calculate a standard deviation with the red line, you commonly get a number that's too small. Thus, standard deviation calculated naively from your data is a biased estimator. We would like to do something to remove the bias by increasing the standard deviation the right amount. Dividing by (n-1) rather than n turns out to have exactly this effect. It creates an unbiased estimator of the standard deviation. Here's the wikipedia article: http://en.wikipedia.org/wiki/Bes... Some of my language above was a bit imprecise. For example, "unbiased" doesn't mean "above the true value half the time and below half the time", but instead that the expectation value of the estimator is equal to the true value. It's more of a mouthful to say the precise way, though, and I trust that readers advanced enough for that to matter can understand what the technical details are from the context.    Embed Quote