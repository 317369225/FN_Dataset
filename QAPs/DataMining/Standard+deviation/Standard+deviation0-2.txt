★Is there an intuitive explanation for the difference between standard deviation and sample standard deviation?
Let's say we have a population . The population variance is defined as: ,  where We would like to estimate based on a sample   An ideal estimate would be . Unfortunately we can't actually use the ideal estimate because we don't know .  We could plug in an estimate for : , where . This would be applying the "population variance" formula to our sample directly.  The catch is that this plug-in estimate is always a little bit smaller than the ideal estimate (and on average is smaller than the true ). Here is the reason. We are trying to estimate the variability of the data around the unknown . The plug-in estimate does this by estimating the variability around But itself has some variability around , which the plug-in estimate ignores. Moreover, is chosen so as to make the variability around it as small as possible. That is why the plug-in estimate tends to be too small, a little smaller than on average. To make it the same size as on average, we have to multiply by a slight inflation factor of This is where the sample variance formula comes from.    Embed Quote