★Why do we square instead of using the absolute value when calculating variance and standard deviation?
A huge part of statistics relies on squaring those differences because the distribution of many functions of the sum of those squared differences can be obtained mathematically so that inferences can be made. Much of this mathematical work was based on classical results. None of this is possible in the same way for the absolute differences. In a sense one could say that we use the squared differences to take advantage of some fairly deep quirks of the calculus. We do this in spite of the fact that using squared differences exaggerates the influence of bad data that we call outliers. In general what we try to do is to identify and remove the outliers, so that they do not contaminate the remaining data, and then apply the theory that is based on the squared differences. However, where the emphasis is on estimating just a location (something like an 'average') or a regression line in the face of quite a few outliers one might resort to using absolute difference estimation, perhaps as long as no further inferences are required.    Embed Quote