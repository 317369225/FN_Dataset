★In linear regression, why would a large standard deviation of Y increase the slope and why would a small standard deviation of X decrease the slope?
First of all SMALL std of X will INCREASE the slope. So does a large deviation of Y. Let me first show it mathematically, then I will try to explain in plain English. The formula for slope in a regression is . Let's say that all of the Y's are multiplied by 2, which increases the std by 2 as well. Then Hence, the slope is doubled. Let's say that X values are multiplied by 2 Hence, the slope is half of the original slope. Before giving the intuition, I need to point out couple of things. If  X and Y are independent (actually it is enough here if they are uncorrelated), then the slope will be zero. Thus it doesn't matter whether std of X or Y increases/decreases; the slope will be still zero. Other thing is that let's assume that there is a positive relationship between X and Y, so that we do not get confused whether is larger than or not. Here is the intuition: the interpretation of the slope is "How much Y will change if X changes by 1 unit". 1) If std of Y is very large, then the resulting change in Y will be larger compared to a case where it is smaller. 2) If the variance of X is already very large, then "1 unit change in X" does not mean much, it's not of a big change for X. Therefore, we do not want this little change (compared to other X values) will have a large impact on Y, thus the slope will be smaller compared to a case where X has a smaller variance and 1 unit change means "a lot of change".    Embed Quote