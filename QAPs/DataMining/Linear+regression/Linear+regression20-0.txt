★What is the difference between singular value decomposition and multiple linear regression?They both seem to find the best-fit subspace for a set of points, unless I've understood the techniques incorrectly.
The SVD is in general a matrix decomposition, and a pretty different thing. This is a reasonable comparison in the sense of using the SVD to find a low-rank approximate decomposition. If you have k variables predicting 1 variable, then you're decomposing that (k+1)-column matrix with a rank k approximation. The result is indeed projecting those k+1 dimensions into k, which "looks" a lot like the result of multiple regression over k variables. It's a different thing though. The fastest way I can describe the difference is that regression is minimizing a different squared distance. It's finding the k-dimensional hyperplane that minimizes the squared distance to the data point along the (k+1)-st dimension. The projection implied by the SVD minimizes squared distance from point to the plane (i.e. perpendicular distance). It's the difference between these two things: (Page on wolfram.com)    Embed Quote