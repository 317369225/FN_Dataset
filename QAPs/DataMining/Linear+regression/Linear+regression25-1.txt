★What are the benefits of using ridge regression over ordinary linear regression?
Ridge regression has two main benefits. First, adding a penalty term reduces overfitting. Second, the penalty term guarantees that we can find a solution. I think the second part is easier to explain. First recall that the objective function for ridge regression is as follows. This has the solution We know that is positive semi-definite, but it may be singluar. This means that it may have some zero eigenvalues, and in that case it can't be inverted. Adding to it effectively adds to each eigenvalue, so the result is invertible. Note of course that corresponds to OLS. As for overfitting, one basic principle that comes up a lot is that estimating parsimonious models has less variance. An example in the case of linear regression is that if two predictors have high collinearity, their coefficients can blow up (this is often called "bouncing betas" and is related to the problem of singularity above). If we want to include as few predictors as possible, we could include a penalty for each additional predictor (an penalty). Unfortunately this is non-convex and quickly becomes computationally intractable. The squared penalty term in ridge regression can be seen as a relaxation of this (as is the penalty in lasso) and is computationally very easy to solve.    Embed Quote