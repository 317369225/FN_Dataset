★What is the difference between linear regression using MLE, Bayesian linear regression and linear regression using MAP?
MLE chooses the parameters which maximize the likelihood of data given that parameter, MAP chooses parameters which maximize the posterior probability  of that parameter in the light of observed data and Bayesian inference computes the posterior probability distribution for the parameters given data. Let the data be where each and . The linear regression model predicts the values of 's as linear combinations of the features 's Adding 1 as a vector to xi and combing and 's into a single vector, this can be written as The observed values of is assumed to have Gaussian noise error i.e. where . The Likelihood in this case is given by then the log-likelihood is given by Now MLE states that the w is given by which in this case reduces to This is why the linear regression model is often known as least square method. Now differentiating with respect to and equating it to zero, we get the estimate of as For MAP, we assume a Gaussian prior on i.e  Then the posterior probability is given by Bayes rule as as we MLE, we will maximize the log-posterior probability and then which reduces to Thus, the MAP estimation can be thought of as regression with regularization. The MAP estimate in this case is In Bayesian regression, the Bayesian philosophy is applied. Both MLE and MAP are point estimates but in Bayesian regression, we look for predictive probability. If is a new point, we compute the probability of , the y-value corresponding to this x is given by [math] P(\tilde{y} | \tilde{x}, y, X, \sigma^2) = \int P(\tilde{y} | \tilde{x}, w, X, \sigma^2 ) P(w|X) dw [/math] MCMC methods are often used for such inference. I can add more details, let me know. In full Bayesian regression, we assume a prior on in addition to prior on .    Embed Quote