★What is Generalized Linear Regression?How do you calculate the coefficients for GLM. Also is it possible to use least squares estimation for calculating coefficients.
-->First of all, what is linear regression? -->Linear  regression is a hypothesis function which predicts the value of target  variable Y based on some feature variable X. So if we assume our  hypothesis function to be a linear function of vector parameter theta  and input features X, the our hypothesis can be expressed as       h(x) = θ(T)*X where θ(T) = theta transpose -->Now,  in order to minimize the cost function J(θ), and to calculate  coefficients, we use different types gradient decent algorithms, mainly  we use batch gradient decent and stochastic gradient decent. Batch  gradient decent is more accurate but takes more time to converge to  global optima and also takes much time to take a single step. Where on  the other hand, stochastic gradient decent is fast compared to batch  gradient decent and takes less time to converge, though with less  accuracy.  -->We can also use analytical method to compute vector parameter θ. -->In case of linear hypothesis, your vector parameter θ can be calculated using this formula, without any iterative algorithm,                θ = inv((X(T)*X))*X(T)*Y where,              inv(x) = inverse of matrix x              X(T) = transpose of matrix X -->Now, coming to your question, about generalized linear regression models specifically, Let's take the same example. --.> Here, y(i) = h(x(i)) + e(i) -->Here,  let's assume that the error term e(i) is distributed independently and  identically(IID) according to Gaussian distribution. So, in order to  maximize the probability of Y given X parametrized by vector parameter  theta, we will have to maximize the likelihood function of theta, that  means we will have to maximize the log likelihood function of theta,  which will result into minimizing of function J(θ). -->Now, to minimize cost function J(θ), we use the gradient decent algorithm. -->Now,  generalized linear models are broader set of algorithms, mainly know as  exponential family distribution. Now if you assume any distribution to  be generalized linear models distributions, there are three assumptions  which you follow in general, 1) y |x; θ ∼ Exponential Family(η). I.e., given x and θ, the distribution of y follows some exponential family distribution, with parameter η. 2) Given x, our goal is to predict the expected value of T(y) given x. In most of our examples, we will have T(y) = y, so this means we would like the prediction h(x) output by our learned hypothesis h to satisfy h(x) = E[y|x]. 3) The natural parameter η and the inputs x are related linearly: η = θT*x. (Or, if η is vector-valued, then ηi = θT(i) x.) -->Here, θT is θ vector's transpose. -->Under  these three assumptions, you get the likelihood function and from that  you get the log likelihood function. And you will use newton's method to  maximize the log likelihood function and thus from that, you will be  able to get the parameters of hypothesis or coefficients of that  generalized linear models.    Embed Quote