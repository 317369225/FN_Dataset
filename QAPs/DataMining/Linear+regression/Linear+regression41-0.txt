★What is the difference between ridge regression and locally weighted linear regression?
Ridge regression is a method of trading bias for variance; it is especially useful when there is colinearity, as one of the main problems with colinear predictors is that it results in parameter estimates that have very high variance and that are highly unstable - that is, a small change in the input data can dramatically affect the parameter estimates (although not the predicted values).  In OLS regression we seek to minimize the sum of least squares where || is the Euclidean norm. In ridge regression we minimize where is some appropriate matrix (how to find the right matrix is what makes it a little tricky - hooray for algorithms and computers!) Locally weighted linear regression (aka loess regression) is designed to fix a different problem - that of highly nonlinear relationships between the independent variables and the dependent variable.  Sometimes, polynomial terms of the IVs will work, but, beyond the cube, they are often hard to interpret and, in some cases, the relationship is not well fit by any reasonable polynomial    Embed Quote