★How do people come up with an appropriate topology for (recurrent) neural networks?Obviously there's training, but fully connected networks don't scale at all. Specifically I'm curious about Hopfield and Boltzmann machine based networks, including deep belief networks and so on. I'd like to know the various heuristics researchers use, and maybe some comments on how well they work and when they might fail.
There is no general recipe for answering your question. In the case of images and sequential data one often uses convolutional or locally (in time and/or space) connected layers. In other cases, it has turned out up to now (because of GPU technology) that it is very efficient to do fully connected layers because large matrix multiplication is like 10 times faster than a bunch of smaller matrix multiplications (or worse, randomly connected sparse matrix multiplication) for the same number of parameters. So fully-connected layers still dominate (at least in part of the network) in state-of-the-art systems. However, this is driven by implementation considerations. It may well be that better models would arise with larger layers that are more sparsely connected. The connectivity situation is not very different whether you consider generative (Boltzmann, DBN) or discriminative (supervised deep MLP) variants.    Embed Quote