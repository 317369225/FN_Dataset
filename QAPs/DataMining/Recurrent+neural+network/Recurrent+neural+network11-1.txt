★Why do long short-term recurrent neural networks not suffer from vanishing or exploding gradients?
The error propagated backwards by common activation functions shrinks rapidly or explodes mostly depending on the number of layers in the neural network. Long Short-Term Memory Recurrent Neural Networks use units known as Constant Error Carousels (CECs) to overcome this limitation. Each CEC has: an activation function f the identity function a connection to itself CECs are connected to several non-linear units. For example units with a multiplicative activation function. The weight changes of these units are stable over time and they are the main reason these networks can learn events that happened thousands of discrete time steps in the past while the memory of important events in regular Recurrent Neural Networks tends to falter within about 10 discrete time steps. CECs are the main reason for the lack of vanishing or exploding gradients in Long Short Term Memory RNNs. I suggest taking a look at these slides for more detailed information http://people.idsia.ch/~juergen/...    Embed Quote