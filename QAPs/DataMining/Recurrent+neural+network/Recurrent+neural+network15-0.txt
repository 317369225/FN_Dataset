★Why is the backward pass in a recurrent neural network linear?
I hope you understand the architecture of backpropogation neural networks and recurrent neural networks. You could see that just like the case of simple BPNN, the backward pass step in RNN we simple get the error derivate by gradient descent rule and this weight update rule is just linear and can be expressed in the form of a generalized delta rule. In  the forward pass step, we induce nonlinearity by introducing activation  functions so that even the data which is not initially linearly  separable can be effectively modeled (eg. XOR function). The backward  pass step is just a way to update the weights so that we reach a local  minima easily by Gradient Descent without having to try out each and  every possible weights (which might take years..). There is no need for  inducing any non linearity in the backward pass step. This lecture might help you in understanding things better:    Embed Quote