★How important is orthogonal initialization to training a (recurrent) neural network?I realized that many neural network implementations (exp. rnn, lstm) initialize the weight matrices with orthogonal vectors through svd. Anyone knows whether this is really more helpful than randomly initialization?
I do not know from direct experience, but one reason to believe it might be helpful is that all eigenvalues of an orthogonal matrix have magnitude 1. This means that taking arbitrary powers of this matrix does not cause "blowup" of the gradients as the number of timesteps gets larger and larger.    Embed Quote