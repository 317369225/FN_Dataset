★What are principles for choosing structures for recurrent neural networks?Ng's recent DeepSpeech model used recurrent connections in its 4th and 5th layer. How can this kind of decision be made? Page on arxiv.org
Choosing deep neural network architectures is much more of an art than a science and is usually a combination of intuition, heuristics and experimentation. One approach that's generally helpful is to start with "toy" versions of the problem (with much smaller training set size) to get a feel for the bias/variance tradeoff as a function of the number of layers, number of units per layer, and other hyper-parameters. In the specific example of DeepSpeech, the authors chose to use one recurrent layer, which was one of the higher layers of the network. Why use one recurrent layer? Due to the sequential nature of the recurrent layers, the recurrent layer is tougher to parallelize on the GPUs (whereas the non-recurrent layers are trivially to parallelize along the time dimension) -- having more than one recurrent layer would be a computational bottleneck. Why have the recurrent layer higher up in the network? Deep neural networks learn increasingly higher-level representations of the raw input data (e.g. speech spectrograms) higher up in the network, so the recurrent layer is more effective using these high-level representations rather than raw or low-level features.    Embed Quote