★Why do long short-term recurrent neural networks not suffer from vanishing or exploding gradients?
There are many good explanations that go into great technical detail, so I'll try to give a less detailed intuitive explanation on why the gradient doesn't vanish (and will try to think of a nice explanation of why it doesn't explode and edit this later).  The main key is the input gate, which blocks input. In a hypothetical RNN that doesn't use LSTM, say our hidden unit at time t gets 1/2 of its value from the hidden unit at time t-1, and 1/2 from the current input.  Then the influence of an input on hidden units decays exponentially over time since it keeps getting halved. Now say we have a single input at the beginning, the hidden unit gets its value from that, and we fully block all future inputs.  Then the influence of that first input doesn't decay at all, since it isn't getting halved at each time step.    Embed Quote