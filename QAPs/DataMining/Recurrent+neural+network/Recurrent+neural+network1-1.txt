★What are principles for choosing structures for recurrent neural networks?Ng's recent DeepSpeech model used recurrent connections in its 4th and 5th layer. How can this kind of decision be made? Page on arxiv.org
I'll try to add something to the answer of Maurizio. So  basically to make your NN the more powerful you want to give him a lot  of weights. And even if theoretically you could put all your weight in  two layers, it's much more efficient to put them across several layers.  It gives much more expressiveness to your model. But their are limitations on both the numbers of weight in your model and the number of layers. Indeed  to learn meaningful weights you need to have much more training data  than weights, else you'll "overfit" your training set and learn  something which is not useful in practice. Also  while distributing weights in layers tends to help your model you also  face new problems like "vanishing gradient" specific to deep NN. Indeed  when you have 5 layers of neurones the influence of a neurone of the 1st  layer on the output is very small, and so it's difficult to learn what  it's activation should be. These problems can be limited with correct initialization of the weight, but they are still there. So that gives you more idea about the trade-off you make when you design a NN. Note  that Ng uses dropout when training it's NN. Using dropout prevents your  NN to overfit, so a common practice is: add layers and weights to your  NN until it begin to overfit your training set, then train it with  dropout. That guarantees you that your model is powerful enough to learn  on your dataset, while dropout forces it to extract regularities and to  generalize well. So that's was about how you generally choose the number of layers and the number of weights. In  this case every thing was also optimized for GPU computing. Which means  the layers don't have too much weights so they can easily fit onto a  GPU memory. For  the choice of the two recurrent layers (one left-to-right, and one  right-to-left) it is due to the nature of the task. When you hear  someone speak, if you mishear a word you'll use the whole sentence to  correct it. So that's what the recurrent layer aims to do: Provide some  context before actually producing the word. As explained before by Maurizio, these layers are computationally expensive because they can't be parallelized. So you  want to provide them high-order features in input so you don't have to  put too much weights in these recurrent layers. And I'm sure the Baidu team didn't get this results at their first try. It likely takes lot of iterations, with different numbers of weights and layers, different gradient descents, different dropout rates and so on...    Embed Quote