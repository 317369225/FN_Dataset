★How do you supply input to a recurrent neural network?
What you mention is the "backpropagation through time" trick (or training method).  In order to compute the required gradients it's convenient to "unroll" the network through time a few "loops", in effect converting the network into a "deeper" one with tied weights (the weights in each "unrolled" network instance are actually the same). No need to unroll too much, due to the Vanishing gradient problem. In normal "forward" operation you don't unroll the net: you just present the inputs required at the time required, and keep handy any previous output you need to present in the recurrent links, or as part of any future "backpropagation" operation.    Embed Quote