★When does parameter-sharing in recurrent neural networks make sense?RNN's share a similarity to CNN's in that both (typically) have some form of weight sharing. That makes sense in CNN when you are looking at images. That is, one can tell a fairly plausible story on why you would want to share weights with a CNN. When does this with RNN's? Perhaps an example where weight sharing is a a good idea (besides computational efficiency) would help.
Best side of param sharing is the reduction of  values to be learned. If this is no a constraint for your rig, don't share parameters. For instance in your particular problem you might have images structurally very different on different spatial positions. In that case parameter sharing is not reasonable since it assumes repetitive structures are all over the given images. From the same observation, if we share parameters in RNN, you also assume that the present output has similar relations with the present input and the recurrent input. I guess this is not also very valid assumption to follow. However, computational efficiency is still valid argument.    Embed Quote