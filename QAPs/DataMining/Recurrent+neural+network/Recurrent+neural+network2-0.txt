★What is the computational power of non-recurrent neural networks vs recurrent neural networks?
The key observation is that "traditional" feedforward neural networks are stateless -- you provide an input, and they provide an output. Thus they are not Turing-complete, because to be Turing-complete, there must be some kind of storage capability. Recurrent neural networks have state. This means that they retain some of the activation from the previous input and feed it in to the current calculations. This makes them much better suited for dealing with sequences, time series, video segments, etc, and then you can talk about them being complete computational models. The exact properties depend on the topology of the neural net. In practice, if you really wanted to, you could "unroll" a recurrent neural net to be a feedforward neural net and present entire sequences of input (e.g. whole sentences) instead of part of the input, and tie the appropriate weights together, but it's not necessarily the most efficient way to do things.    Embed Quote