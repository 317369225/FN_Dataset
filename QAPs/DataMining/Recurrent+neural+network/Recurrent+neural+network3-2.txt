★Is there any other problem with recurrent neural networks besides complexity?My understanding is that the reason people can't use RNN's too much is because if you have too many neurons, you run into a very large and intractable combinatorial problem with obnoxious complexity. Is my understanding correct? Are there other reasons as well?
As others have remarked, they are unstable and it's hard to come up with good learning algorithms. This may point to a more fundamental problem with our understanding of cross-coupled systems. Steven Strogatz in his popular-science book "Sync" argued that we are still far from understanding networks where the components signal each other and achieve a form of synchronization or orderliness in the dynamics.   One type of RNN which tries to reduce the complexity is "reservoir networks"[1], of which "echo state networks" and "liquid state machines" are two instances. The RNN is initialized randomly, but each node also has a connection to the output node. This is where the reservoir networks depart from other RNNs. The weights in the jumbled interconnected network are kept unchanged. Only the weights from each node to the output is tuned during training. This reduces the learning problem to a simple linear optimization. As it turns out, even a randomly initialized RNN is rich enough in behaviour that some cherry-picked nodes can provide whatever output is desired.   In practice, sometimes the networks work, sometimes they don't. How to optimize the fixed network for the task at hand is a big research question. You could perhaps say that the trouble in making RNNs work has been moved from the training phase to the network initialization. Early on, the field latched onto the "connectivity" of the network, claiming that a sparse weight matrix, which would yield a loosely coupled network, were the best. This would seem plausible, but in a surprising twist, authors started reporting that even fully connected networks performed just as well. Like a page out of Strogatz's book, it is argued that the "small world property" of sparse networks make them no less connected than highly connected ones. Apart from results related to the spectral radius of the weight matrix, a guarantee to in theory reproduce any nonlinear filter with memory and the general advice of tuning the timescale to match the scale of the system under study, there appears to be no general theory of when ESNs suceed and when they don't.     The basic schema of an ESN, illustrated with a tuneable frequency generator task. Solid arrows indicate fixed, random connections; dotted arrows trainable connections. (Source: Scholarpedia)   [1] Echo state network    Embed Quote