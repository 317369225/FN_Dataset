★I have an imbalanced dataset with two classes. Would it be considered OK if I oversample the minority class and also change the costs of misclassification on the training set to create the model?
Why change the costs? You can multiply the network output (probability distribution) by the prior probabilities to "un-distort" the distribution. Eg. you have a 10% minority class and a 90% majority class, and you sample them at 50%/50% (essentially taking out the prior probabilities). When you evaluate the network, you can just multiply the output for the minority class by 0.1, and the output for the majority class by 0.9, then normalize again if you need to.    Embed Quote