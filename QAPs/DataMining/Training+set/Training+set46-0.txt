★How should one go about solving a Multi-Class classification problem for a large training set without a large number of features? Are there any approaches which will work without having to load the entire data into memory?Would love to know about the implementation details too
One very popular is to train SVMs using Parallel Stochastic Gradient Descent that can be implemented in a Map-Reduce fashion. To apply it in a multi-class setting one can either: 1. Direct multi-class formulation. 2. Train N SVMs where each SVM discriminates between a given class and all other classes. 3. train N*(N-1)/2 SVMs, one for each pair of classes. Depending on your application you need to consider tradeoffs in training speed and query (prediction) speed, and the prediction quality based on the method used. As for the PSGD, the idea is to send subsets of training data to different machines, and train a separate linear SVM (can be implemented in the map phase of MR). The final SVM is obtained by averaging the SVMs trained on all machines (reduce step of MR). This approach is not limited to linear SVM since one can still apply the kernel trick and impose linearity by using some kernel approximation technique like Random Kitchen sinks or the Nyström method. The following papers are very useful: 1. Parallelized Stochastic Gradient Descent - Martin Zinkevich 2. Random Features for Large-Scale Kernel Machines - Ali Rahimi, Benjamin Recht 3. Using the Nyström method to speed up kernel machines -Williams, Seeger 4. Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison - Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou    Embed Quote