★Why would adding more neurons per hidden layer in a neural network hinder convergence within the actual training set?I know that overfitting can often be a problem when using too many neurons, however I thought this was only the case if the neural net is intended to be used outside of the training set. I am having the problem that by adding more neurons my neural net seems to not converge as closely as when using fewer neurons, and this is considering only its application with training set data.
I can't think of any reason why this would happen if it is all set up correctly. Are you randomly initialising the weights for each layer of the network?    Embed Quote