★Which language model would perform better for spam detection for a relatively large training set : Smoothed Unigram or Stupid Backoff with unsmoothed bigram with a back off to a smoothed unigram?In both cases using Naive Bayes for calculating the final result.
From the programming assignment of Stanford Online NLP course (www.nlp-class.com), I think the Stupid Backoff algorithm performs better than the smoothed Unigram model. In the slides of that course, the professor mentioned that Stupid Backoff algorithm is better in the case of larger dataset, for example Google N-gram corpus. When applied to spelling correct, the results of different algorithms are shown below (excerpted from the course website ): Laplace Unigram Language Model (a unigram model with add-one smoothing.): 0.11 Stupid Backoff Language Model (use an unsmoothed bigram model combined with backoff to an add-one smoothed unigram model): 0.18Because there are several ways for smoothing, such as add-one, good Turing, Kneser-Ney. So you can have a try in order to decide which one is more applicable to your work.    Embed Quote