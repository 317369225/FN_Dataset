★What are ways to prevent over fitting your training set data?Is it possible to "test" if you have done so?
As others have said, the best ways to address under-fitting and over-fitting are to use regularization and/or cross-validation. Regularization controls the penalty for complexity, which (when successful) will prevent under- and over-fitting. Cross-validation separates model selection from testing, resulting in a more conservative estimate of generalization. For the case of over-fitting only, obtaining more training data will also help. To "test" for under- and over-fitting, it can be helpful to plot your regression/classification error by some measure of complexity. For under-fitting degrees of complexity, training and testing errors will both be high. For over-fitting degrees of complexity, training errors will be low and testing errors will be high. (A) An example of overfitting in black (versus a good fit in red) (B) Underfitting on the left of graph, overfitting on the right of graph An alternative graph to use is a "learning curve" which plots regression/classification error by the number of training examples used. For under-fitting, training and testing errors will converge with larger training sets (and both be high). For over-fitting, the gap between training and testing errors will remain large (with the training error being smaller). (A) An example of a learning curve with over-fitting    Embed Quote