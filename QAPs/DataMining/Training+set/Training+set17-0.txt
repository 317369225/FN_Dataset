★Does training set need to have the same distribution as the target population in classification?
The proportion number of samples you should have in each class when training really depends on your error function. If you just throw a 95% positive/5% negative set into a Decision Tree, it will probably classify everything as positive because that will produce a very low error on the training set. That's why a 50/50 sample is typically recommended, because it forces the tree (or other classifier) to accurately separate the positive and negative events. If you have enough data, I would training on a small portion of your positive events so that you get a 50/50 set. If that isn't an option, the next best choice is to get more negative events. If that's still not an option, a third choice is to use regression instead of classification. Instead of asking your model to output a 0/1 classification, get a score from 0 to 1. Then you can validate slices of that set against a test set to come to conclusions such as "rows with a score from .2 to .3 are 80% likely to be positive."    Embed Quote