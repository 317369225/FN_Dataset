★In using Adaboost in Classification, typically how many weak classifiers  is expected to have? Also does an unbalanced training set (more - than +) greatly affect the classifier?for the Viola Jones for example?
The usual number is around 10 (E.g. for Weka the default is 10 http://weka.sourceforge.net/doc/...). People have run it with much more (notably: Grove and Schuurmans http://www-poleia.lip6.fr/~amini...). You can also determine it empirically using cross validation. Unbalanced training sets are tricky. But because AdaBoost maximizes the margin (i.e. focusing on the instances that are incorrectly classified), it's probably going to give you considerably better results on unbalanced training data compared to the weak classifier underlying it. As always with AdaBoost though, there's a risk of overfitting.    Embed Quote