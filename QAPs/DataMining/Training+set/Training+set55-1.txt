★I have an imbalanced dataset with two classes. Would it be considered OK if I oversample the minority class and also change the costs of misclassification on the training set to create the model?
This is an interesting problem, because we do not know how small is the minority class, for e.g. if it has 5 data points then sampling 50%-50% is definitely not a good idea because it is hard to build a classifier model with 5 data points that can generalize well. Oversampling the minority class using SMOTE or other algorithms has the disadvantage that it suffers from over-fitting. That is, you may perform well on the training set but on the test set your performance may suffer badly. Making a classifier cost-sensitive is a great idea, because if a class is in minority, it shows this is an important class but somehow it is difficult to collect those samples. For. e.g. a fall, or fault, or anomaly...which occurs rarely and therefore missing to identify it will be more costly than reporting an instance of majority class as an outlier. However, the major challenge is how would you infer those costs, unless you are domain expert. If you deduce cost from the data - it is bad approach because cost should be domain dependent and not data dependent. Another option is one-class classification, where you train your model on only the majority class and you only observe minority class during testing. However, the challenge is how would you decide the class boundary just by seeing the instances of majority class? Before employing any of these techniques, think which one suits your need and application.    Embed Quote