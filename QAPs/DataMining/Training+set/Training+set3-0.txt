★What are ways to prevent over fitting your training set data?Is it possible to "test" if you have done so?
Many machine learning algorithms come with a knob that controls overfitting. Typically in many algorithms you minimize some linear combination of the error on your training set and a regularization term, and there is a knob that trades off between these two terms. The regularization term, such as the squared norm of the weight vector in an SVM, penalizes the complexity of the learned model and favors simpler models. Too high a weight on the regularization and the model underfits. Too low a weight and the model overfits. For algorithms that don't have a regularization term, such as decision trees or neural networks, you usually still have some control over the model complexity. The more complicated a model that you are trying to learn, the more likely it is that you will overfit. So you could do things like reducing the number of parameters, for instance by reducing the depth of a decision tree, or sharing parameters, etc. And of course, it always helps to have more data. In the limit of infinite data of course, crudely speaking, you can never overfit. Pretty much the only certain way to check if your model has overfitted is to use cross-validation. In other words, split your data into two, train on one part, and test on the other. The sign of overfitting is a falling training error and a rising test (or validation) error.    Embed Quote