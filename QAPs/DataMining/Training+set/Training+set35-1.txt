★Which language model would perform better for spam detection for a relatively large training set : Smoothed Unigram or Stupid Backoff with unsmoothed bigram with a back off to a smoothed unigram?In both cases using Naive Bayes for calculating the final result.
This is wrong. It all depends on the data set. Stupid Backoff performs well usually on a very large dataset. Using it for spam detection is not very useful. I got much better results by removing the "stop words" and using smoothed unigrams. Stupid Backoff is also useful when applied to Noisy Channels for auto-correction for example. But since the nature and variations of spam are very broad, using n-grams is usually not a good idea. If training sets are unbalanced and not properly cleaned, using bigrams might sometimes put a ham message into the spam bag and vice versa.    Embed Quote