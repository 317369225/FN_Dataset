★What is an approximation algorithm?
I assume familiarity with complexity classes P (complexity) and NP (complexity). Succinctly speaking, consists of all the problems which can be efficiently solved. , on the other hand, consists of all the problems whose solution can be efficiently verified. is often misconstrued as "non polynomial time" but the correct way to interpret it is as "non-deterministic polynomial time". The equivalence between non-determinism and verification of a solution can be easily shown. The classic asks whether the two classes are equal i.e. whether a problem can be solved efficiently. This problem was first formulated by Stephen Cook in 1971 and has since remained the biggest open question of Theoretical Computer Science. It is one of the seven Millennium Prize Problems and carries a $ 1 M for the first correct solution. Ever since its inception, computer scientists have had little success trying to prove or disprove the inequality. Assuming , researchers started asking if we can find a near-optimal solution: one that is not optimal but not very far off from the optimal. Consider a optimization minimization problem, say Vertex cover problem. The problem is: Given an undirected graph , find a subset of vertices of minimum cardinality such that every edge has at least one endpoint incident at . A subset of vertices satisfying the above property is called a Vertex Cover (VC). We need to find the Vertex Cover of minimum cardinality. Let's the solution of above problem as . Clearly, cardinality of any VC will be greater than or equal to . Suppose an algorithm returns a VC which is always at most () times . We call such an algorithm as -factor approximation algorithm. Let the solution returned by be . We have                . In other words,                     . For a maximization problem, the approximation factor is defined in an analogous way. Note that here, . Analysis of an approximation algorithm is tough and there's no general way to do it. However, the design of approximation algorithm can be broadly divided into the following categories: Classical methods: Greedy algorithm or local search: Algorithms are easy to design but their analysis can be a bit tricky. Example: Set cover problem which admits an approximation factor of . Dynamic programming: Examples of this category include Knapsack problem and Bin packing problem. You can get arbitrarily close to the solution but never equal to the actual optimal solution i.e. and the running time depends on . For more information, refer: Polynomial-time approximation scheme. Convex programming based techniques: In these kind of methods, first write down the program as a convex program and then try finding its solution. I am assuming familiarity with convex optimization and theory of duality. If not, skip to the next section. Linear Programming: Write down the Integer programming formulation of the optimization problem and relax it to a linear program. Dual fitting: The greedy solution is interpreted as dual variables. Shrink the dual variables by dividing it by appropriate constant such that all the dual constraints are satisfied. This constant turns out to be the approximation ratio. Randomized rounding: Take an Integer Program, relax it to a Linear Program, solve the Linear Program and round it back to an integral value. Example: factor approximation algorithm for Set Cover where is the maximum frequency of an element. Primal Dual schema: This algorithm relies on Primal complementary slackness condition and Dual complementary slackness condition. Start with some initial value and update both the primal variables and dual variables such that all the constraints are satisfied. Semidefinite Programming: This technique is based on quadratic programming and relies on the theory on Positive-semidefinite matrix. This was started by a seminal paper by Michel Goemans and David Williamson who applied semidefinite programming to arrive at an  improved approximation factor for Maximum cut problem. Random Hyperplane projection: Write down the Quadratic programming for an optimization problem, relax it to vector program, change it to a semidefinite program by introducing suitable positive semidefinite matrix, solve it and round the solutions by projecting them on a random hyperplane. Primal Dual schema: This is a relatively new technique introduced by Arora & Kale where they use Matrix Multiplicative Weights Update Method (MMW) to update the primal and dual variables (as defined in Primal Dual schema for LP).    Embed Quote