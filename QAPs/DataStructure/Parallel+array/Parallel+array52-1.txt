High Performance Computing: What are the major differences in the classes of problems that can be effectively sped up using GPGPU (eg CUDA), Symmetric Multiprocessing (eg OpenMP), and Message Passing (eg MPI) respectively?
You are asking about three programming models here, but they are tied to hardware types. So you need to consider hardware in the equation too. Look at them in the context of a "cluster": each node in the cluster is probably an SMP, consisting of at least one multicore chip. A single multicore is an SMP, and multiple such "sockets" on a cluster node typically behave like an SMP. On the clusters that I work with, a node has 12 or 16 cores, spread over 2 or 4 sockets (chips), that can all access each other's memory. There are many "threading" programming models that can use this SMP-ness; the only one I'm really familiar with is OpenMP, but there is pThreads, Cilk, TBB, &c. Btw, the fact that this node is physically an SMP does not mean that you have to program it as such; see my next point. Such a node is not more powerful than a beefy PC, so if you have a really really  large problem, you let a thousand of such nodes work together by hooking them up with some network, typically infiniband. There are no compilers that can transform a simple program into a program for a cluster, so that's where you need MPI: you specify how data goes through the network. Now you have a cluster with SMP nodes, so you could do a hybrid approach of SMP / multithreaded programming on the node and MPI between nodes. As Greg Lindahl remarks, that is often not optimal: it suffers from the overhead of thread management, and it has its own version of Amdahl's law, in that you get penalized for sections that are MPI parallel but not SMP parallel. Therefore people often ignore the SMP part and just program 1000 nodes times 16 SMP cores as if they are 16,000 independent processors, and they spawn that many MPI tasks. Typically, the amount of memory per core is enough that this is feasible. GPUs. They are in a way orthogonal to this story. If you have a cluster as described above, you can attach a GPU to each node, and you can speed up your hybrid MPI/OpenMP code by including CUDA kernels. But let's now talk about overlap. MPI is needed if you have a really large problem; suppose your problem fits in a couple of Gigs, you then have the question to program it in a thread library (including OpenMP), or to attach a GPU and 1. solve it completely on the GPU, or 2. find some division of labour between CPU and GPU. For this I refer you to the answer by Julius Bier Kirkegaard, who has outlined for what sort of operations a GPU can be usefully employed. 