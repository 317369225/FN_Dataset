High Performance Computing: What are the major differences in the classes of problems that can be effectively sped up using GPGPU (eg CUDA), Symmetric Multiprocessing (eg OpenMP), and Message Passing (eg MPI) respectively?
tl;dr: Scroll down. A certain problem might consist of several sub-problems, each of which might be best approached by different parallelization schemes. Thus the methods can often be combined. Keep this in the back of your mind, when you read through the following. GPGPU A GPGPU is a vector processor. That means that if you are able to vectorize the problem, Cuda or OpenCL can probably give you a high speed-up. Here's a ridiculously simple example in pseudo code: array = [1, 2, 3, 4, .... 1000]for i in [0, 1, 2, ...999]:    array[i] = array[i]^2 + 2 Clearly array is already vectorized, and we would expect that a GPU-class would be able to understand this: array = piecewise_gpu_array([1, 2, 3, 4, .... 1000])array = array^2 + 2 If you are able to rewrite your code into this vector form, then GPGPU is the way to go. A GPGPU is designed to handle many, simple numeric operations in one go. Within a loop, only one operation is done at time. On a GPGPU, if the problem is vectorizeable, you can distribute all the calculations to their own, individual processing unit. These will then perform the calculations in parallel. In general, if you can put your problem in vector form, it highly advisable to use this method. You can expect insane speed-ups. The G in GPU, as you know, stands for Graphical. Good examples of vector problems are raytracer algorithms or particle swarms, which both are used in most modern computer games. For simple problems you should be able to get speed-ups in the hundreds or maybe even thousands. One drawback of GPGPU is the transferring of the data between the CPU and the GPGPU. This is insanely slow, and might be reason not to use this approach. However, if the calculations takes much more time than the transferring  then you're good. As an aside: Of course your GPGPU does not necessarily have as many processing units as your problem size, but it is a standard approach to program as if you had an infinite amount of processors. Multiprocessing (shared memory) Multiprocessing is probably the simplest parallelization scheme there is. It can almost always be used, but it does not scale to large clusters. Borrowing some C++ OpenMP notation, we can parallelize the program from above by writing something ala (still pseudo code of course) array = [1, 2, 3, 4, .... 1000]#pragma omp parallel forfor i in [0, 1, 2, ...999]:    array[i] = array[i]^2 + 2 The for loop is then spread over the CPU-cores that we have available. Multiprocessing is great and easy to use. The drawback is scaling. You rarely see multiprocessing used for more than 64 cores due to the memory sharing issues. So when do you want to use it: when a speed-up of maximum the number of cores you have available on a single computer is all that is needed. This is not a technique used for networks of computers. However, combined with a network technique it can prove really useful. You might use a different scheme to divide the problem across computers and then multiprocessing to further divide among the cores within the computers. Message Passing Interface Use this when nothing else suffices. MPI is extremely scalable. The biggest clusters in the world (with more than a million cores) all use MPI for parallelization. The drawback is that it requires more time to program. Most parallelizable problems will be parallelizable by MPI. So the only concerns are: is it easy (enough) to do? Do I really need this speed-up? Is my cluster big enough for this to make sense? Again let's look at the example from above. if processor == 0    array = [1, 2, 3, 4, .... 1000]    for i in [0, 1, 2, ...999]:        non_blocking_send(array[i], processor=i)message = blocking_receive(processor=0)non_blocking_send(message^2 + 2, processor=0)if processor == 0    for i in [0, 1, 2, ...999]:        array[i] = blocking_receive(processor=i) (This is very, very pseudo-coding. It is just to keep the example simple! Also I assume that we have at least 1000 processors) With MPI you have full control over which CPU does what, and you have full control over e.g. latency hiding. All in all, MPI is awesome, but a lot more work. TL;DR Use GPGPU if you can vectorize the problem and the data transfer to the GPU takes less time than performing the calculation itself. Use multiprocessing if you only need a speed-up of the number of cores you have in your computer. Do not use for networks of computers. MPI can always be used, but is a lot more work. It scales to as many computers as you want it to. 