I don't buy the story about complexity of searching a hash table: can someone explain?
By your logic, the cost of an individual key match is always going to be O(log n). Therefore, a linear scan is really O(n log n), not O(n); a binary tree is really O(log n log n), not O(log n); and a hash table lookup is really O(log n), not O(1). Now, let's generalize it a little. m is now the key length (which must be no less than log n if all keys are unique, but can of course be higher); n is the number of entries. An individual key match is O(log m), a linear scan is O(n log m), a binary tree is O(log n log m), and a hash table lookup is O(log m). But that's not how big-O notation is used. Big-O is inherently an estimate used to compare operations in a relative sense. That means we remove commonalities. The "log m" term is a commonality in all of these expressions, and one that obscures the interesting part of the behavior of these different data structures, which is that they scale at vastly different rates when the number of entries changes. So we remove the "log m" term, leaving us with the traditional time complexity expressions for them. Besides, it's worth remembering that in some cases, like platform-native integer keys, the match time is fixedâ€”it's a single CPU instruction that doesn't take more time for larger data. "log m" is actually not necessarily accurate. So basically, we treat the key matching function as a black box, and pretend it's O(1), because the speed of key matching is not the interesting part of the question. The interesting part is how organizing the data differently can require far fewer comparisons to locate a particular key. 