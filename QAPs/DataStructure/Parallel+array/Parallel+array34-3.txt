What is the fastest algorithm to find the largest number in an unsorted array with multiple processors?
As Ricardo mentioned in his answer, in order to find the actual highest number in an array, all elements in the array must be checked. There is no way around that. It's O(n). And he's very much right about it being easy to run in parallel. However, if the array is large enough — suppose we're dealing with an infinite array — checking all elements might simply not be doable at all even with multiple processors. In that case, you can employ a randomised method. Check n random elements in the array and use the highest one. You're no longer guaranteed to find the actual maximum, but you might be close enough for it not to matter. Here's a trivial example, in Python. The first function shows how to find the actual maximum, by looping over all elements in the array. The second shows how to find something close enough by looking at a random sub-set of the array. def max_correct(array):    # initialise the max variable with the first element    m = array[0]         for v in array[1:]:        # for each element in the array afterwards, check if        # that value is higher than the current max        m = v if v > m else m    return mdef max_montecarlo(array, n):    size = len(array)    n = min(n, size)    # initialise the max variable with a random element    m = array[random.randint(0, size)]    for _ in range(n):        # for a given number of iterations, pick        # an element at random and see if it's bigger        v = array[random.randint(0, size)]        m = v if v > m else m    return m This method is typically not used to find the maximum, but rather the average. It is called Monte Carlo integration, and it is exceptionally useful when dealing with so much data that it's simply not feasible to check all of it, or if you have a function that cannot be integrated exactly, and various other situations. It's used in non-realtime 3D rendering all the time. One commonly used method for 3D rendering is to trace the various paths that light can take through a scene. This represents an infinite set of light intensities. You can't compute all values in an infinite array or loop over it to find the average or maximum, but you can check a finite subset chosen at random, and if you check enough of them you'll be close enough for the error to be irrelevant. This is also the foundation for things like customer surveys. Since it's not feasible to ask all customers, you just pick a bunch of them semi-randomly, and extrapolate. Same principle. A few notes: The Python examples could be made a lot more compact and fast by using certain functions in the random module. I wrote it this way to make it easier to understand for readers without knowledge of Python. For small arrays and/or large values for n, it becomes more likely that an element will be checked more than once. This is easy to optimise away. You can use a semi-random number picker that is guaranteed to not generate the same number more than once, for example, or keep a record of all elements already checked, or build the sub-array in a structure that doesn't allow duplicates. The quality and reliability of this type of approach is strongly dependent on the quality of your random numbers. Both methods are equally multi-threadable.edit: fixed an error in the example code Updated 5w ago • View Upvotes