High Performance Computing: What are the major differences in the classes of problems that can be effectively sped up using GPGPU (eg CUDA), Symmetric Multiprocessing (eg OpenMP), and Message Passing (eg MPI) respectively?
The surface differences have been described already.  I think it's useful to think of the properties that each model can take advantage of. First, think of your application as a really big dataflow graph: particular hardware-atomic operations (multiplication, whatever) depend on operands, and those are the product of other ops, etc.  An important property is the "width" of this dataflow graph - especially when there are regular patterns, such as computing a particular operation on a regular sequence of data (vectorization). for GPUs, you would like very wide vectorization (tens of thousands), and very regular access patterns.  ideally, something like, well, graphics: running a short independent program (shader) for each pixel within a polygon.  shaders can run conditionals, but they hurt if nearby shaders take different paths (divergence).  (moving data between host and GPU is not that big a deal: it's mostly a latency thing, making short computations inefficient.  mundane systems can do more than a few GB/s.)  It's worth noting that conventional CPUs have hardware support for vectorization. for threading on CPUs, you want basically want a lot of computations to involve dataflow nodes with very high fan-out.  that is: a lot of data which is read-only or -mostly, since updating shared data is what kills performance for any threaded program.  with threading you don't need O(10,000) wide vectorizability, and divergence isn't an issue at all. MPI is shared-nothing - you have a bunch of completely independent programs running, which happen to be able to exchange messages.  of course, there's the old duality between messaging and shared memory (send is like a write, etc).  but there's a big asymmetry: a message takes about 1 us, but a shared-memory read takes 50 ns or so (assuming a cache miss).  so it's a lot more efficient to have threads accessing read-mostly shared memory, rather than explicitly sending those values.  in other words, it matters how cleanly you can partition your dataflow graph to avoid messages along the critical path.Conceptually, the most "virtuous" path is probably to use MPI first, then threading, then vectorization/GPUs.  That's because MPI forces you to be quite explicit about the dataflow, and then threading lets you optimize that by read-shared memory, then vectorization/GPUs let you take advantage of highly-regular and independent sub-graphs. 