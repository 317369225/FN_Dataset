What is the exact difference between parallel and concurrent programming?
Parallelism refers to physically doing more than one thing at once. Concurrency refers to logically doing more than one thing at once. So usually, the two terms refer to different levels of techniques.  In practice, since there is heavy overlap between the two classes, people tend to focus on the cases which are "concurrent but not parallel" or "parallel but not concurrent". Example of "concurrent but not parallel": (single-threaded) event driven architectures, such as gui event loops, or single-threaded event-loop-based message passing systems. Example of "parallel but not concurrent": vector operations, systems that provide the abstraction that you are the only thing running (ex: transactional systems). Example of "parallel and concurrent": any form of system-level multithreading. Example of "not parallel and not concurrent": single-threaded systems that we're used to. Both techniques are usually used for the purpose of performance, though you can find examples of concurrency that are required for correctness. Though perhaps the real answer is that "parallel" and "concurrent" are simply technical terms that we (hopefully) can agree on the meaning of.  There is probably some deeper reason why you are asking about the difference (ie you're not just wondering what people mean when they say those words), in which case it might be more helpful to ask directly about that instead of some less-direct middle layer. 