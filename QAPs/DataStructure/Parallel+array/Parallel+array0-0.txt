Does a dictionary object take up more memory than parallel arrays?
We need to more clearly define "dictionary" to answer this. In the most general version, "dictionary" may just mean "a data structure with some supporting operations that supports key-based access of values". In that case, your parallel arrays, plus methods that support accessing entries in the value array based on the entry in the key array, IS a dictionary. So (ignoring the size of the methods I just mentioned), no, a dictionary can take exactly the same memory as the parallel arrays version. Most of the time, however, dictionaries come with additional requirements. Rather than "key-based access of values", they promise efficient, key-based access of values. So that generally means that the dictionary internally with use either a sorted structure that allows O(log n) access of items, or a hashtable that allows O(1) access to items. If you just make sure that your parallel arrays are kept sorted by key, then again, your parallel arrays are still a dictionary; ergo the dictionary and the parallel arrays take the same amount of memory. And key accesses in this data structure will be marginally faster than using a tree because of fewer dereferences required -- and because the arrays are probably densely packed in memory while the tree is fragmented, so key lookups in the array will cause fewer page faults, which is a much bigger deal than the number of dereferences required. However, you'll pay the penalty of having to enlarge the array periodically, which will cause a periodic (and, unless your code is paying very close attention, apparently unpredictable) pause during insertion. With a tree, you don't get that pause. The parallel array will be faster overall for mixed read/write operations, and potentially much faster for read-heavy use cases, but the tree will show more predictable performance for insertions. If you're using a hash table -- which in many SDK's is the default: C#, Python, Ruby; Java provides multiple implementations, but defaulting to HashMap is usually the right decision -- then the dictionary will be somewhat larger than the parallel arrays. That's because efficient hash table implementations generally keep an array of larger capacity than the current number of elements in the table (hash tables are most efficient when they're less than 75% full; I could have that number wrong, just pulling it from memory). So at minimum, the hash table impl is probably 33% larger than the parallel arrays version. But many hash table implementations support bucketing when hash functions collide for distinct keys, and that comes with extra overhead (generally implemented as an array or list; so for each hash bucket that contains an entry, add in however much overhead the list takes). Bottom line: most of the time, a dictionary is a hash table, so yes it takes more memory than parallel arrays. But not always. 