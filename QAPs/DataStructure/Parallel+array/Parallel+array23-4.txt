I don't buy the story about complexity of searching a hash table: can someone explain?
First of all, I'm impressed that you noticed that there is an issue about the time it takes to read and process the key. That being said, I think you're reading into it a bit too much, and you're misunderstanding a small part. When we say a hash table has a lookup time of O(1), we are talking amount of time based on the number of number of entries in the table (not the length of the keys). What we're saying is that it takes the same amount of time to lookup something in a hash table of size 1 as it does to look something up in a hash table of size 1 million. Here's the part you misunderstood. You said that a hash table of length n will have keys of length log(n). The size of the hash table has no relation to the size of the keys. If there was a relation ship between the size of the keys and the length of the hash table, a key like "LookMeUp" (which is of size 8), would only be usable if the hash table had somewhere around 256 items (2^8). Yet, if I were to search for the key "LookMeUp" on a hash table of size 1 not only would it work, but it would take about the same amount of time as searching for it in a hash table of size 1 million. You are confusing size n (the size of the hash table) with size m (the size of your key). Please don't do that, they are not the same. Here's the part that I'm impressed with, you realized that the time complexity of looking up an item in the hash table is dependent on the time complexity of the hashing function (not to be confused with hash table). Unless we're doing something involving security (or academia), we are going to pick a quick hashing function. We are going to pick a hashing function that is so quick, that it doesn't matter if it runs in O(n^4)*. Some programs enforce a max bit size on your keys. If there's a max key size, then we can take that size, plug it into an equation that measures the number of steps it takes to compute the hashing function, and get back some constant named 'c'. Big O of 'c' is the same as Big O of '1'. Because of this, it doesn't really mater most of the time. I don't really think we'll be using any hashing functions that run in O(m^4) time. While it is possible to write a hashing function that runs in O(log(n)), where n is the size of the hash table, I don't think that would be a really good hashing function. A hash table like that will be filled with a lot of collisions that could slow it down to a speed that's a lot worse than O(log(n)). If you really think the hash table is looking things up in O(log(n)) time, then you might as well post the code for this hashing table so that the rest of us can take a look at it. * When I said O(n^4), we were talking about the computation space of the hashing function. Since we are talking about the computation space of the hashing function, which is dependent on the length of the key, it is assumed that the reader understands that n represents the size of the key for this specific case. =============================== Answer re-write based on comments between Anonymous and me. =============================== When analyzing the computational space (and memory space) of an algorithm or data-structure, we (computer scientists) tend to run these algorithms and data-structures on theoretical machines. This lets us make simplifying assumptions that let us focus on what we're trying to accomplish. Why do we use theoretical machines instead of a real machine when we do our initial analysis? I'm not certain how to answer that question. And, if you really want to answer that question, I would advise you to start by looking at the “Theory of Computation” article in Wikipedia. Anonymous is asking why it doesn't take the machine an amount of time that is equal to the number of bits used to represent the hashed key under which we are storing the actual key in a hash table. I'm going to start by defining the terms I'm going to use: Big Int: This is a data-structure used to represent numbers that surpass the size limitations of the standard integer structures that are used. You could call this an n-bit number. Hashed Key: This is a number that we get by putting our key through a hashing function. Each Key should have it's own Hash Key, but there are collisions. Hash Key Collision: A Hash Key Collision can be caused by two things: The hashing function gives the same Hashed Key for two different keys Two different Hashed Keys full into the same location of the Underlying Data-Structure used to store our collection of items. Key: This is the item (or entry) that we are looking up. Underlying Data-Structure: This is a data-structure inside of the Hash Table Structure that actually holds our collection of items. When looking up an item (or Key) in the Hash Table, certain steps need to be taken: Step 1. Pass the Key into a hashing function that will return a Hashed Key. Step 2. Use the Hashed Key to get a collection of items from the Underlying Data-Structure. Step 3. Check if the smaller collection of items holds our Key. The collection may hold more than one item when we have a collision. Step 2 should always take the same amount of time despite the number of items held in the Underlying Data-Structure. In other words, it shouldn't matter if the Hash Table is holding 1 item or 1 million items, the amount of time to perform Step 2 should be the same. Because of this, we say that the time space for a Hash Table look up runs in O(1). For well implemented Hash Tables this is true, within certain limits. The hash function implementations I've looked at all return a number for a Hashed Key. We usually place that number inside of an int or a long, but we could go with a short or even a Big Int. Except for the Big Int, the structure we use to hold our Hashed Key will place an upper limit on the number of Hash Keys that are possible. A 32-bit integer will only have 2^32 possible Hashed Keys, and a 64-bit integer will only have 2^64 possible Hashed Keys. While I'm no expert when it comes to Hash Tables, I've never heard of a Hash Table that uses different structures to hold the Hashed Keys. Once you implement a Hash Table to use a given structure (like an Int8) for the Hashed Keys, then all the Hashed Keys will be held in with that structure. It doesn't matter what number that Int8 is holding; it should always take up the same amount of space on your computer and computation time. This means that the hardware should always take the same amount of time to read and/or compute an Int8. It takes the same amount of time to read 00000001 (aka 1) as 01010110 (aka 86). Therefore, the size of your Hashed Key will always be the same no matter what value is held in that Hashed Key. Since the size of your Hashed Key is always the same, the value in the Hashed Key won't affect the look up time in Step 2. Because of this, there really isn't much of a reason to look at the size of the Hashed Key when calculating the time complexity of a Hash Table look up. Now, all that being said, it's possible that you might be using an Int64 on a piece of hardware designed for an Int16. In this case there may be an issue where some values take longer to process than others because the math will need to be done at the software level instead of the hardware level. Or, you might not have enough memory to hold the entire Underlying Data-Structure in memory at once, causing paging issues. There are all sorts of things (at the hardware level) that can complicate the look up time on a Hash Table. At this point, we are no longer talking about the Theory of Computation. I've never worked with a Computer Hardware Engineer, but I've heard that Computer Scientists and Computer Hardware Engineers are always fighting over how how a physical computer should work. 