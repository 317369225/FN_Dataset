Are all O(n) algorithms "Embarrassingly parallel"?
Take many graph theoretic algorithms that take linear time, many times it isn't embarrassingly parallel.  Usually these involve exploring a graph and unfortunately depending on the model of computation (and architecture) this could be easy or hard to parallelize.  An embarassingly parallel program usually just means you distribute the work of something like a loop over the processing units, you won't be able to do that with something like a graph problem very easily.  Which cores should have what nodes?  How do they know eachother has them?  Do they pass information about the graph around?  Do they adopt a master-slave approach where one is assigned the root (if possible)? 