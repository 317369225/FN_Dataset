I don't buy the story about complexity of searching a hash table: can someone explain?
The confusion is about what you decide to count and why. If you count nanoseconds, or whatever, time, then you are absolutely correct that asymptotically comparing the keys would take longer and longer if you put more and more of them in the hash. However, people don't like this kind of analysis, because then the complexity analysis of the hashing operations would depend on what kind of keys you are storing. The analysis for hashing, say, Strings would become completely different from the analysis for hashing integers, because key comparisons for Strings can take a long time. That's no good - we want a black box analysis that does not care about the contents of the hash! So instead, we measure the efficiency of the dictionary data structure in terms of the number of key comparisons. We can then say that our hashing operation stores a new key using only [math]O(1)[/math] key comparisons, as opposed to a balanced BST, or a binary search, which both require [math]O(\log n)[/math] key comparisons. This is a much more useful analysis because it does not depend on the kind of objects we're storing in the hash! In many practical applications the keys are all 32 or 64 bit integers. No existing hash table stores more than 2^64 entries, and key comparisons are not faster for small integers than they are for large integers. So for such applications it also makes sense to pretend that a key comparison takes a constant amount of time. 