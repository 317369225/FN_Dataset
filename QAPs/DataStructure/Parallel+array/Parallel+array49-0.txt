Are there any programming concepts not yet implemented? What are they?
Sure. I can think of a bunch of fun ones. Region-Based Memory ManagementRegion-based memory management is basically static garbage collection: heap-allocated variables are freed deterministically based on information known at compile time. Variables are allocated in "regions", with the restriction that pointers between regions can only go in one direction, which lets the language free whole regions at the same time. Unfortunately, it turns out that automatically inferring regions for variables is quite a  difficult problem, so there aren't any particularly compelling implementations. One immediate problem is that it's too easy to get a large region that lives as long as the whole program, which can heavily inflate memory usage; to be practical, this region has to be managed specially—it has to have a garbage collector itself! These issues have kept region-based memory management, as awesome as it sounds, out of popular programming languages. The only example I know of is in Cyclone, a safe system language based on C. Cyclone is not in active development any more. It's too bad, really. Rust's lifetimes are a similar feature which seems very promising, but I don't know how it feels in practice or how it plays with inference. Refinement TypesRefinement types let us define "subsets" of types that only contain elements that satisfy a certain predicate. For example, we could imagine a type of odd integers: { x : Int | x % 2 == 1 }. Refinement types would be a natural compliment to Hindley-Milner style type systems since those are traditionally awkward for working with overlapping types like different sets of numbers. A similar effect can be achieved with dependent types, but they tend to be too expressive which makes them significantly more difficult to use for most tasks. Practical refinement types require decidedly non-trivial computation at the type-level, since they need to be able to enforce invariants based on a large variety of predicates. Essentially, you need the capabilities of a normal programming language at the type level, except with enough limitations to make analysis and compilation tractable. Happily, SMT solvers have recently emerged as a practical foundation for working with predicates over things like integers, bitvectors or even floats symbolically, which makes it easier to implement a useful set of predicates inside the type system of a language. Even so, no popular language I know currently supports refinement types. Refinement types have been implemented as third-party extensions to popular languages. One project of note is UCSD's Liquid Types, which adds refinement types based on an SMT solver to languages such as Haskell (LiquidHaskell), ML or even C. Substructural TypingSubstructural typing extends a type system by changing how often a variable can be used. The simplest and most useful example is linear typing, which restricts a variable to be used exactly once. This is very important for memory management as the variable can be overwritten once it has been used: it's guaranteed to never be used again. This also changes the sort of logic the language corresponds to, and makes it a viable model for quantum computation since it ensures information is not duplicated or destroyed. Substructural typing also extends to other sorts of rules, notably affine types and relevant types. Affine types restrict variables to be used at most once and relevant types restrict variables to be used at least once. Linear typing is actually making it into the mainstream: Rust uses linear types for safe deterministic memory management. However, I don't know of any language that supports other kinds of substructural typing, much less providing substructural types as a generic feature of the type system. Optimistic EvaluationOptimistic evaluation is an alternative to laziness for implementing non-strict semantics (ie Haskell). Instead of always delaying the evaluation of an expression behind a thunk, an optimistic system would speculatively evaluate all expressions with some mechanism for aborting if the expression either takes too long or produces an exception. This would avoid many of the time and space issues presented by simple lazy evaluation and take more advantage of parallelism. This is quite similar to the speculative execution modern CPUs do behind the scenes with branch prediction and pipelining. A CPU is perfectly happy to start running the branch it thinks will be chosen before it has finished evaluating the condition; if the condition fails, that evaluation is just abandoned. This would apply the same idea at a higher level for non-strict languages like Haskell. Speculative evaluation was designed and implemented for Haskell at one point, but the actual performance gains proved limited. The feature was ultimately abandoned because the increase in complexity for the compiler did not justify the modest performance advantage. It was great stuff -- and some lazy programs went a lot faster -- but it was also jolly complicated when all the details were worked out, and interacted with a lot of other tricky details of the run-time system. In the end we decided it was just too complicated to permanently incorporate in GHC's code base, alas.  - Simon Peyton-Jones on the fate of optimistic evaluation in GHC.   Automatic ParallelismAutomatic parallelism is exactly what it sounds like: automatically breaking up and distributing code over multiple cores. This is something of a "holy grail" for modern parellel programming and, like the actual holy grail, remains ever elusive. Most languages make automatic parallelism essentially a non-starter due to pervasive side-effects. However, even in a side-effect free environment like Haskell, it is very difficult to figure out ahead of time what level of parallelization would actually lead to an increase in performance. We can evaluate pure Haskell code on multiple threads without worrying about correctness, but unless care is taken chances are the code will actually run slower overall. It's all too easy for the necessary multithreading bookkeeping to dominate any gains from using multiple cores! Automatically parallelizing general-purpose code and actually getting a consistent improvement in performance is still an open problem. Happily, we can already get part of the way there with Haskell. Libraries like Parallel Strategies make it easy to manually control the level and granularity of parallelism for a specific task, still without worrying about correctness. There are also libraries that actually do automatic parallelization for specific domains like regularly shaped polymorphic arrays (REPA), but are not general enough for other kinds of applications. A project that's particularly worth following is Data Parallel Haskell which extends automatic parallelization to irregular (ie ragged) arrays. It's currently still very much a prototype, but is already showing a lot of promise. All of the features on this list are very exciting—but also very hard to realize. Hopefully, we'll start seeing practical versions of these features emerging in the next five to ten years and filtering down to popular programming languages some time after that, but right now it's tricky to predict whether any of them are even possible! 