What are the best practices for implementing pagination on a website with a large amount of data?In my case: I have to group and sort about 1.5 million items for pagination (average number for each category) - MSSQL server makes this but slowly.
The best optimization is to create an index that you've designed to specifically work well with the types of queries you'll run. Depending on your application, a lot of the time you can precompute the rankings of items that you're going to show. Let's say you want to create a directory of products indexed by keywords and ordered by some arbitrary score, so the first page of results for the query "flowers" would give you the highest scoring flowers-related pages out there. In practice, you might want to divide the load across several machines and there are a bunch of ways to do that (for example, one server might handle all queries starting with the letter "f" assuming that the load is uniformly distributed across all letters). You can add sharding later on if necessary, but sometimes it's cheaper to stuff a ton of RAM into your machine. Replicating your database also distributes load (cool fact: if pick the less loaded of two randomly chosen servers, you can exponentially reduce the expected maximum load compared to randomly choosing one! [1]). At its simplest, your lookup table would have three columns: keyword, product_id, and rank. Notice that the keyword and rank are precomputed offline; you really don't want to be computing this information on the fly. Create a composite, clustered index on the keyword and rank columns, in that order. The composite index makes it efficient to queries rows first by a keyword and then by their rank relative to other rows with that same keyword. The clustering means that rows with the same keyword and similar ranks will be close to each other on disk (preferably in the same database page) so they can be read sequentially and exploit spatial locality. In MS SQL, the composite index will likely be two B-trees which should be fine; the tree index on rank makes range queries efficient. In theory you could use a hash index for the keywords and B-trees for the ranks. Some DBMSs will let you partition the index as well. Once the database is set up, you can retrieve a range of results. SELECT P.* FROM products AS P INNER JOIN lookup AS L ON L.id = P.product_id WHERE L.keyword = 'flowers' AND 0 <= L.rank AND L.rank < 10; It should be easy to map page numbers in your application to ranges of ranks, and vice versa. Alternatively, if you don't want page numbers and just have "Next" and "Previous" links you can change the range query to 0 <= rank LIMIT 10, and then use the ranks of the first and last results to determine the queries for the next/previous pages. Finally, memcached helps a lot as Jinghao points out, especially if you are performing joins in your query later on. Memcached is also great for serving hot queries very quickly. I built something similar to this at Facebook two years ago. On a cache miss, the mysql_query calls take less than 12 ms. Some slides for further reading: http://www.scribd.com/doc/146832... [1]. Y. Azar, A. Z. Broder, A. R. Karlin and E. Upfal, Balanced Allocations, Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, pp. 593â€“602, May 1994. 