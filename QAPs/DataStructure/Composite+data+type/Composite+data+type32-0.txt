Why are C# and Java not considered as Object Oriented Language compared to the original concept by Alan Kay?Today I just learned that C# and Java don't support Object Oriented Programming (OOP) if judged by the standard of the original concept coined by Alan Kay.  Does that mean that all the things we've been learning about OOP and reading in books are a complete non-sense? Could anybody explain to me why?
Almost the TL;DR answer is that Kay's conception of objects was analogous to the way the internet is structured, if you can imagine servers and routers as "objects," the data packets that are sent over the network as "messages," and the protocols that are used over it as "interfaces." The way I'd classify Java and C# is that they enhance abstract data types. I thought for years that ADTs were really OOP, because I recognized this quality of C++ and Java, and later C#. Everyone I respected was calling it OOP. So I associated the two. It was only after listening to Alan Kay talk about his concept for a while that I understood that ADTs are a pale shadow of OOP that I think only a computer scientist of average stature could mistake for it. Really, the sense I get from listening to him is that OOP is not really well defined yet. There was a decent attempt at it that was done in the 1970s, and early '80s, called Smalltalk, which I will describe below, but it's an idea that can and should be criticized for improvement. There are many reasons the languages you mention don't conform to what was his conception of OOP. These are the main ones I can think of: One, they don't really do message passing between objects. Method calls are bound early, when you compile the program. In Kay's conception, objects completely control their own environment, including method dispatch. (Again, think of a server.) Objects are capable of interpreting the messages they are sent at run-time, and responding to them differently depending on who sent them. In C#, at least, there is a way of doing something like this, but you end up violating encapsulation in the process, because you reveal how the message is sent. Secondly, it's not a standard part of .Net. It's a "feature" that can be used with your own classes (not with Framework classes). Second, in these languages, the objects are considered the abstraction. In Kay's conception, the messages are the abstraction. Objects are just endpoints for messages. I gave a simple example of these points at Mark Miller's answer to Why Smalltalk?: What features of Smalltalk are important for gaining insight into better object-oriented design/programming? Third, Kay was after a "no centers" system design. The larger goal was to create an architecture that could scale from the very small to the very large while maintaining its coherence, and not sacrificing reliability. He actually wanted to "kill" the idea of applications, because they create stovepipes of functionality and data. His idea was that you could create a network of relationships between objects, and those relationships would be established by messages, sent by senders, and interfaces set up by receivers. And of course, most objects would be both senders and receivers of messages. This gets into overall system design, not just language design. A fourth point gets into overall system design as well. In his conception of objects, they should be capable of doing everything a computer can do. So, as a few examples, they should be able to render themselves, communicate over a network, and persist their own state. They can do this in conjunction with other objects, for example objects that specialize in communicating over a specific network, with the graphics hardware/subsystem, and storage hardware. This kind of separation of responsibilities would be important, since hardware on a system can change, and you don't want to tie an object's rendering implementation, for example, to specific hardware. Frequently what one sees in systems that use Java and C# is that data is "wrapped" in objects for brief periods of time in memory, while it's being used and manipulated, but then it's "unwrapped" from objects when it's stored, and just stored as raw data again. This is not in keeping with his concept of OOP. The reason he chose this method of organization is he realized that the scheme of having procedures as separate entities from data structures didn't scale. To create something that scaled, you needed to have code be a metaphor for data. You could still work with data representationally (such as the number 2), but "2" would in fact be a "live" object, with its own functionality. Fifth, most of the features of these languages are bound early, when a program is compiled. In .Net there's been some late-binding with DLLs, but objects inside those DLLs are still bound early.  In Kay's conception, there was late binding to all objects. If you look at Smalltalk source code, you can kind of make out that it doesn't look too different from Java or C# code, but in fact the runtime does a lookup in its database of classes when an object is instantiated, and an object does a lookup of its methods when a message is sent to it. Messages are not bound at compile-time to methods. Messages are matched to methods at run-time. A reason for this can be surmised by using the "internet" analogy that I posed at the top of this answer: It would be disastrous for a network of such large size if every time a server was taken down, or a new one was added, or a new end-node (PC, or subnet) was connected to it, that the entire network would need to be taken down to do it! The system would be useless if that was the case! In the old implementation of Smalltalk, the way "broken links" were resolved was the debugger that was built into the system would prompt the programmer, saying something like "unrecognized class," or "object does not understand message: X," and leave it up to the programmer to find another object or message the code could use, and then let the thread continue operating. No processes had to "die" to fix code. Code could be fixed in situ. In Kay's conception of a development system, it should be possible to modify code while it's running. Early-bound systems are not designed for this, though a couple implementations from Microsoft have shown that it can be done in a limited way, using a feature they call "edit and continue." However, the process has to be stopped (in a debugger) to modify code. In Kay's system, the process did not need to be stopped while code was being modified. You didn't need to activate a debugger to do it. You could just be running a thread, and modifying the code that was running it at the same time. This enabled experimentation with a "live" program. The only similar example of this I've seen on a modern operating system is F-Script on OS X. Sixth, Smalltalk is strongly typed, but unlike in Java and C#, it does not depend on type tags to establish types. Instead, types are established by interfaces, which dictate what messages an object can receive, and what interfaces are expected of the objects that are sent in the messages. Interfaces are not tagged. They are merely established by specifying the messages an object can receive, and what messages are sent to objects. Since interfaces can be shared by multiple classes, types go beyond class boundaries. This goes back to the idea that the abstraction truly is in the messages, not the objects. Interfaces are a "gentleman's agreement" in Smalltalk between senders and receivers. I talk more about Kay's conception of OOP, generally, at Mark Miller's answer to What is Alan Kay's definition of Object Oriented? Re. inheritance Seeing some of the other answers, I thought I should say something about this. From listening to Kay, inheritance was something they tried at Xerox PARC as a way of easily allowing programmers to modify existing behavior. I get the sense that he thinks that was a weak solution that created more problems than it solved. From my read of the experience OO programmers have had in the past, the recommended course of action is composition. Instead of using inheritance, if one wants to modify a class's behavior, it's better to wrap an instance of the class whose behavior you want to modify inside another class that conforms to the same interface. Updated 4 Aug • View Upvotes