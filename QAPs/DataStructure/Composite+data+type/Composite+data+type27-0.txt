What is the most efficient way to sort 4TB on a single machine with 4GB of RAM?
I vote for an external merge sort that uses a local in-memory quick sort. This approach assumes that the sort does not need to be done "in place". It also assumes that the external store (such as a hard disk) does faster sequential access than random access (e.g. reading off whole tracks in track order). 1. Break the 4TB into 1000 4GB data sets. 2. Load, quicksort, and write out each 4GB data set. 3. Merge the 1000 data sets in a single pass, writing out to a new store as you go. Step #2 will involve one full sequential read and one full sequential write of the full 4 TB data set. The quicksort steps will be "instantaneous" relative to external store read-write time. Step #3 will involve one full semi-sequential read and one full sequential write of the 4 TB. The read is semisequential in that whole blocks can be read and processed at once, but all 1000 lists need to be read in parallel, which is close to random-access. To be smart about it, one can read 1000 separate 4MB sequential blocks to increase the amount of sequential reading.  Or, more cleverly, keep 1000 separate 4MB read buffers in memory and fill them strategically as they run low using an elevator disk access algorithm. This algorithm will perform > 2x faster if two hard disks are used instead of one. The reason is that with two disks, the reading and writing can happen in parallel while substantially reducing track-to-track head travel "thrashing". Updated 6 Mar 2012 • View Upvotes