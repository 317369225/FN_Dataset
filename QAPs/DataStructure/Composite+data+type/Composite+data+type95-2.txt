How do Bayesian algorithms work for the identification of spam?Please provide an example or two.
I assume by "Bayesian algorithms" you mean the naive Bayes classifier. The Graphical Model: The naive Bayes classifier is a simple probabilistic model with strong independence assumptions. In particular, the underlying Bayesian network consists of a single node Y  (whether the email is spam, for instance) that determines a set of features F 1 ,...,F n   independently (with F i   corresponding to the word at position i  , for instance). For example, if the first word is "Free", the email is more likely to be spam than if it is "Hey" (Or more accurately, spam is more likely to begin with "Free" than Hey). The graphical model is below: Based on this assumption, we know that the distribution of Y given the features (words at various positions) is Pr(Y|F 1 ,...,F n )∝Pr(Y)∏ i P(F i |Y)  (Recall that Pr(Y|F 1 ,...,F n )=Pr(Y,F 1 ,...,F n )Pr(F 1 ,...,F n )   by Bayes rule). In other words, to determine the distribution, you simply compute Pr(Y)∏ i P(F i |Y)  and normalize the final distribution. Remember, Pr(Y|F 1 ,...,F n )  is a distribution where, in the case of spam identification, the two possible values of Y are "Spam" or "Ham". Training (Parameter Estimation): Now that we understand the graphical model underlying the algorithm, let's see how we train the model. The input to the trainer is a set of emails that are hand-tagged with "Spam" or "Ham". During training, we need to compute various probabilities like the probability that a certain word is emitted if the email is spam, and the probability that a certain word is emitted if the email is ham. The most basic way to do this is the Maximum Likelihood method, which sets the parameters P(F|Y)  equal to the exact ratio of a specific word appearing when Y=spam and Y=ham (i.e. the distribution that would maximize the likelihood of our observation). There are many issues to this, in particular data sparsity, that have a wide range of creative solutions involving smoothing, that I won't get into. Classification (Spam Identification): We can now use the computed P(F|Y)  s to classify novel emails. We would compute Pr(Y=spam,F 1 ,...,F n )  and Pr(Y=ham,F 1 ,...,F n )  , where the features F i   are the words at position i  . Then we classify the email as spam if the former is higher, and ham otherwise. Remember, the joint probabilities Pr(Y,F 1 ,...,F n )  are proportional to the conditional probabilities Pr(Y|F 1 ,...,F n )  , so it suffices to compare just the joint probabilities to see which conditional probability is higher. Discussion and Criticisms: The Naive Bayes classifier has many weaknesses, in particular the whole underlying assumption of independence of features: Words in an email aren't at all independent. As a result of the strong independence assumption, the classifier gives much stronger predictions than are justified (i.e. the probabilities are much more peaked). For a better discussion of the algorithm and its drawbacks, in addition to some simple examples you can walk through, you can refer to the Wikipedia article for http://en.wikipedia.org/wiki/Bay... and http://en.wikipedia.org/wiki/Nai.... Updated 11 Sep 2012 • View Upvotes