What is the role Big Data will play in materials science research?
"Big data" usually refers to the challenge of analyzing, capturing, storing, and sharing large volumes of data, often being generated at large rates. Storage, sharing, and generation are typically not at the scale of "big data" in materials property data, at least right now. As far as I know, all existing databases are easily accommodated by relational databases (would love to hear if otherwise though). However, I imagine the term "big data" in the question is being used to refer to predictive analytics, and Inna Vishik's answer touches the major points. I'll just provide some tangible examples of where statistical learning can add value through prediction. Ultimately though, statistics serves to supplement understanding driven by experiments and theory. Surface and interface science. "God made the bulk; surfaces were invented by the devil." -- Wolfgang Pauli      Surfaces and interfaces are hard but they're incredibly important for materials science applications such as catalysis. A lot of materials chemistry in this area is driven by using quantitative structure–activity relationships -- a convoluted term for statistical correlations -- to identify new materials and understand governing factors.      Surfaces are difficult to calculate from first principles because we often don't know how the surface really looks: it's an extrinsic property of the material, dependent on how it was prepared. They also depend on the environment the surface is exposed to. Interfaces with liquids are especially challenging because the approaches for computing the behavior of solids and liquids typically require different computational schemes (density functional theory for solids and molecular dynamics for liquids). Experimentally, there's a large concern of the observer effect on the chemistry of the surface, if not the surface itself. Most of the conventional surface science techniques such as synchrotron spectroscopy and scanning tunneling microscopy can cause degradation or alteration of the surface, as well as how chemical species interact with it.      Bulk properties are much easier to calculate and measure, so a lot of work explores correlations between the chemical behavior of the surface with the material's intrinsic properties. Consequently, feature selection methods are still effective tools for developing understanding in these areas. Structure prediction.      First principle tools are powerful for identifying stable structures but one needs to calculate a huge chemical space of potential crystal structures. This is incredibly costly computationally. Machine learning is useful for identifying probable candidates at much lower expense -- I've talked more about that in my answer to Is machine learning helpful for density functional theory? Materials synthesis and metastable properties.      Prediction isn't limited to quantitative data. Synthesis condition prediction is a really valuable area of research that hasn't gotten much attention, probably because it's incredibly difficult. The Olivetti group at MIT is working on predicting materials synthesis and processing conditions using text mining and machine learning. This should be distinguished from the previous point, which is driven by identifying stable structures rather than how to synthesize metastable systems or stable materials with optimal properties for a given application.      Especially for metastable systems, the material properties are determined by the kinetically trapped state, which may or may not be accessibly determined from first principles. Glasses come to mind as a particularly challenging system to predict material behavior from first principle models.In many ways, one can think of statistical learning as a useful engineering tool wherever combinatorial materials chemistry is useful. The two are complementary for screening large chemical spaces and identifying intrinsic material properties that may govern material response to external stimuli. 