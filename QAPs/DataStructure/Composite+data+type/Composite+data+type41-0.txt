In Hadoop MapReduce, is it possible to support multiple reduce methods for the same Mapper input?
Short answer - yes you can do it - but it _may_ be a pretty bad idea. Long answer - just define your map output key to be a composite key that combines the metric-type and the actual key (for that metric). The keys are now grouped by metric-type and key and the reducer can invoke a different reduction method depending on the metric type. The reason it may be a bad idea is that the primary overhead of map-reduce comes from sorting all the data. In this case - you could have written 'n' (=4) map-reduce tasks that each made a full scan over the data and did sorting/grouping/reduction for each metric type separately. By sorting all four outputs at once - we may be greatly increasing the cost of doing this processing. The above assumes that the amount of data to be sorted is large. If the map output is small (you are doing substantial reduction in the mapper itself) - then it's likely that the sort overhead is small compared to the scan overhead. In which case combining all map-reduce jobs into one will make more sense. As an fyi - where such choices arise in Hive - we typically scan the data once and split the output (using a map-only job). Then we run separate map-reduce jobs on each split (where the mapper is likely an identity mapper). 