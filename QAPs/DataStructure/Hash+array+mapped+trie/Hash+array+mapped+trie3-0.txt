Is object immutability in functional programming inherently performance intensive?
Some functional programming languages (or most?) depend on persistent data structures, which eliminates the need to copy the entire structure. I became aware of this because of the Clojure persistent vector, discussed here: Understanding Clojure's Persistent Vector, pt. 1 which refers to the Hash Array Mapped Trie by Phil Bagwell: Page on lampwww.epfl.ch Martin Odersky mentions that this data structure: Persistent data structures in Scala More background from Martin and Phil Bagwell's work in a post, sadly on the occasion of Bagwell's funeral: R.I.P. Phil Bagwell However, when speaking from a performance perspective, this will depend on the garbage collector. Even if the garbage collector has good performance in terms of CPU, there may be a large amount of temporary objects present, making this approach expensive in terms of working memory. However, Clojure and Scala are based on the JVM and have their low-level parts written in imperative Java; it may be better to have a system made purely for functional programming. For example, Haskell GHC says this: https://www.haskell.org/haskellw... Haskell's computation model is very different from that of conventional mutable languages. Data immutability forces us to produce a lot of temporary data but it also helps to collect this garbage rapidly. The trick is that immutable data NEVER points to younger values. Indeed, younger values don't yet exist at the time when an old value is created, so it cannot be pointed to from scratch. And since values are never modified, neither can it be pointed to later. This is the key property of immutable data. 