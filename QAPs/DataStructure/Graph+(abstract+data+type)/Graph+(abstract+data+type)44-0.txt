What does one mean by 'elegant' code?I have heard some programmers, especially those who are fond of functional programming, state that it is easier to write 'elegant' code in Haskell than in Python or Java. What do they exactly mean when they tell that? Examples would help me a lot because I am relatively a newbie to the world of functional programming.
It's very closely related to elegance in mathematics. Elegant code is simple, gives you some new insight and is generally composable and modular. These qualities, although they may look almost arbitrary, are actually deeply related, practically different facets of the same underlying idea. Simplicity The biggest one, perhaps, is simplicity. But remember: simple is not the same thing as easy. Just because some code is very simple does not mean it is easy for you to understand. Easiness is relative; simplicity is absolute.  This is especially relevant for Haskell: often, the most elegant Haskell code comes from simplifying a problem down to a well-known, universal abstraction, often borrowed from math. If you're not familiar with the abstraction, you might not understand the code. It might take a while to get it. But it is still simple. Simple code like this is also often concise, but this is a matter of correlation, not causation. It goes in one direction: most elegant code is concise, but much concise code is not elegant. One way of thinking about simplicity is that there are fewer "moving parts", fewer places to make mistakes. This is why many of Haskell's abstractions are so valuable—they restrict what you can possibly do, precluding common errors and shrinking the search space. Consider the difference between mapping over a list and using a for-loop: with the loop, you could mess up the indexing, have an off-by-one error or even be doing something completely different like iterating over multiple lists at once or just repeating something n times. With a map, there's only one possible thing you can be doing: transforming a list. Much simpler! It leaves you with fewer places to make a mistake and code that's easier to read at a glance, since you immediately know the "shape" of the code when you see map. In fact, that's probably my favorite test for simplicity: given that I'm familiar with the relevant abstractions and idioms, how easy is the code to read at a glance? Code is read more often than it's written, but it's skimmed even more often than it's read. That makes the ability to quickly get the gist of an expression—without having to understand all the details—incredibly useful. Insight Another thing that elegant code does is give you a new insight on its domain. Sometimes, this is a surprising connection between two things that seemed disparate. Sometimes it's a new way of thinking about the problem. Sometimes its a neat idiom that captures a pattern that is normally awkward. Almost always, it's an idea that you can apply to other code or a common pattern you've already seen elsewhere. Beyond the immediately practical reasons, mostly illustrated in the "simplicity" section, this is why I'm so drawn to elegant code:  it's the best way to learn new things. And these things, thanks to their simplicity and generality, tend to be pretty deep. Not just pointless details. Elegant code also displays the essence of the problem its solving. It's a clear reflection of the deeper structure underlying either the solution or the problem space, not just something that happened to work. If your problem has some sort of symmetry, for example, elegant code will somehow show or take advantage of it. This is why that QuickSort example—which, unfortunately, has some problems of its own—gets trotted out so often. It does a marvellous job of reflecting the structure, and especially the symmetry, of QuickSort which the imperative version largely obscures in implementation detail. The key line quicksort lesser ++ [p] ++ quicksort greater reflects the shape of the resulting list. Composability The final characteristic of elegant code, especially elegant functional code, is composability and modularity. It does a great job of finding the natural stress lines in a problem and breaking it into multiple pieces. In some ways, this is just the same point all over: elegant code gets at the structure of what it's doing. Really elegant code combines this with giving you a new insight and letting you split a problem into two parts that you thought inseparable. This is where laziness really shines, coincidentally. A great such example is splitting certain algorithms into two phases: constructing a large data structure and then collapsing it. Just think of heapsort: build a heap then read elements out of it. That particular algorithm is elegant on its own, and is pretty easy to implement directly in two parts. For many other algorithms, the only way to separate them and maintain the same asymptotic bounds is to construct and fold the data structure lazily. Conal Elliott has a great talk about this which is well worth a look. It includes some specific examples of splitting up algorithms that seem inseparable into a fold and an unfold—most of which only work lazily. I think modularity is one of the best ways to avoid bugs and, to illustrate, I'm just going to reuse the same pictures. The first represents code that's less modular; the second represents code that's more modular. You can see why I'd find the second one more elegant! Imagine these graphs to be parts of your code with actual, or potential, interconnections between them. If all your code is in one big ball, then every part could potentially depend on every other part; if you manage to split it into two modules with clear module boundaries, the total number of possible interconnections goes way down. Not very modular, pretty complex—not very elegant. Simpler and more elegant. An Example But that was all pretty abstract. So let me give you an example that captures all of these ideas and neatly illustrates elegance. Lets say we have a bunch of records containing book metadata: data Book = { author, title :: String , date :: Date {- ... -} } We want to sort our book collection, first by author, then by title, then by date. Here's the really elegant way to do it: sortBy (comparing author <> comparing title <> comparing date) We can use comparing to turn each field into a comparison function of type Book -> Book -> Ordering and then use the monoid operator <> to combine these comparison functions. It does exactly what you expect it to—but if you're not familiar with monoids and the Ordering type, you might not know why it does what you expect. On the other hand, there is the really explicit version which replaces each <> with pattern-matching on EQ, LT and GT. To somebody who's not familiar with the relevant abstractions, this might be easier to read—but it's also more complex and noisy. Less elegant. This example is simple because it neatly abstracts over all the plumbing needed to combine the comparison functions. It's very easy to tell, at a glance, exactly which fields we're sorting by and with what priorities. It's insightful because it takes advantage of the natural way to combine Ordering values—the way they form a monoid. Moreover, going from the Ordering monoid to the Book -> Book -> Ordering monoid is actually also free—if we know how to combine any type o, we know how to combine functions a -> o. So the abstraction that hid the plumbing? We got most of that for free, from libraries that are not specific to Ordering at all! Finally, this version is definitely more modular and composable than the alternatives. It's very easy to mix and match different comparison functions with this pattern. We can trivially extract parts of them to be their own functions. It's very easy to refactor. All good things. Hopefully that's a nice illustration of what people mean by elegant and why it comes up often in languages like Haskell. 