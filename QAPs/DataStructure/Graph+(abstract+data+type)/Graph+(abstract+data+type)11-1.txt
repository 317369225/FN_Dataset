What are the core ideas behind TensorFlow?
TensorFlow is specifically a C++ library that was created to support Deep Learning Neural Networks.   It borrows most of its ideas from the Python based Theano library.   Specifically it supports the definition and execution of a Computational Graph and it supports Symbolic Differentiation for calculating gradients in the backpropagation learning phase of a Neural Network.  Incidentally, Deep Learning is a class of Neural Networks, which is just one of many other Machine Learning algorithms.     Google adds to Theano's ideas by including capabilities that support distributed computation.  This includes the ability to perform computation on Partial Subgraphs and the presence of coordination mechanisms such as queues and synchronization.   So one could arguably make the simplification that TensorFlow is a distributed implementation of Theano.   However, it is important to note that Google did not release  "distributed" capabilities in its original Open Source release.   TensorFlow however can not claim to be the first Distributed Deep Learning framework.  There are plenty of other alternatives such as H2O, DL4J,  Spark MLLib, Petuum, Singa, etc.    In fact, Google had previously a distributed framework called DistBelief that they claim has been replaced by TensorFlow.  The claimed benefit of TensorFlow is that it reduced the complexity of deploying Deep Learning solutions as compared to its previous system.  This in essence is the core idea of TensorFlow, that is, it reduces the complexity of formulating new DL models in a distributed context. 