What are some actual projects data scientists have worked on?Given how much interest there is in data science, I was thinking it would be great if some data scientists could walk through a project they've worked on, what tools and analytical techniques they used, and most importantly, their thought processes behind each step of their analyses and execution, as well as what mistakes they made.
While I've done a lot of little projects for Uber (company), some of which I make public over on their Uber Blog (#uberdata), for the purposes of this question I think my brainSCANr project probably fits best. Note that Uber, brainSCANr, and my actual experimental neuroscience research really inform one another in amazingly wonderful ways. brainSCANr website Automated cognome construction and semi-automated hypothesis generation paper The Problem The opening lines of the paper: The scientific method begins with a hypothesis about our reality that can be tested via experimental observation. Hypothesis formation is iterative, building off prior scientific knowledge. Before one can form a hypothesis, one must have a thorough understanding of previous research to ensure that the path of inquiry is founded upon a stable base of established facts. But how can a researcher perform a thorough, unbiased literature review when over one million scientific articles are published annually? The rate of scientific discovery has outpaced our ability to integrate knowledge in an unbiased, principled fashion. One solution may be via automated information aggregation. In this manuscript we show that, by calculating associations between concepts in the peer-reviewed literature, we can algorithmically synthesize scientific information and use that knowledge to help formulate plausible low-level hypotheses. Inception Stage In May 2010 I was invited to speak at Berkeley's Cognitive Science Student Association (CSSA) Conference. At that conference I sat on a Q&A panel with a hell of a group of scientists, including my friend and colleague George Lakoff and the (then) Chair of Stanford's Psychology department, James McClelland, who helped pioneer Parallel Distributed Processing (PDP). On that panel I A'd many Qs, one of which was a fairly high-level question about the challenge of integrating the wealth of data hidden in the neuroscientific literature. It was a variant on the classic line that neuroscience is "data rich but theory poor". This is a problem I've been struggling with for a long time and I'd had a few ideas. In my response I said that one of our problems as a field was that we had so many different people with different backgrounds speaking different jargons who aren't effectively communicating. I followed with an off-hand comment that "The Literature" was actually pretty smart when taken as a system, but that us individual puny brains just weren't bright enough to integrate it all. I went on to claim that, if there was some way to automatically integrate information from the peer-review literature, we could probably glean a lot of new insights. James McClelland really seemed to disagree with me, but the idea kept kicking around my brain for a while. Creation One night, several months later (while watching Battlestar Galactica (2003–2009 series) with my wife Jessica Bolger Voytek), I turned to her and explained my idea. She asked me how I was planning on coding it up and, after I explained it, she challenged me by saying that she could definitely code that faster than I could. Fast-forward a couple of hours to around 2am and she had her results. I did not. Bah. The idea I discussed with her was very simple (and probably simplistic) and was based on the assumption that the more frequently two neuroscientific terms appear in the title or abstracts of peer-reviewed papers together, the more likely those terms are to be associated with one another. For example, if "learning" and all of its synonyms appears in 100 papers with "memory" and all of its synonyms while both of those terms appear in a total of 1000 papers without one another, then the probability of those two terms being associated is 100/1000, or 0.1. We calculated such probabilities for every pair of terms using a dictionary that we manually curated. It contained 124 brain regions, 291 cognitive functions, and 47 diseases. Brain region names and associated synonyms were selected from the NeuroNames database, cognitive functions were obtained from Russ Poldrack's Cognitive Atlas, and disease names are from the NIH. The initial population of the dictionary was meant to represent the broadest, most plausibly common search terms that were also relatively unique (and thus likely not to lead to spurious connections). Note that this process requires some expert knowledge, but it could also be relatively easily automated by rank-ordering n-grams (by frequency) from all those papers and including any that appear more than x times, for example. Tools Used We counted the number of published papers containing pairs of terms using the National Library of Medicine's ESearch utility (their API) and the count return type. Here's the example for "prefrontal cortex" and "striatum": Conjunction: Page on Nih Disjunctions: Page on Nih & Page on Nih This process was repeated for every pair of terms using a quick Python script to populate an array with the resulting association weights. Here's what the method looks like: We note in our manuscript that this method is rife with caveats, but this wasn't meant to be an end-point, but rather a proof-of-concept beginning. In the end we get a full matrix of 175528 term pairs. Once we got this database we (okay, my wife) hacked together the brainSCANr website to allow people to play around with terms and their relationships. We wanted to create a tool for researchers and the public alike to use to help simplify the complexities of neuroscience. You enter a search term, it shows the relationships and gives you links to the relevant peer-reviewed papers. As an example, here's Alzheimer's: The  website was created using Google App Engine. Graph connectivity plotting was performed using the JavaScript InfoVis Toolkit. My wife and I threw the first version together (with help from my Uber (company) buddy Curtis Chambers) over about a week. We actually did this during our New Years vacation where we used it as a way to hide from our friends the fact that my wife was pregnant with our first child (we'd just found out the day before we were supposed to leave). Data analyses were ultimately done in MATLAB and Python (programming language). Clustering of weights was performed using k-means clustering and hierarchical clustering. Results I like to joke that this took us a week and about $11.75 to put together compared to the $8.5M, 3-year Human Connectome Project. We first wanted to see if the resulting clusters made any sense. I taught neuroanatomy at Berkeley for 3 semesters so you'll have to trust me somewhat when I say that the relationships between brain regions that we algorithmically extract purely from textual relationships in the peer-review literature very closely map onto the known connections between these brain regions. Honestly I was so ridiculously excited when I first saw the results. When we performed some simple clustering on these terms it was amazing what was associated. None of the results were terribly surprising, but it's really cool that things like the visual system just fall out of the literature: LGN, V1, pulvinar, superior colliculus, and visual extrastriate, for example, all get placed into one cluster together. But still, so what? I spent a long time struggling to come up with something we could do with these data. In the end I settled on an algorithm to try and find missing relationships. Imagine you've got two really close friends. Chances are--statistically speaking--that those two people know one another. In fact, it would be surprising if they didn't. Furthermore, if they did end up meeting they would probably get along pretty well because you're such good friends with each of them. That's the analogy for the algorithm I use to discover possible relationships between ideas that should exist in neuroscience, but don't. A friend-of-a-friend should be a friend. Basically Facebook or LinkedIn (product)'s recommendation algorithm, super simplified. Here's that analogy, visualized: I call this "semi-automated hypothesis generation". In this example you can see in panel D that the term "serotonin" appears in 4782 papers with the brain region "striatum". Serotonin also appears in 2943 papers with "migraine". It turns out, we know a lot about the neurochemistry, physiology, and distribution of serotonin in the brain. That's on the neuroscience side. Apparently--and I did not know this prior to running this algorithm--there is a very rich medical literature on the serotonin hypothesis for migraines. Given these two pieces of information it is statistically surprising that there are only 16 publications that discuss the striatum--a brain region that strongly expresses serotonin--and migraines, which is strongly associated with serotonin. Maybe we're missing a connection here. Maybe medical doctors who study migraines aren't talking with the neuroscientists. This isn't necessarily a correct association, just one that may be worth exploring. And now we have an algorithmic way of doing something that many researchers do anyway. For example, when I have what I think is a new idea the first thing I do is turn to PubMed and start searching to see if it really is novel. But what if I could occasionally skip that step where I need to have the idea in the first place? I'm not saying that creativity and organic idea generation doesn't have a place, but that we can now augment that process. We took a few steps to try and verify the validity of the data. For example, we looked at how the associations between neurotransmitter terms and brain regions in our database related to actual gene expression values for the genes associated with those neurotransmitters. To do this we integrated our results with the Allen Brain Atlas (who graciously makes their data freely available online!) We also used the ABA to find brain regions that strongly express a neurotransmitter-related gene but are statistically understudied. This is another way to find gaps in the literature. In the example above, you can see in panel C that there are an over-abundance of papers that look at serotonin and the nucleus accumbens (nAcc), but the region that most strongly expresses serotonin-related genes--the zona incerta--is woefully understudied (probably because it's such a difficult region to examine). We also observed that our presumed relationships significantly correlate with real gene expression values. Although the association was weak, it supports our argument that textual relationships reflect real-world knowledge to at least some degree. Future Directions This project was put on hold for two years while my wife and I adjusted to parenthood and I focused on my work with Uber (company) and finishing up my post-doctoral research. But now that I'll soon be starting my own lab at University of California, San Diego, my wife and I are hoping to put in a grant to try and take this stuff to the next level. I'll close out with the final paragraph of the paper itself: We can leverage the power of millions of publications to bootstrap informative relationships and uncover scientific "metaknowledge"... By mining these relationships, we show that it is possible to add a layer of intelligent automation to the scientific method as has been demonstrated for the data modeling stage (Schmidt and Lipson, 2009). By implementing a connection-finding algorithm, we believe we can speed the process of discovering new relationships. So while the future of scientific research does not rely on these tools, we believe it will be greatly aided by them. This is a small step toward a future of semi-automated, algorithmic scientific research. See also: What discoveries or insights have come out of brainSCANr? How are brainSCANr and the Allen Brain Atlas similar; how are they different? What data analysis and visualization technologies underlie brainSCANr? 