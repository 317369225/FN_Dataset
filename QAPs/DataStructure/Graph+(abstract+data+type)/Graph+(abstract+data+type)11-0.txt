What are the core ideas behind TensorFlow?
A few key ideas are : Tensor as the unified data type. Each computing vertex takes in zero or more tensor as input, and output zero or more tensor. Computation as a graph, thus the name "flow". Many computing vertex form a large computation graph, the system is responsible for scheduling and distribution. Using graph to model computation (allowing cycles and loops), and "variable" instead of dedicated parameter servers in the DistBelief, it can model beyond FNN, easily model RNN and other more complex network architecture. Common abstraction of computation (ops, attributes, etc), thus providing cross language functionality: user can write computation in either python or C++, or other language down the road. It also allow extensibility through those abstractions, user could define its own operations. Kernel abstraction to provide cross platform functionality: GPU, CPU, etc. Automatic gradient computation. Support distributed computation across machines, this is the killer feature of TensorFlow over other platforms like Theano, Torch, etc. Unfortunately this part is not open sourced. It supports both data parallel and model parallel, like DistBelief. 