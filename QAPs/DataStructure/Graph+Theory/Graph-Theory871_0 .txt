How is this Big-theta graph for a linear search algorithm possible?Trying to follow along with this tutorial[1] - how is it possible for the running time to be higher when n is smallest then lower momentarily as n grows? Also, what are some practical examples of what constants k1 and k2 could actually be in this example? Footnotes [1] Khan AcademyGood Afternoon!  It follows from the definition of asymptotic order (Big Oh, and Big Omega). It has to be eventually non-decreasing. Eventually non-decreasing means that there can be dips, but there exists a value (these would be values of n at the dashed line or after it) where it is satisfied for any n and no more dips occur like that. The constants don't have to be anything practical, it just has to satisfy the definition. You just need a positive real constant and the value for which the inequality holds. Note that for Big-Theta, you need both Big-Oh and Big-Omega to be satisfied, you don't need to show them at the same time like in the diagram. They can have different constants. As for the dip, it could be that the growth function has for base cases (small values of n) that it does more work (for some odd reason). It suggests two things are done differently in the algorithm's running time. Typically growth functions grow with respect to the input size (in this case, n). Hope this helps! 253 Views  View Upvotes  Answer requested by Chris Purrone