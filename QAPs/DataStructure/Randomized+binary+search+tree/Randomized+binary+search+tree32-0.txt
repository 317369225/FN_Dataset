To what extent does functional programming require different data structures and algorithms?
Most purely functional data structures are distinct from those that you'd use in imperative programming but many of them only differ in small ways. The underlying principles (asymptotics, cache behavior... etc) are the same but often have to be handled in different ways. Which data structures are widely used—and which aren't—also differs substantially. Most data structures are different because they're immutable and usually persistent. That's less common outside of dedicated functional programming, although persistent data structures are picking up steam for certain applications even in otherwise staunchly imperative languages. If you were going to count that, you'd conclude that functional programming has its own universe of data structures that almost entirely disjoint from normal ones. But that's a bit misleading: many immutable and persistent data structures are close variants of "normal" data structures. Often, all it takes is to rewrite well-known algorithms in a functional style to go from an imperative data structure to a persistent one—enough to make it a new data structure, perhaps, but otherwise not a big difference. The analysis of the structure might be very different, but the structure itself and the algorithms for using it are completely recognizable. A practical example of this is the PATRICIA trie, used in Haskell's Data.IntMap. If you're familiar with the original PATRICIA trie, you can follow along with the design of the functional version and see that the algorithms are still almost exactly the same but with mutation replaced by returning a new copy. This is a systematic change and yet it gets us all the properties we care about, including persistence—since we only return a new copy when something changes, we only need to copy part of the trie to update it. You can convert a lot of the data structures you're used to like this, including most kinds of trees. However, some imperative data structures don't translate well. Hash maps are a lot less common in functional programming. Hash maps simply don't lend themselves well to being immutable or persistent: a lot of their efficiency comes from being able to make changes in place with fast random access read and write access to the underlying memory. Instead, they are replaced with alternatives like balanced binary search trees and, more efficiently, various tries which can be made persistent. In Haskell specifically hashing isn't commonly used and many custom typed don't bother to derive a hashing function at all, but this doesn't have to be the case: functional languages on the JVM like Scala and Clojure do rely on hashing for efficient structures like Hash Array-Mapped Tries (HAMTs). Another place that Haskell specifically differs is laziness. Many functional data structures can be implemented more simply in a lazy manner while still maintaining the asymptotic performance they need. Laziness also opens data structures to new uses, a sort of hybrid between storing data and representing computations. For example, we could make a quadtree to store graphics data that can extend downwards forever, giving us resolution independence. Actually using it, of course, relies on laziness to ensure that we only evaluate enough of our quadtree to render at the resolution we care about. Lazy algorithms and data structures are really a specialization unto themselves, a bit distinct from functional data structures in general. So: many data structures used are just immutable versions of normal structures. Some of the structures common in imperative programming aren't common in functional programming and vice-versa. Some structures rely on laziness which would make them awkward in any non-functional language. If you wanted to learn about functional data structures you'd be able to reuse the high-level concepts you already know, you'd recognize a lot of existing data structures but you'd also have to learn some new ones and even pick up some new high-level concepts, especially for analyzing complexity and working with laziness. 