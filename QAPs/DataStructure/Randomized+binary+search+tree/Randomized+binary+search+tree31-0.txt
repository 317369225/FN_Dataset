Randomness (statistics):Â Why does picking data samples or pivots randomly reduce the odds of poor performance or worst case, in some cases?
Because worst-case performance happens when your data is perverse; that is, when your data clash with your choosen algorithm in the worst possible way. This can be random bad luck, or it can be deliberate; for example someone might be feeding your algorithm data in a sequence that's known to make it slow as a form of denial of service, or it might just happen. Let's say you're building a binary search-tree to make it easy to rapidly find a piece of data again. You're expecting O(log2(n)) performance, in other words on the order of 20 steps for finding a given piece of data in a set of a million elements. But then, as it turns out, the data you feed into the search-tree turn out to be sorted sequentially. There's many ways this could happen; sorting data isn't a particularily rare thing to do. So the three ends up degenerating into a linked list and you get O(n) performance instead of O(log2(n)) performance. This sucks, and can easily break your application. half a million steps to find a average element is a LOT more than 20. Adding the data in a random sequence avoids this. If the data is already sorted, it doesn't matter. And if your sequence is genuinely random (or close enough to not be guessable by an attacker) then the statistical odds that your tree will be extremely unbalanced are exceedingly small. (and will stay consistently small even if you start getting already-sorted input or maliciously-crafted input) 