Why is Computer Science important?
This is very difficult to answer, as Computer Science differs between institutions. I've seen it range from a genuine hard science to simple coding and everything in between. There is absolutely zero consensus as to what this label entails. Universities sometimes offer a second course at the opposite end of the spectrum they place CS under. This, too, will have a less than informative title. What I will do here is detail the sorts of things that such courses include and why they are important. First off, linear algebra. You really can't do intensive mathematics without this. A lot of optimization also requires this. It's difficult to do much systems analysis without it. Next, formal logic. This often starts with abstract data types, goes through grammars, abstract state machines and ends up with lambda calculus or pi calculus. If Z Notation, Higher Order Logic or Petri Nets are covered, they'll be in here somewhere. A1-class operating systems need formal proofs, vehicular software (such as self-driving cars and aircraft autopilots) don't but I wouldn't trust such software further than I could spit the printout if formal methods haven't been used in some form. Abstract data types covered in formal logic are your basic ones - stacks, queues, trees and graphs. Most people will have covered these at previous levels, but here they're defined with greater rigour and greater abstraction than is typical in high school/sixth form. They also become more of a means to teach things like lambda calculus or first order logic than an end in themselves. Microprocessors sometimes get covered. It's not unusual to have to write a program to simulate one. The boundary between hardware and software is rather more flexible than the "real world" presumes and the ability to think across the lines is valuable. I've also found it a hard sell in job interviews, with many companies refusing to believe that good programmers are agnostic to such details. Writing a BIOS or UEFI in Forth or assembly, designing an FPGA in System C, designing an OS driver in C or C++, or writing a web app in Cold Fusion - it's just inputs, outputs, logic and a specification to describe the lot. Good programs compartmentalize and abstract, logic should never care where data comes from or goes, logic should never even contain any I/O if it's to be maintainable. This sort of problem agnosticism and environment agnosticism is where you separate scientists from implementers. Network Theory. This doesn't have to be a network like Ethernet, it can be how data travels around a parallel program on a single computer. It's really just a special purpose aspect of graph theory, showing how these kinds of abstraction apply as much to software and hardware architectures themselves as they do to the problems the software and hardware solve, and how understanding that abstraction can tell you a lot about robustness, performance and other characteristics. Parallelism/Communication. This can range from the trivial, such as reliable communication between a few threads, to the truly complex, such as unreliable communication over complex networks to thousands of nodes. It depends on whether the lecturer prefers brains raw or deep fried. These classes of problem are truly complicated and involve all kinds of non-deterministic situations which a computer scientist will be expected to identify, classify and mitigate. Information Theory. Usually only a fragment is taught, this is a big subject. Information Theory covers the capacity of a communications channel, error correction codes, encryption, data compression, the efficiency of data structures, conservation laws, all kinds of good stuff. Weak AI. Expert systems, neural nets, heuristics, genetic algorithms, stuff like that. Many classes of problem don't lend themselves to a concrete solution obtained by direct algorithm. Rather, they lend themselves to a particular way of reaching an acceptable solution obtained through trial and error. Frequently, it will not be possible to prove there is no better solution than some give one in finite time, so you need to know what "good enough" means in the context of a deadline of some sort. Everything else you're likely to encounter (specific languages such as Occam or Ada, specific problems such as computer vision or graphics, specific contexts such as the web or low-level hardware operation) are largely meaningless. They're good practice for breaking you out of insular thinking, although as noted above that doesn't seem to do much for you in practice, but they're all just about looking up the specifics that apply to the case and using those to fill in the details in the abstraction. A computer scientist, software architect and software engineer should be different ways of applying identical skills across identical abstractions across any layer from VHDL for an ASIC to some fifth generation language running over an extranet of mobile heterogeneous nodes. At no point should the environment matter beyond the abstract qualities covered by the topics I listed. In practice, courses will probably only cover a subset of topics. That limits the utility of what you learn, but it does not alter the field itself. The field itself covers essentially everything. (Insofar as the subject is correct, it cannot be complete.) 