What is the point of using dynamic programming when the time complexity in most of the codes is O(n^2) (Which isn't so good i.e. we use double for loops even in DP)?
Hmmm... this is gonna be long. Let us first try to understand what DP is. Dynamic Programming is a mathematical technique to solve problems. In layman terms, it means Recursion (or iteration, for that matter, depends whether you follow bottom-up or top-down approach) along with Memoization. Let us now understand Memoization. Yeah, sounds like a fancy word, but actually in crude terms it means to memo-down stuff i.e. store what you have calculated once and store it in a memo (dictionary) for further reuse to avoid repeated calculations, hence, decreasing the effective computation time. Now, since we are clear with basic terms, let us take a few examples to advocate the cause of DP. Problem: Write a program to give nth Fibonacci  number, given n>0. Someone, who doesn't think about Memoization, immediately thinks about a simple recursive algo. func fib (int n ) {   if (n==1 || n==2)      return 1;   else       return (fib(n-1) + fib(n-2));} Seems okay. Right? Try running it on a normal home PC with a ~3 GHz processor and 4-8GB RAM. Take n as a 50 or 60 odd number. You, can go and make a cup of coffee by the time this program gives the answer. Why, you ask ? What is wrong with this program? It is a simple program that one executes first while studying recursion for the first time. The answer, lies in the complexity. This algo takes exponential time. O(2^n). When you execute this for a higher n, the senses of a computer machine goes for a toss because it keeps calculating fib(lower Ns) repeatedly. If you draw the recursion tree for the function calls for this algo, you'll see how many repeated calls are made. Now, use a bottom-up approach for this program. // Do some precalulation. i.e. Memoize first.int [] memo = new int[1000];Arrays.fill(memo,0);memo[0]=memo[1]=1;// Do precalulation(i.e. Memoize) this takes O(n) time.precalc() {   for(int i=2;i<1000;i++){      memo[i]=memo[i-1]+memo[i-2];   }}// Now this function is a cake walk taking O(1) time.func fib(int n) {    return memo[n-1];} As you can see, because of building a dictionary, the complexity becomes linear i.e. O(n) + O(1) = O(n). Run this for a n in the higher ranges, you'll get the result instantly. Ohh, I get it, you are not impressed. Not a perfect DP example. Consider this now: Problem: Given some coins denominations in the array denom[] ={1,3,5}; Find the minimum no of coins required to make a sum of S =11. You immediately jump, reading this question and say, this can be easily solved using Greedy algo. static int minCoins(int [] arr , int S) {   Arrays.sort(arr);   int count=0;   int N=S;   for(int j=a.size()-1;j>=0;j--){      if (N>0){        count+= N/arr[j];        N= N%arr[j];      }   }   if (N==0)     return count;   else      return -1;	} Seems good. But, now try this for denom[] ={1,10,25} and sum S=40. Ohh, sadly the greedy algo bombs. Such a question can be solved correctly for all inputs using DP. static int countCoins(int []coins, int S){   int [] result=new int[S+1];   Arrays.fill(result,1000);   result[0]=0;   for(int i=1;i<=S;i++){      for(int j=0;j<coins.length;j++){         if(coins[j]<= i && (result[i-coins[j]]+1 < result[i]))            result[i]=result[i-coins[j]]+1;      }   }   return result[S];	} Coming back to the "point" of complexity of O(n^2). If a problem talks about calculating the length of Longest Common Subsequence. Given two sequences, find the length of longest common subsequence present in both of them. A subsequence is a sequence that appears in the same relative order, but not necessarily contiguous. For example, “abc”, “abg”, “bdf”, “aeg”, ‘”acefg”, .. etc are subsequences of “abcdefg”. So a string of length n has 2^n different possible subsequences. /* A Naive recursive implementation of LCS problem */#include<stdio.h>#include<stdlib.h> int max(int a, int b); /* Returns length of LCS for X[0..m-1], Y[0..n-1] */int lcs( char *X, char *Y, int m, int n ){   if (m == 0 || n == 0)     return 0;   if (X[m-1] == Y[n-1])     return 1 + lcs(X, Y, m-1, n-1);   else     return max(lcs(X, Y, m, n-1), lcs(X, Y, m-1, n));} Time complexity of the above naive recursive approach is O(2^n) in worst case and worst case happens when all characters of X and Y mismatch i.e., length of LCS is 0. Considering the above implementation, following is a partial recursion tree for input strings “AXYT” and “AYZX”                          lcs("AXYT", "AYZX")                        /                 \          lcs("AXY", "AYZX")            lcs("AXYT", "AYZ")          /            \                  /               \ lcs("AX", "AYZX") lcs("AXY", "AYZ")   lcs("AXY", "AYZ") lcs("AXYT", "AY") You get the idea of the recursion tree. Since, we can observe from the recursion tree that many subproblems are solved repeatedly, we can solve them once and store the result somewhere. So that if the next time such a sub-problem occurs, instead of calculating it again, we can do a look-up from our memo (dictionary). /* DP solution of LCS problem */	static int longestCommonSubSeq(char[] s1, char[] s2){   int N = s1.length;   int [][]dp = new int [N+1][N+1];   for(int i=0;i<N;i++){      dp[0][i]=dp[i][0]=0;   }   for(int i=1;i<=N;i++){      for(int j=1;j<=N;j++){         dp[i][j]=max(dp[i-1][j],dp[i][j-1]);         if(s1[i-1]==s2[j-1]){            dp[i][j]=max(dp[i-1][j-1]+1,dp[i][j]);         }      }   }		return dp[N][N];	} This runs in O(n^2) time. Now, it doesn't take a genius to figure out that 2^n is far far poorer that O(n^2). 