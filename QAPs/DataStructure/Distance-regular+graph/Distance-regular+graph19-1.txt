What is the importance of this algorithm?
I have an example of the problem you described up and running at http://esolangs.rutteric.com/wor... The dictionary it uses is a little bit iffy in that it contains a number of non-words, but the idea is there in principle. It uses the same search algorithm you used, which is known to computer scientists as A*. It also compares words using a measure called Levenshtein distance. This serves as the search heuristic for A*. This is what you were talking about: A large number of paths does not get you closer to the output. So one way is that as soon as a level is completed, I match all words with the output word and keep only the most relevant. This can reduce the exponential time down to a polynomial.The pruning method you describe would be great for a game-playing AI, but it risks finding non-optimal paths in this particular problem. In general, you cannot just reject a word just because it doesn't get closer to the target word according to your heuristic, because your heuristic is only a good underestimate, and doesn't necessarily reflect the nature of the underlying graph. Yes, others have mentioned graphs in answers here, but allow me to explain for a moment before we get back to the problem of correctly implementing A*. A graph is a collection of branching points called nodes or vertices. In our case, the nodes are the words in the English language. These nodes are connected by edges or arcs. In our case, there is an edge between two (word) nodes if they differ by exactly one letter. In general, edges can have a length (or weight) associated with them, such as in the graph corresponding to the graph of roads traversing the world's continents, but in our case, we have an unweighted graph which means the length of every edge is 1. This is also true in the case of the LinkedIn user graph. Now, in a basic graph search, like breadth-first search, the method for finding a shortest path is exactly as you describe: 2. Take the input word, and find all possible words that replace exactly one of the letters. For example, some transformations of cat are [car, cot, dat, mat....]. 3. If the output word was not found in that list of transformed words, then apply step 2 to each word in the list (modifying it further).  Continue in this fashion until the target word is found.In computer science terms, you take each (word) node and expand it, which means adding all of its neighbors (words that differ by one letter) to the FIFO queue (which serves as a list of words we have yet to expand, with the guarantee that the nodes come off the list in the same order we put them on, e.g. oldest/closest to the source first) unless they are already in the queue or in the list of (word) nodes we have already processed. After a node has been expanded, we add it to this latter list. This takes care of your path convergence problem: 2. Lots of paths converge together, so a given level is likely to have many duplicates, I removed them.We also keep track of which node was the parent of each node, i.e. the node we were expanding when we first saw each node. This lets us figure out later exactly which path we followed to get from our source node (starting word) to our target node (ending word). As soon as the target word is seen in our search, we halt and return the path we took to get to it. If we run out of nodes to check before seeing the target, then the source and target are in separate connected components, and there is no path between them. This may happen, for instance, if we are trying to get to or from an isolated vertex: 1. Some words have no transformations. For example, if you replace any single letter in words such as system, or dragon, you get nothing. This method is guaranteed to find the shortest path in an unweighted graph and, since it processes every edge in the graph at most twice, it is guaranteed to do so in time polynomial in the size of the graph. However, it is known that in most real-world circumstances this algorithm will take a lot longer than necessary, which is why we move to incorporating the heuristic. Here's all we need to change: we replace the FIFO queue with a priority queue. A priority queue is a list q and a function f so that when you push a sequence of items to q and then later pop an item x from q, you are guaranteed to extract an x such that f(x) is minimized. In our case, we are going to use the queue to select the item that we guess is probably on the shortest path between our source node and target node. We already know x's distance from the source node, which we will call g(x): it is simply 1 more than the minimum of all of its neighbors distances. Every time we see the node while we're searching (up until we actually expand it), we check if its g value is more than one more than the g of the node we are currently expanding, and decrease it if so. (In the case of a weighted graph, we would use the length of the edge plus the current node's g instead of one.) Thus, by the time we expand x, we are guaranteed that g(x) is exactly the shortest distance to the source node. However, we have not yet seen the target node, nor a large portion of the graph near it, so we can't say how far the node is from the target. This is to be expected,  since if we already had this information, we'd already know the shortest path. This is where the heuristic comes in. It's just a guess as to the node's distance from the target. In real-world path-finding problems, the straight-line distance between the node and the destination is a reasonable guess. In our problem, the edit distance (such as the Hamming or Levenshtein distance) is a reasonable guess. What qualifies a guess as reasonable? We call a heuristic admissible if it is guaranteed to be no larger than the actual distance between the node and the target. Why can't we overestimate? Remember that the priority queue guarantees us the smallest priority node. If we grossly overestimate the distance of a node that is on the shortest path, its priority will be so large that we'll never expand it. We'll never look at it at all, and we won't find the shortest path. Thus, breadth-first search (and its weighted variant, Dijkstra's Algorithm), which doesn't use a heuristic at all, is actually a special case of A* where the heuristic is set to zero everywhere. This is an admissible heuristic in an unweighted graph (or a graph with non-negative edge weights) because zero is always going to be no more than the actual distance to the target node. So, let's call the heuristic function h (in our case, the edit distance) and define our priority function f as f(x)=g(x)+h(x). So we replace our FIFO queue with a priority queue that always chooses the node with the minimum value of this function. We know that f(x) is guaranteed to be less than the length of the actual path from the source to the target via x, but the closer we can get it to the actual value (by clever choice of heuristic) the faster the algorithm. This is because the queue is more likely to be giving us back a node that is actually on the shortest path in each step. In fact, back in 1968, Peter Hart, Nils Nilsson and Bertram Raphael together proved that for any given choice of heuristic, A* is faster than any other possible algorithm for finding the correct shortest path. In particular, even in an unbounded search space (such as the one your problem would have if we couldn't quickly eliminate non-words, and we are allowed to traverse words of all lengths), as long as the heuristic gives a distance within a constant factor of the logarithm of the actual distance to the target, the algorithm will still traverse the exponential search space in polynomial time (which is pretty much what you were saying about the running time, although it remains to prove that the edit distance heuristic has this property). I was saying earlier that your pruning method would break, so now that we have the correct method in hand, let's see why. Consider the shortest path in this particular problem: ballots ballets ballers bailers bailees bailies ballies bollies boolies booties boothes soothes soother smother smotter smatter smalter smaller staller stallar stellar We can see that many of the steps here actually increase the Hamming distance to the target. For instance, boolies->booties increases the Hamming distance from 6 to 7. Yet this is a shortest path. Pruning "booties" from your search may force you to take a longer path to avoid it. That's why you keep nodes that seem "poor" in your list so that you can eventually check them. Could we speed this process up any further? Sure! We could do something called a bidirectional A* search which means we search from the source and target simultaneously and find a place where the searches meet each other in the middle. In the case of an exponentially branching search space, this will massively decrease the number of nodes expanded. (It could let you expand the square root of the number of nodes you would have otherwise.) It will also let us detect isolated vertices and small connected components a lot sooner. (For instance, if we are searching from a connected component with a million nodes, but the target is in a separate connected component with only five nodes, bidirectional A* will figure it out after expanding only five nodes, while regular A* has to expand a million.) Another way to get massive speedups is by using a heuristic that incorporates centralized nodes for which all paths have been found in advance ("landmarks") to give an estimate based on the triangle inequality. This is one of those space/time tradeoffs that you see all the time in computer science. You only need to expand a tiny fraction of the nodes you would otherwise have to, but you have to store an additional word of data per node for every landmark you use. The example I linked above does not use bidirectional search. It would not be possible due to another optimization I made. I pre-constructed a graph of all words in my list so that I could avoid having to "generate and test" potential words while searching, but I stored in a directed graph form, so that shorter words are connected to longer words that they can reach by inserting a letter, but the reverse is not true. This restriction means that bidirectional search is not possible. It also does not not use landmarks for similar reasons: you already have to download a 19MB graph to use it, and adding landmarks would drastically increase the data to be downloaded. Feel free to view the source on that page to see how A* (as well as a specially modified Levenshtein distance algorithm) is implemented. The relevant function is called "findPath". Some additional notes: A priority queue takes amortized logarithmic time (in the length of list) to push and pop an element, while a FIFO queue takes constant time. A good heuristic so drastically decreases the number of nodes expanded that this slight increase in the time it takes to process each node is warranted. (In general, a BFS queue will have many more nodes in it than an A* queue.) Finally, here is a legal path from maple to ample in 9 steps: maple maile baile bagle eagle engle angle anole amole ample Updated 21w ago • View Upvotes