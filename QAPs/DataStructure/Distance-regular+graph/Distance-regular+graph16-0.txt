What are some brain storming/puzzle questions asked in a data scientist/analyst interview?
There are a number of classic Machine Learning questions that every Data Science applicant should know how to answer.  Here are a couple that I like to use: How do you find the right "K" for K-means? I’ve found this is a great barometer question for how experienced a Data Scientist is because answers can differ wildly.  A common myth is that “there is no right answer.”  The reasoning for this is that as K increases the Sum of Squared Residuals (distance of points from the center of their clusters) or SSR will necessarily decrease.  The “Wikipedia” level answer takes this a step further: As K increases, SSR will necessarily decrease, but at some point the gain in SSR will not be as dramatic as one increments the K value.  This is referred to as the “elbow method” because in graphs one will see an “elbow shaped” kink in the curve: (wiki image) The problem is that there are a number of competing ways of analytically determining the optimal “elbow kink” in a curve like the one depicted, so as a result, there is not a universally received answer to how to "find the elbow.” More sophisticated answers will reference alternative metrics to SSR.  SSR will measure cluster density (how “tightly clustered” are data points) but does not give any weight to the separation between clusters.  Candidates with experience working with clusters will often mention the Silhouette metric, which tracks both cluster density and separation in giving the final score (rule of thumb is to target Silhouette scores >.2).  There are actually hundreds of potential clustering quality metrics out there in addition to information criteria measures (see diagram) sophisticated candidates will know of their existence, but will reference their existence, but are not expected to be familiar with them. Advanced candidates will either have used or at least studied the “gap statistics” method (Page on stanford.edu), which is my personal favorite.  The intuition behind this method lies in comparing how the K-clustering performs on the given data set with how it performs on a “random” set of data points distributed evenly across a region of the feature space. What is the difference between L1 and L2 regularization? This is a classic question that experienced Data Scientists should be well prepared to answer: The “by definition”  answer is that L1 regularization (aka lasso regularization) penalizes the use of parameters by adding a term proportional to the sum of the magnitudes of each parameter to the cost function during optimization, whereas L2 regularization (aka ridge regression) will add a term proportional to the sum of squares. This answer shows textbook understanding of the difference but is just a start.  When asked the follow up question “what is the effect of using one over the other?”, sophisticated candidates will explain that L1 regularization will drive unimportant parameters to exactly 0 during the optimization process, whereas while L2 regularization will drive unimportant parameters values “close to 0” they will not vanish to 0 precisely.  The effect of L1 regularization is sometimes referred to as having a “parameter sparsity” effect. The sophisticated Data Scientist will be able to answer the final followup question: “Why does L1 regularization cause parameter sparsity where L2 regularization does not?”  My favorite answer to this question will reference the “geometry” of L1 and L2 spaces.  This is intuitively illustrated by referencing the “unit ball” (i.e. set of points equidistant to the origin) in L1 and L2 spaces respectively: L1 unit balls are “diamond shaped” whereas L2 unit balls are “rotationally symmetric” (i.e. spinning the axes would not “change the shape” of the L2 ball.  Because of this difference L1 regularization treats “walking along the axes" differently so if (for example) we want to get to the line depicted in the image, it might make more sense to walk along the x1 axis and not move at all along the x2 axis to get to the line when our steps are penalized in an “L1 way” whereas when our steps are penalized in an “L2 way,” we need not worry about keeping strictly to specific axes.  This intuition translates to trying to find the “lowest point on an error surface” so optimization done with L1 regularization tends to avoid “walking” in directions (viz. using parameters) that don’t give a good return in finding the lowest point on the error surface. Updated 61w ago • View Upvotes