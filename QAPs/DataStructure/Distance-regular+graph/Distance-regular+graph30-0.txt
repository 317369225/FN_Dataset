How is isotonic regression used in practice for calibration in machine learning?
In many cases we have a classifier that predicts class labels but does not inherently provide an accompanying probability estimate of a test point belonging to a class - P(c|x). Sometimes there is a score provided, say S(x) in the range [0, 1] - but it does not estimate this probability well. A couple of examples of such classifiers are Naive Bayes (NB) and Support Vector Machines (SVM). NB does provide a score, but it is a bad estimate of P(c|x). SVMs don't inherently provide a score, but the distance of a point from the separating hyperplane may be used as a proxy (greater the distance, higher the score) - and this too, happens to be a bad estimator. How do we decide whether  S(x) is a good estimator? If S(x) = 0.3 for a bunch of points, and if S(x) indeed represented P(c|x) well, then only 30% of these points would belong to c. Since you already know the labels of these points, you can calculate the empirical probability of this bunch belonging to c - this is simply the ratio, r: ​ ​ ​ If the ratio and the score don't differ by a lot, then you know S(x) is a good estimator. An easy way to see how good S(x) is to plot a graph with S(x) on the x-axis and these ratios on the y-axis. Such a plot is known as the reliability graph, and for a good S(x) you would see the plot approximating a line with a 45 deg slope.  For NB (first pic) and SVM (second pic) the plots look like these - the blue points are the plotted points, the diagonal line is the reference (from [1]): ​ ​ ​ ​ ​ ​ ​ ​ Note that although the estimation is off for NB and SVM, their ordering of points seems to be correct i.e. if S(xi) <= S(xj), then r(xi) <= r(xj).   Calibration is the process of mapping these S(x) values to right empirical ratio values. Consider the point in the NB graph whose score is ~ 0.45. The corresponding value of r is only ~0.3. You want a mapping function f which takes in the value 0.45 and gives out 0.3 for this point. If you have f, P(c|x) for data point would be estimated with f(S(x)). This is where isotonic regression (IR) comes in. Assuming that the scores can rank points correctly, which we saw was the case for NB and SVM, IR finds a reasonable f - the specific curve IR finds is piecewise constant and non-decreasing. A sample fit with IR looks like this - a linear fit is shown for comparison (source [2]): ​ ​ ​ ​ In our case, the data points would be represented by S(x) and their desired value - the empirical ratio. The new mapping function learnt via IR, fits the arrangement of blue points (source [1] - showing just the new NB reliability graph*).  Take care in interpreting the graph below, it can be a little confusing - the x-axis is still the NB-score as before. The f that is learnt, shown by the jagged line, tells you that for a particular value of the NB score, which is on the x-axis, the corresponding f(S(x)), which is the y-coordinate from the jagged line, is quite close to the empirical ratios shown on the y-axis ( y coordinates of the blue points ). ​ ​ ​ ​If you were to plot f(S(x)) on the x-axis and r on the y-axis, you would see straight line. I was able to find a nice graph here - Classifier calibration with Platt's scaling and isotonic regression- which shows the mapped values against the empirical ratios. The blue line shows S(x) and the green line, found via IR, shows f(S(x)). Vowpal Wabbit (Fast Learning) was used for classification in this example. ​ ​ ​ IR as a means to calibrate was originally suggested in [1] which used an algorithm called Pairwise Adjacent Violators (PAV) to perform the IR. However, that is not the only algorithm available. Also, there exist other methods for calibration, Platts' method ([3]) being a common technique. * I am deliberately not reproducing the SVM graph here - if you refer to [1] for it be careful and note that the x-axis before and after IR differs. References: [1] Transforming Classifier Scores to Accurate Multiclass Probability Estimates - Bianca Zadrozny, Charles Elkan [2] 1.14. Isotonic regression [3] Probabilistic Outputs For Support Vector Machines And Comparisons To Regularized Likelihood Methods - John C Platt EDIT: removed a confusing graph Updated 69w ago • View Upvotes