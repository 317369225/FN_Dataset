What are some of the most ingenious algorithms in computer science?
I would like to put in a plug for Breiman's Random Forest (RF) algorithm, because I think it illustrates (a) how machine learning has evolved over the past decades and (b) why computer science is a fun discipline. RF is really an expansion of an older algorithm, decision tree learning. BORING DIGRESSION ON DECISION TREES FOLLOWS: A decision tree learning algorithm takes a bunch of observations with predictive variables (X) and a variable to be predicted (Y). It generates a tree of if-then rules to model the relationship between X and Y by splitting the data over and over. Typically, you build this by starting with all of the data in one group at the root, then splitting based on the variable that has the strongest impact on Y, then splitting those two halves best on the variable that has the strongest impact on Y within each group, and so on until the tree gets "big enough." There are many subtly-different algorithms to decide when the tree is big enough, to decide the exact split, to "prune" trees so they don't get too complicated, and to try and optimize them for different real-world cases. None of these is rocket science, but there are lots of picky little distinctions to worry about and parameters to choose. People wrote plenty of papers on these topics, and decision trees were a hot topic in data mining from like 1984 until at least the late 90s. In practice, they run into all kinds of issues, and their accuracy is pretty bad compared to modern machine learning algorithms. END OF DIGRESSION. Ok, so, if decision trees are kind of picky and complicated, and random forests are built on top of decision trees, they must be complicated, right? Not at all. Here is the RF algorithm: (1) Choose a random sample from your learning data X and Y (2) Build a decision tree on top of this data, but whenever you want to learn a new node in the tree, you are only allowed to look at a randomly-chosen subset of the variables. So, maybe we randomly select that when splitting node A, we can look at the "height" and "gender" variables but not the "weight" variable.      (2a) Ignore all of the fancy tweaks to the decision tree algorithm and just build the biggest tree that the data will allow (3) Return to step 1, using a new random sample and building a new tree. Do this N times (maybe a couple of hundred times), and save all N trees. Now, when you want to predict the Y for a new observation, you ask each one of the trees to predict the Y, and you average their results together. That's it. No fancy rules and tweak. Oh, and now you'll get excellent results instead of the crappy results from each tree by itself. Wait, that sounds like you gave this problem to a frustrated CS sophomore who just said "Ah, let's sprinkle some calls to rand() here and a couple more rand()s there and do it over and over." There are no fancy statistical tricks or optimization algorithms, but it all works great. Why is this a cool algorithm? Well, expanding on my original two reasons, RF: (a) Uses lots of compute power combined with a simple, but clever algorithm to give great results. This application of compute power to statistical problems has been the crucial factor in the growth of machine learning over the past decade, much more so than the much-hyped growth in data volume. Brute force allows us to get good results from un-tweaked, off-the-shelf machine learning code for a huge variety of problems in a matter of minutes, whereas a past generation of would-be "data scientists" faced a choice between inaccurate-but-fast algorithms and writing heavily customized code to eke out more accuracy at the cost of enormous development time. (b) Is incredibly interesting in practice and not that interesting in theory. Part of the charm of computer science is that methods like that can still thrive in academic literature and jump into common usage almost immediately. Even more amazing is that RF is a significant improvement in one of the most widespread problems in the world (how to do predict Y from X?), and it was only discovered in the early 2000s despite requiring no complex math. Anybody with basic training in statistics could have made the leap from decision trees to random forests, but for 20 years, no one did. Who knows what simple algorithm will come along next to have a huge impact on statistical practice? 