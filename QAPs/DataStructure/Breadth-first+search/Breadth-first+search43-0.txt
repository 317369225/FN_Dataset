What is the best (programming) algorithm that you have ever created?Anything that you're proud of and would like to share the algorithm as well?
I've got two algorithms that I'm pretty proud of: 1) When I was at LinkedIn, I came up with a faster algorithm to find routes between members. I eventually found out this algorithm is called bi-directional breadth-first search, but I didn't know that when I came up with it. In those days, LinkedIn supported seeing profiles of people who were 4 degrees away, and doing breadth-first search out to 4 degrees took approximately O(n^4) time (where 'n' was the average number of connections per user). Instead of doing a BFS out to the 4th degree, I modified our code to do a BFS out to the 2nd degree for both the viewer of a profile and the member being viewed. I'd then sort the members in each of those two 2nd degree networks and check if there was any overlap. The two members were <=4 degrees apart if and only if there was overlap in their 2nd degree networks. The result was an algorithm that was O(n^2 * log n) in theory, and about 200x faster in practice. 2) At Google, I had a task where I had a dictionary of strings, and I had to come up with a way to see if a new string was within a given Levenshtein distance of distance to any string in the dictionary. (Sample use case: say you have 1 million passwords, and you want to see if a user's new password is within an edit distance of <=2 to at least one of the passwords in your dictionary.) Levenshtein distance is an O(length longer string * length shorter string) algorithm, so running that against every string in a large dictionary is no fun. I came up with an approach that computed a character distribution histogram for every string: you take a 64-bit long, split it up into 16 sets of 4 bits, and then compute the number of characters that are X modulo 16 for X = 0...15. For example, in the string "hello", the ASCII values of characters are 104, 101, 108, 108, 111, which are 8, 5, 12, 12, 15 modulo 16. In a 64-bit long, "hello" would thus correspond to a 0001 in the 9th, 6th, and 16th sets of 4 bits, a 0010 in the 13th set, and a 0000 in every other set of 4 bits. In the end, every string I had boiled down to a single 64-bit histogram. After computing these histograms for every string in the dictionary, I could compare them to the histogram of the search string and use bit arithmetic to quickly determine a lower bound on the edit distance. For example, if one string has 5 letters that are 3 mod 16 and 0 letters that are 4 mod 16, and another string has 0 letters that are 3 mod 16 and 5 letters that are 4 mod 16, then their edit distance will be at least 5 (i.e. it takes 5 edit edit operations if you replace 'aaaaa' with 'bbbbb'). Because the lower bound was almost always much higher than than the the distance threshold I was interested in, I could avoid the full Levenshtein distance computation about 99.99% of the time. As a result, I could do my edit distance check against an entire 1m item dictionary in milliseconds, which was very acceptable for the use case I was working on. 