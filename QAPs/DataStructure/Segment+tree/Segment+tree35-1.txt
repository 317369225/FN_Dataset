What are the advantage of binary indexed tree (BIT or fenwick tree) over segment tree?
BITs are a more space-efficient version of segment trees. In a BIT, the extra data that a segment tree typically needs to hold in addition to the original array is stored within the original array itself, so that it essentially uses no extra space. However, you can only use a BIT in place of a segment tree when certain conditions hold, as described in detail below. Segment trees are more general. Part 1: The problem Segment trees are used to answer queries about properties of subarrays of a given array. Here are two sample problems: Given two indices i and j, what is the sum of the subarray from index i to index j? Given two indices i and j, what is the minimum of all the values between index i and index j? Generally speaking, let [math]\cdot[/math] be an abstract binary operator (such as + or min, but not limited to them), and let [math]a[/math] be the array, with its elements denoted [math]a_0, a_1, ..., a_{n-1}[/math]. Segment trees are used to answer queries of the form "Given [math]i[/math] and [math]j[/math], what is [math]a_i \cdot a_{i+1} \cdot ... \cdot a_{j-1} \cdot a_j[/math]?" Part 2: Segment Trees Naively, this would take O(j-i) time. However, suppose we accelerate the process by constructing a summary array S of size N/2. Let [math]S_i = a_{2i} \cdot a_{2i+1}[/math]. In other words, if we want to evaluate a query on the range from 1 to 7, we would have  [math]a_1 \cdot a_2 \cdot a_3 \cdot a_4 \cdot a_5 \cdot a_6 \cdot  a_7[/math] [math] = a_1 \cdot (a_2 \cdot a_3) \cdot (a_4 \cdot  a_5) \cdot (a_6 \cdot a_7)[/math]. The quantities in parentheses can be evaluated in one operation by looking at the summary array: we want [math]a_1 \cdot S_1 \cdot S_2 \cdot S_3[/math]. This is a speedup, but only by a constant factor. We're still going through roughly (j-i)/2 entries in the summary array. We get the idea to calculate a summary array of the summary array, to speed up that layer as well, then a summary of that summary, and so on until we have a summary of size 1.  Here's an example for the case of answering range minimum queries. Raw data: [2, 6, 4, 2, 7, 1, 3, 10] Summary level 1: [2, 2, 1, 3] Summary level 2: [2, 1] Summary level 3: [1] In  each level, the summary is computed by applying the binary operator (here the min function) to the two corresponding elements of the previous level. Now, we can break up the requested operation into a small number of chunks that correspond to entries in the raw or summary arrays. We have [math]a_1 \cdot a_2 \cdot a_3 \cdot a_4 \cdot a_5 \cdot a_6 \cdot a_7[/math] [math] = a_1 \cdot (a_2 \cdot a_3) \cdot (a_4 \cdot  a_5) \cdot (a_6 \cdot a_7)[/math] [math] = (a_1) \cdot (a_2 \cdot a_3) \cdot (a_4 \cdot a_5 \cdot a_6 \cdot a_7)[/math]. Here, [math]a_1[/math] = raw[1] [math]a_2 \cdot a_3[/math] = summary1[1] [math]a_4 \cdot a_5 \cdot a_6 \cdot a_7[/math] =  summary2[1] We can prove that at most two values from each array need to be  accessed, and since each summary cuts the size of the data in half,  there are [math]O(\log N)[/math] summary arrays. This means our query  time will be [math]O(\log N)[/math]. What must be true of our binary function [math] \cdot [/math] to allow the above decomposition into chunks ("segments") to work? The key property is that it must be associative -- without that, we could not have split the evaluation into the chunks we wanted. Both the [math]min[/math] and [math]+[/math] functions are associative, so we can do subarray sums as well with this technique. Even things like querying the product of subsequences of matrices could be done with segment trees, because matrix multiplication is associative. There's one more aspect to talk about, which is how to update the segment tree when the underlying raw data is updated. When an entry in the original array changes, the only entry that needs to be recomputed in the first level summary array is the one entry that depends on the value that changed. Then the second level summary will need one entry updated as well according to the same logic, and so on for all the summary arrays. As such, update operations require [math]O(\log N)[/math] time. Part 3 -- From Segment Tree to BIT The sum function has an additional property we can exploit, and here's where BITs come in. Consider what the summary arrays look like for [2, 5, 3, 5]: Raw: [2, 5, 3, 5] Summary1: [7, 8] Summary2: [15] But here, we have some redundant info. Because [math]+[/math] is an invertible function, we could know 8, for example, from knowing 15 and 7! From here, we get the idea to store the summary arrays in-place in the original array. For every two values we're summarizing, we will overwrite the second one with the summary value. For example, after we store summary1 in the raw data array, it looks like this: [2, 7, 3, 8], because 2+5 = 7 and 3+5 = 8. The original values of 5 and 5 can be recovered by doing 7-2 = 5 and 8-3 = 5. We then store summary2 in place as well, getting [2, 7, 3, 15]. 8 can be recovered by 15-7. This data structure is more space efficient, because while a segment tree required us to store [math]N + N/2 + N/4 + ... + 1 < 2N[/math] values, we store only N values with a BIT, no increase from what we needed to just store the raw input. Note that to make this optimization, we had to have an invertible operator in addition to the associativity requirement. min is not invertible and hence is not subject to the same optimization. Updated 8w ago • View Upvotes • Asked to answer by Anonymous