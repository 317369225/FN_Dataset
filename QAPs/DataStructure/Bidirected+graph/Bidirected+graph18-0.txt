What precisely is the relation between Semantic Web and natural language processing?- How are semantic web triples or tuples generated, using NLP? - How is a text corpus, such as a book, parsed into semantic web triples or tuples? - Can a semantic web corpus be used in conventional NLP?
the relationship is bidirectional: 1. you can export the results from a NLP process (usually an Information Extraction analysis) as RDF triples according to some predefined ontology model, e.g. find the entities, relations, etc in text and export then as RDF data, so that they can be added to some Knowledge Graph, integrated with additional information in the graph, analysed, queried, etc. An important step in this phase is the proper disambiguation (linking) of the entities extracted in text to the existing instances in some Knowledge Graph (see examples below) (in the above examples the intermediate export format is JSON, but all the important information -- classes and instance mappings for the entities found in text -- is already there and is trivially RDF-ised according to some pre-defined ontology model) 2. you can also use background infromation from existing Knowledge Graphs to improve your NLP (IE) process, e.g. get background information: (disclaimer: I work for Ontotext and I'm involved in the development of the Ontotext S4 platform - this is an example of how the various text analytics components developed in our company work) most NLP/IE tools currently focus on just the first aspect - publishing data in RDF -- and rarely reuse semantic data from existing Knowledge Graphs for improving the NLP/IE process. 