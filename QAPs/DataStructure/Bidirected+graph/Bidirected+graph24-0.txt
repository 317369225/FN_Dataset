Has Wikipedia ever been mapped graphically?
This is a fantastic idea and I'm sure someone with enough computing power and time has gotten it done at some point. What you're probably looking to do is construct an Ontology, which is definitely achievable and there are many metrics and approaches for such a problem. I also believe many of Wikipedia's articles already have a predefined ontology if they're within broad categories (like 'Physics' or 'Art'). According to Wikipedia:Size of Wikipedia, there are 4M English language articles, which is large but manageable given the right resources and program design. Step 1: Reduce the representation of each document. Wikis automatically give some helpful information in this regard as many articles are already classified. After stripping stop words and stemming, store the document. After processing the entire corpus, you can generate tf-idf scores and reduce each document to a bucket of some number (e.g. 50) of the most important words. There's more that can be done - for instance, you can re-weight the words to assign more importance to those that are in bold, in bigger font, or that appear near the top or in the first paragraph (generally summary). Step 2: Wikipedia articles consistently link amongst themselves or to common resources - this is probably a strong indication of a possible relationship. Generate and store the link graph or write a method to be run on-the-fly comparing a document against a set of links (this latter approach will result in computational redundancy but obviously requires far less storage). Since there are 4M English articles, we can assign a unique integer identifier to each. Assuming each article links to an average of about 20 other Wikipedia articles (I have no basis whatsoever for that assumption), we'd have to store 80M integers (which is ~30MB if we strictly enforce the 20 link limit, so quite small and easily stored in memory). That handles interWiki links but ignores external ones, which is a good first approach I think. Step 3: Choose a metric for Cosine similarity. There are a lot of ways to do this and the end result here will be something tailored to Wikipedia entries. Similarity will be based both on the reduced representation from Step 1 and whether we have a (1) bidirectional link, (2) unidirectional link, or (3) triangular link (from Step 2). This is among the harder problems because it's difficult to evaluate the importance of the various features (some type of learning methodology might be useful here). Step 4: Apply clustering using the similarity metric from Step 3. This is where things get interesting. The underlying representation requires the use of a Union-Find (disjiont set) data structure to store clusters. Now, if we were to use a naive clustering approach, we would have to compare every distinct pair of documents (n(n+1)/2) - in this case, that's about (4M)^2 / 2 = 8e12 comparisons, which is definitely not feasible. In order to resolve this, you need be strategic and define entire clusters as a single entity that is most central or representative. Then, adding a document to the graph is merely an issue of comparing against all the clusters at the time of addition, which can be strictly limited to a set number. I've actually had it in my mind to tackle the issue of writing a dynamically-readjusting clustering application for a while but the complexity is enormous so haven't quite gotten around to it :) Step 5: Take the clusters and represent them in a fancy visual graph a la the Facebook world map. You can display at least each edge between the clusters generated and resize each cluster and the distances between them according to number of documents and cosine distance, respectively. You can also apply the same method recursively in each cluster (clusters within a cluster!) and display those as well. In any case, that's my initial pass at how I would approach it. I'm probably wrong in certain places and please correct me if that's the case! Thanks for the question - I really enjoyed thinking about it! 