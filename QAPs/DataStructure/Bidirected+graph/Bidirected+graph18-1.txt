What precisely is the relation between Semantic Web and natural language processing?- How are semantic web triples or tuples generated, using NLP? - How is a text corpus, such as a book, parsed into semantic web triples or tuples? - Can a semantic web corpus be used in conventional NLP?
Regarding your first question, the answer is yes, but it's not the only way. Resources such as Wikidata and DBpedia have automatic methods to extract structured information from free text in the web, so NLP methods are applied for that. For instance, DBPedia extracts structured information from Wikipedia (info boxes, redirection, etc.). Wikidata also extracts information from the web, but in addition it is reliant on human contribution. I don't know the answer to your second question, but I think you may find the answers in papers about the construction of such knowledge resources: 1. Freebase 2. DBPedia 3. Yago 4. Wikidata As for your last question, of course it can be used for many NLP tasks. We used it for lexical inference: given two terms x and y, predicting whether the meaning of y can be inferred from x (e.g. cat cat→animal,LadyGaga→person  ). We used the paths connecting x and y in the knowledge resources (each triplet is a labeled edge between two concepts in the resource), and classified them as indicative or non-indicative for inference. For example, Lady Gaga -(occupation)-> singer -(subclass_of)-> person indicates that Lady Gaga is a person, but Lady Gaga -(place_of_birth)-> New York -(instance_of)-> city doesn't indicate that Lady Gaga is a city. If you're interested in more details, you can look at our paper. 