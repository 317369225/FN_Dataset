Can all mathematical functions be fully replicated using a sequence of '+', '-' , 'x', '/' operators (as can y to the power x)?
Early electronic calculators worked in exactly this way, using series expansions to calculate transcendental and other functions. You could watch the display flicker while the series was expanded. You can't fully replicate functions in this way, but you can certainly approximate them over some interval. For example, on early calculators, random numbers were approximated by raising a seed to a power modulo a range, and any log and power functions that were part of that were calculated using series expansions. 