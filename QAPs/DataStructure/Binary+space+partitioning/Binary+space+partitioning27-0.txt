What's the difference between distributed and distributional (semantic) representations?Besides the differences in learning algorithms, i.e. LSA for distributional and neural net language models for the distributed, what is the intrinsic difference between these two kinds of word semantic representations?
Even though the names sound similar, they are different techniques for word representation. Distributional word representations are generally based on co-occurrence/ context and based on the Distributional hypothesis:  "linguistic items with similar distributions have similar meanings" and hence words with similar distribution have similar representations. The distributional property is usually induced from document or context or textual vicinity (like sliding window). Lets call this distributional input as M w,d   . The goal is to then reduce this M w,d   into m w,k   where k<<d  and also preserving the similarity. A simpler transformation is linear decomposition, but there are other sophisticated techniques too. Some of the common representation techniques are Latent semantic analysis Latent Dirichlet allocation Self-organizing map HAL Independent component analysis Random indexing Distributional techniques are memory intensive and not as efficient (not a compact representation) as distributed representations. But it is simpler to construct and even a linear decomposition like LSA works fairly well on low rank co-occurrence matrix, so popular than other methods. The representations are dense like distributed. Distributed representations are compact, dense and low dimensional representation, with each factor in the representation representing some distinct informative property. These latent factors jointly represent the space efficiently. To understand distributed representation, it has to be contrasted with local representations like non-parametric methods. In local representations each region in the vector space is represented by an exemplar and thus divides the space. Because of the curse of dimensionality, this is inefficient. An efficient scheme is binary coding which might result in log 2 (n)  bits instead of n  bits. The image shows how just 3 bits can efficiently partition a space using distributed representation, instead of 8 bits using  local methods. Here it differs from distributional representations as the constraint is to seek efficient dense representation, not just to capture the co-occurrence similarity.   Distributed representations comes with a cost. They are computational intensive and the algorithm is not simple like other techniques. One common approach is using neural networks and advancement in deep learning techniques are helping in lowering the training complexity. Common distributed representations are Collobert and Weston embeddings HLBL embeddingsOne good news is distributed representations are reusable, at least for common english words. One such instance is downloadable from here: Word representations for NLP References: Word representations Word representations for NLP Yoshua Bengio's Research 