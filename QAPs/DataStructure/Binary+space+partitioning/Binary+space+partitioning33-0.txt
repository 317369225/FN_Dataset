What does the following passage from a deep learning paper in machine learning about distributed representations vs. non-distributed representations mean?
Part of this answer by Bengio answers your question: Yoshua Bengio's answer to Artificial Intelligence: How are deep belief networks fundamentally different from clustering? You can think of it as partitioning your input space into meaningful regions. In a clustering approach, with k  clusters, you can partition the space into k  regions. This is because only the closest cluster centroid "fires" for a given input, and none of the other clusters. If you think of cluster assignments as a one-hot vector of k  dimensions, this gives you k  number of possible hidden representations. In a distributed approach, every splitting hyperplane (i.e. every unit in a hidden layer) divides the input space into two, which means addition of another unit multiplies the number of regions by 2, giving you a total of 2 k   regions. If you concatenate these binary decisions into a k  dimensional vector, you have 2 k   number of possible hidden representations. Essentially you can "encode" much more with the same number of parameters (note that both approaches require a single vector for every "hidden unit" with the input dimensionality). However this discussion is purely based on representational power, and does not consider learnability. It might be possible that even though the distributed approach has more representational power, it might be difficult to learn from data, and this representational power might be underutilized. I think we can say that distributed approaches have proven to be better than local approaches in many tasks empirically, so the experimental results in the literature still gives some weight to this argument (but your question does not address this, so it's okay). 