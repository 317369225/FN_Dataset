What are the differences between Crunch and Cascading?
From the Crunch FAQ [1]: The main difference between Crunch and Cascading/Pig/Hive is in their data models. In Pig and Cascading, most operations in a pipeline are performed on collections of Tuples (here are the Javadocs for the Pig Tuple and the Cascading Tuple). Using the Tuple data model makes it much easier to implement common operations, like joins and aggregations, and Pig, Hive, and Cascading all provide large libraries of built-in functions that are designed to operate on their respective Tuple types. They also provide APIs for developers to create their own user-defined functions that interact with the Tuple-based data model. Crunch uses a data model that is essentially a thin layer on top of the key-value inputs/outputs of a Java MapReduce, and currently has two implementations, one based on Writables and the other based on Avro. This is a reflection of its origin in helping developers who were working with structured binary data records (e.g., HDFS files, seismograms, hyperspectral satellite images) that did not naturally map on to the Tuple-based data model. The virtue of the Crunch data model is that it makes writing user-defined functions extremely easy and avoids the overhead of creating and destroying Tuples that are made up of a single field. We can also take advantage of the performance benefits of Avro serialization during the shuffle phase of a MapReduce. [1] https://github.com/cloudera/crun... 