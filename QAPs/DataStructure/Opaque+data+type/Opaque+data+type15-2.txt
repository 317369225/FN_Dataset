What is New SQL?
"NoSQL" The story actually begins before "NoSQL". At a point in time, MySQL was by far the dominant database in the web applications arena. It had a number of "known" problems around which everybody had to code at the application level, and no credible vendor backed answer to these problems. These "known problems" were: Schema changes lock tables for a long time. There are no data types that are convenient for loosely defined data, and as a side story to that. No good data types and ways to deal with tree structured data or other hierarchical data structures. Replication is a bitch to set up and manage in MySQL. There is no solution in native MySQL for sharding or data that is larger than a single instance of MySQL. "Distributed anything", including distributed locking and distributed transactions are a largely unhandled problem in conventional MySQL. The entire "NoSQL" thing is in a way an unstructured way of a market to attempt to answer that. 1 Schemaless The idea of "schemaless" in its various incarnations is an attempt to get rid of the long term locks that "ALTER TABLE" in MySQL made necessary. In a "schemaless database" there is no schema that is being enforced or managed by the database server. That does not necessarily mean that the data stored is a structureless BLOB, though. Some "NoSQL" products offer only that, though: They are simple key-value stores where a shapeless opaque BLOB is stored under an arbitrary key. Even simple operations such as enumerating all keys currently present are unavailable or severely restricted in the way that enumerating the keys is not an atomic operation or that enumerating they keys and then fetching the value for one of these keys is not atomic. Almost all "NoSQL" products allow you to do only point operations atomically and have problems with atomic operations on multiple keys. Some "NoSQL" products allow you to store structured data, commonly expressed in JSON notation ("Hash of Hashes", basically a throwback to hierarchical databases of the Pre-Codd era). Application programmers slowly woke up to the truth that there is no such thing as "schemaless", though, because data has meaning. That is, even if the store does not enforce a schema, your application requires certain invariants on the data to be true at all times. Not having the concept of a schema in a database leaves you with several architectural choices: Ignore the problem. The application breaks eventually because data exists in the persistent store that for one reason or the other violates such a required invariant ("Required attribute <something> is null"). Version records with a "version" attribute and write migration procedures. That usually is a loop which runs through a collection of keys, reads each value associated with that key, checks the "version" attribute and then executes a number of functions which perform the migration and upgrade the "version" attribute. The modified record is written back. Version records with a "version" attribute and keep any number of upgrade procedures around in the codebase. Modify the ORM such that for each record read the record is migrated to the current version before it is being handed to the application. When the application releases a record, write it back. This is the "lazy write" or "upgrade on access" version of strategy #2. In practice this is basically what ALTER TABLE does - old ALTER TABLE is basically doing #2 with proper locking, new ALTER TABLE does a combination of #2 in the background and #3. Modern MySQL on InnoDB does not lock tables under ALTER TABLE any more, the rest uses pt-schema-change, so the problem is basically solved in MySQL land. Postgres always did things this way, in fact, DDL in Postgres is transactional and can be rolled back. 2 "losely defined data" A lot of data in modern web applications is semi structured data, often written in JSON or XML notation. While it is theoretically possible to normalize the structure and create a schema for it, the resulting schema is inconveniently convoluted, slow to access and makes it hard to write applications against it. "Document databases" (MongoDB, CouchDB) promise to tackle the problem by allowing you to store such data ("documents", a record containing JSONified data) directly. They often have functionality to extract attributes (if present in a record), modify attributes ("partial document updates") and index attributes. At the same time they have problems implementing proper local persistence (MongoDB is notoriously bad at this, ignoring 40 years of research and going for a silly mmap/msync approach with a zillion reliability and performance problems that are inherent to mmap) and proper scaling (CouchDB implements proper MVCC, but has the vacuum problems that Postgres has, only magnified by a very simplicistic approach). MySQL is only new tackling the problem with "virtual columns" (MariaDB) and "JSON data types" (Oracle MySQL), which are indexable and updateable. There is not yet a lot of reliable real world data on the performance and scalability of this, but at least the underlying storage engine knows how to save data efficiently. Postgres at the moment seems to lead the pack with this kind of data, in all MongoDB benchmarks Postgres comes out as the "better MongoDB than MongoDB and a proper database for free" product. 3 "Hierarchical data" "Document databases" allow programs to store and load tree structured data as JSONified structures, which basically are small trees. With Point-Operations, applications can directly code large tree operations in app-land (as opposed to db-land), often lacking proper locking and foreign key enforcement, though. MySQL still has no SQL statements and no recommended default data model for large trees, and small trees can be handles within the new XML or JSON data types. Postgres has tree-like SQL with its modern SQL statements (see Modern SQL in PostgreSQL  on slideshare for details), and does transactions on all of this nicely. 4 Replication is a pain to manage MySQL has a good replication story, which with the advent of row based replication and parallel replication became even better. Traditionally, setting up MySQL replication was a manual process, though, and requires a good understanding of backup, restore and point-in-time recovery before it can be done successfully. Especially MongoDB has had a better story here: things happened largely automatically and their story also ties in nicely with their sharding. MySQL has made recent developments that make it shine, though: Binlog servers as proposed by Jean-Francois Gagne and as discussed in MariaDB Replication, MaxScale and the need for a binlog server make managing and scaling replication a lot easier and allow you to do away with many complications that the competing model of global transaction ids introduces.  orchestrator/orchestrator completely takes the pain out of replication management in MySQL. 5 Sharding MySQL has had no code at all to handle schemata that are larger than a single database instance. Admittedly that is some kind of first world problem to have: MySQL actually runs fine on machines with 40+ cores and 256 GB+ of memory. That's good for handling databases larger than a terabyte of data without many disk reads at all, if you are careful in the way you design the schema. Most people will never have need to shard. Many "NoSQL" products are bad on a legendary scale when it comes to writing to disk. Redis basically can't write at all, other KV stores simply don't. MongoDB also is known for major braindamage when it comes to lock handling, writing to disk and scaling write load, and the problems they have are intrinsic to the tenets of their design and very hard to fix. Thus, the pressure for sharding in order to scale is much higher with users of these products. Also, because these products usually have no transactions and no joins, the things that make sharding in SQL databases hard to provide are 'problems left as an exercise to the end user' in NoSQL products, and so they often can provide sharding "solutions", but they are without distributed transactions and of course also with no joins (so you end up with the moral equivalent of a hash join inside the application, but with all kinds of dirty reads, stale data and locking problems). MySQL now provides a sharding story that is equivalent to the "NoSQL" sharding story through proxies such as MaxScale. 6 "distributed anything" Nobody in OSS land has a serious and complete solution for that. MySQL has "NDB cluster", but you'd want to stay away from it. And "NewSQL"? In "NoSQL" land, people at some point realized that "NoSQL" solves the actual hard stuff by leaving it to the application programmer, and not only at the write-side, but also at the read-side: People are accessing data structures horizontally, manually enumerating records by pushing cursors through tables or indices and applying predicates in a loop manually. This kind of sucks, and generally the need for a declarative data access language instead of procedural data access was seen. SQL is precisely that: An ad-hoc compiler that takes a declarative data access statements and automatically generates a procedural data access program from it which does the above: manually pushing a cursor through data or indices and fetching all records that qualify. Adding a declarative language code generator on top of your favorite "NoSQL" product is generally what can be considered the common denominator for all things under the "NewSQL" marketing term. The actual level of sophistication varies a lot. crate.io for example is a thing veneer of SQLish on top of a largely unmasked Elasticsearch, Hive and Impala are actually rather thick and powerful SQLish layers on top of Hadoop and HBase respectively. What is missing is the moral equivalent of Googles F1 as an open source product. Overall, "NewSQL" is the "NoSQL" proponents acknowledging that procedural database access sucks, big time, while recent MySQL releases and improvements and tools as stated above are the acknowledgment of the MySQL community that they have had work to do and have been addressing their most glaring shortcomings. All the while the Postgres people have, in relative obscurity, been hard at work to silently improve the awesomeness of their product (not that it is problem-free, they are just much quieter with everything, good and bad). TL;DR: »"NoSQL" is the nonanswer to a long list of MySQL shortcomings of the past. These shortcomings are mostly moot points by now. "NewSQL" is the reaction of the "NoSQL" people to the realization that procedural data access programs are teh suck and is adding compilers from SQLish-to-datastore on top of any NoSQL-product.« 