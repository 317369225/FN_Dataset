How can Bayes' theorem be interpreted in terms of inferring causal relationships in the data?
I'm not really sure that causality is well-defined in the context of probability/statistics. I'll try to explain one way of looking at it, but in my opinion it's not a very good way. I say this not because I think the method is itself bad on a relative level, but because on an absolute scale I don't think that there is a good way to infer causality in a fixed dataset using purely probabilistic/statistical principles. Someone else has already mentioned Judea Pearl and his paper. If I recall correctly, the paper hints at Bayesian networks, but doesn't explicitly say anything about them. Bayesian networks are directed acyclic graphs, where the nodes represent random variables and the edges encode conditional dependence. One way that I've used Bayesian networks is to do structure learning and then pretend that the edges represent causality. The motivation behind this interpretation is that if A causes B, then we ought to infer that B is conditionally dependent on A (or that an edge exists from A to B). This is easy to mess up for at least a few reasons. In particular, structure learning for Bayesian networks is basically a combinatorial optimization problem. I think it's NP-complete, but I don't have a source handy for this. 