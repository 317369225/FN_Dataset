If I want to start with bioinformatics or computational biology, what topics should I learn? What are the actual challenges in this field?
There are already some nice answers on this page but they mostly describe concepts that you should learn. I am all about learning how to "do computational biology" and so this answer will focus on the 5 (in my opinion) core papers and techniques you got to learn. Along with each paper that I recommend, I also note some of the cutting edge challenges in the field which are related to the paper but often not completely solved. The challenges are sorted by difficulty and written in bold to draw attention. Also, disclaimer: This answer is most relevant to someone trying to read Computational Biology papers with a view of doing research in genomics. Some of the principles used by the authors of these papers are however applicable to all fields of computational biology. I am going to try and limit myself to 5 principle papers that have influenced me the most for the sake of brevity. With these disclaimers out of the way, here goes: 1). Basic local alignment search tool This paper describes the BLAST algorithm used to align nucleotide as well as amino acid sequences. I think most of modern computational biology can be traced back to this paper and is a classic paper to read. Plus, while newer software has been developed, BLAST is still heavily used. Classic paper. MUST READ! Challenge: Honestly, local alignment has been solved extremely well by now. So there is not a big challenge here. This paper mainly helps you to learn to think about computational biology and can be applied to solve some other challenges downstream. However, I think a good challenge to start with is "After reading the paper, try to implement the algorithm on your own". This will get your juices flowing! 2). Exploration, normalization, and summaries of high density oligonucleotide array probe level data When the microarray technology came around in 1999, it created a revolution. Scientists could study things at the whole genome level and hence several new questions could be answered. One major bottleneck though was that hardly anyone knew the correct way to analyze the data. This paper, written by a group of statisticians, laid the foundation of how to analyze microarray data in a mathematically rigorous manner. Several important discoveries made using microarray's have methods introduced by this paper and ones derived from this paper at their backbone. In a way, this paper started the era of software for genomics. MUST READ! 3). Ultrafast and memory-efficient alignment of short DNA sequences to the human genome It is well known and often cited that nucleotide sequencing is following and exceeding Moore's law [1]. A major improvement in cost and efficiency occurred when people started using shorter reads for sequencing nucleotides. However, this created a big analytical hurdle. The human genome is 3 billion characters and many of the reads being generated are as short as 35 bps. Also, there are routinely tens of millions of such reads generated per sample with at least 6 samples per experiment (3 case, 3 control). So, for a simple sequencing experiment also one could imagine having to very accurately map several hundred million reads uniquely to the human genome. An important break through in analyzing this vast amount of data when Bowtie was developed. On a regular laptop used by the majority of scientists, Bowtie is able to very accurately map several million reads to a large genome such as the human genome in a few hours. This kicked off a storm of papers describing methods for such short read alignment and as a first step in almost every genomics analysis pipeline. MUST READ! If very interested, also read: Fast gapped-read alignment with Bowtie2 Several new features added on top of Bowtie! Bowtie2 has now almost completely replaced Bowtie. Challenge: Many applications were not compatible with Bowtie1 when it was released as acknowledged by the author. 4). Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation A major reason for using nucleotide sequencing is to accurately estimate the abundance of all the transcripts and compare this abundance across conditions. Here's a very simplified experiment: Sequence RNA derived from a healthy subject and a cancer patient. Compare the RNA species different between the subject and patient. Such a comparison would likely yield information about pathways going wrong in a cancer patient and bring us closer to a cure for cancer. An important problem in this experiment is to accurately estimate abundance of different transcripts and this paper describes the Cufflinks suite that is widely used to solve this problem. Additionally the authors have done a great job of conceptually explaining the mathematical foundations behind their software. MUST READ! Please also read: Differential analysis of gene regulation at transcript resolution with RNA-seq. This describes a technique in the Cufflinks suite to find transcripts that are significantly different between 2 conditions. For a protocol describing step-by-step analysis pipeline, refer: Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks 5). The Spectrum Kernel: A string kernel for SVM protein classification Disclaimer: This is almost like a pure computer science paper. So, if you're a biologist, please be patient in trying to understand the paper. You can only go so far by analyzing experimental data; whether your own or publicly available. The next step in becoming a good computational biologist is to be able to develop models and classifiers learning differences between different conditions and being able to predict what class a new data point will be assigned to. This paper helps to begin addressing these questions. Machine learning has been a transformational subfield in computer science and support vector machines (SVM's) in particular lead the charge in classifying objects. This is a problem routinely encountered in biology. Imagine a scenario in which you need to learn from existing experimental data features that distinguish DNA sequence bound by different proteins and next classify novel DNA sequence based on which protein it will interact with. SVM's are very good at solving such problems. One of the tricks used by SVM's for such non-linear classification is to map properties of the input on a higher dimensional space (kernel) and draw a complicated boundary surrounding each of the different classes. Most SVM kernels developed in computer science are for numbers while most biological problems have sequences i.e. characters of different lengths (k-mers) as input. This paper described a string kernel which can be easily used to map sequence (k-mer) features on a higher dimension. While SVM's are being competed out with the advent of deep learning, this paper is a great start to get an understanding of methods used by several subsequent papers. 