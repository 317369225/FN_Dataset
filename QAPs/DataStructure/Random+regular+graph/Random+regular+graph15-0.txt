What would be the effect to inject a methamphetamine-like input to an artificial neural network?
I rolled my eyes and chuckled when I initially saw this question, but then I started thinking of an experimental setting in which something like "taking meth" can be mathematically approximated, wrote some code, and came up with a result. Part 1. The Meth After googling around for a few minutes, I came upon websites like DrugFacts: Methamphetamine, which explain that methamphetamine releases neurotransmitters into the brain, meaning that it amplifies the synaptic connections, and essentially "kicks your brain in high gear," so to speak. I'm going to assume that this effect can be approximated by amplifying the strength of the synaptic connections between the nerve cells in the presence of meth. (This could be a crude oversimplification, but I need something to work with.) Part 2. The Artificial Neural Network An artificial neural network is a mathematical model of cascading non-linearities which has very high variance, and which is usually trained using stochastic gradient descent (SGD) on a differentiable objective function. For the purposes of our experiment, let's consider a toy example: a network of [math]10[/math] inputs, with [math]10[/math] hidden units, and a single output. The inputs [math]x_1, x_2, ... x_{10}[/math] are binary values, and we would like our network to learn [math]x_1 \oplus x_2 \oplus ... \oplus x_{10}[/math] (where [math]\oplus[/math] denotes the XOR logical operator). In vectorized notation, the feedforward pass of our network essentially computes: [math] o = f^o(W^of^h(W^hx+b^h)+b^o)[/math] where [math]f^o, f^h[/math] are the activation functions for the output and hidden layers, respectively. I built my network with [math]f^h = sigmoid(x) = \frac{1}{1+e^{-x}}[/math], and [math]f^o = x[/math]. I tested the network on 200 randomly-sampled test examples, and I trained it using Stochastic Gradient Descent (SGD) with [math]1000[/math] on-the-fly generated training examples, measuring the network performance on the test dataset after each update. To simulate methamphetamine use, I multiplied all weights in the network by [math]3[/math], which was reflected in the following updated equation: [math] o_{meth} = f^o(3W^of^h(3W^hx+b^h)+b^o)[/math] This is supposed to make all synapses "more sensitive" by a factor of 3. I trained the networks for 1000 steps, with no regularization, no momentum, and a learning rate [math]\alpha = 0.03[/math], and compared three different regimes: (A) no "meth" given, (B) "meth" given to the network from the beginning, and (C) "meth" administered between 25% and 75% of the training process. The random number generator used for initializing the weights and generating the test dataset was reseeded for each experiment, so that all test datasets are identical and all networks start out with the same set of weights. The results are summarized in the chart below: As you can see, the best performing network is the one to which no "meth" has been given (no multiplicative factor on the weights), drawn in blue on the graph. The scenario where "meth" was given from the beginning (the red line) does show evidence of learning, but is highly unstable. The scenario where "meth" was started at 25% of the tranining, and stopped at 75% is the more interesting one. Upon starting to amplify the weights, the error shot up dramatically. This is because the output layer is linear, and amplification had a linear effect on it. If the output layer had been non-linear, then learning would have effectively stopped, because the sigmoid neurons would have saturated instead. Upon reaching 75% and stopping the "meth", the network went back to normal. I uploaded the MATLAB code snippets used to repeat this experiment to Page on filedropper.com. Disclaimer: Do not take these results seriously. There is no such thing as "putting an Artificial Neural Network on drugs." 