What do eigenvalues and eigenvectors represent intuitively?
Strictly mathematically speaking, given a Vector space [math]V[/math] and a Linear operator [math]A[/math] acting on that vector space, the eigen vectors represent those points in [math]V[/math] whose direction remains unchanged post the action of the operator. This is essentially what [math]A(x) = \lambda x[/math] means i.e when I apply [math]A[/math] to [math]x[/math] which is the eigenvector, I get another vector whose direction is the same as [math]x[/math] but whose magnitude is scaled by some scalar which we call eigenvalue. In machine learning, eigenvectors pop up in different places and sometimes have very direct meaning and sometimes it is more to do with intuition. 1] Markov Chains and Random Walks Given a finite state-space time-homogeneous markov chain with a transition probability matrix [math]P[/math], finding a stationary distribution of this chain comes down to solving the following equation [math]Px = x[/math]. This is equivalent to finding the eigenvector of [math]P[/math] corresponding to eigenvalue 1. One can think of PageRank algorithm as an application of Markov Chains and page rank values are actually the entries of the dominant eigenvector of the transition matrix. In these two cases, eigenvectors (that too, one of the eigenvectors) represent different things. In one case, it represents a stationary distribution whereas in the other case, it represents page rank values. So, they have very direct meanings. 2] Dimensionality Reduction Consider a feature vector space [math]X[/math] where your feature vectors reside. Let us assume that [math]X[/math] is a Hilbert space. Now, if [math]A[/math] is a compact self-adjoint linear operatorÂ  (do not worry about what it means for the moment, just that [math]A[/math] is some matrix satisfying certain properties), then the eigenvectors of [math]A[/math] also forms the orthonormal basis of [math]X[/math] according to Spectral theorem. If [math](\lambda_1, u_1), \ldots, (\lambda_n, u_n)[/math] are the eigenpairs (assuming finite dimensional hilbert space), then any point [math]x \in X[/math] can be written in the transformed space as [math]A(x) = \lambda_1\alpha^x_1u_1 + \ldots + \lambda_n\alpha^x_nu_n[/math]. In essence, if we represent [math]A(x)[/math] using the eigenvectors as basis, then they can be written as [math](\lambda_1\alpha^x_1, \ldots, \lambda_n\alpha^x_n)[/math]. The superscript x indicates that the alphas are dependent on x, but also highlights that lambdas are independent of x. Now, if the linear operator [math]A[/math] is such that that many of the eigen values are small and close to zero, then in the representation of [math]A(x)[/math] given by using the eigenvectors as basis, we can practically remove all those dimensions corresponding to which eigenvalues are zero. This gives us dimensionality reduction. The question then comes down to how to choose the linear operator [math]A[/math]? Principal component analysis tells you to choose the covariance matrix. In weka, you would find that they use the correlation matrix. So, in dimensionality reduction, eigenvectors correspond to a basis where we can get sparse representations for our points. The absolute eigenvalues signify in some sense the importance for a particular dimension. 3] Laplacian of the graph A Graph Laplacian matrix is a Discrete Laplace operator. However, this operator pops up in several places. One can observe it in information retrieval where people have modified page rank algorithm using the graph laplacian or in classification where it comes up when using Manifold Regularization. However, it is not entirely clear at least to me what eigenvectors and eigenvalues of graph laplacian truly represents in these cases. However, I know of at least one case where the eigenvectors of a graph laplacian are used as a basis for representing functions over manifolds. This appears in the work of Proto-value Functions Group for reinforcement learning. As you can see, mathematically the eigen vectors have a very fixed meaning and then there are host of properties that one can study about them. These ideas were essentially developed from the perspective of solving linear differential equations whose solutions are actually written in terms of the eigenvectors of the corresponding differential operator. However, in machine learning, eigenvectors have taken several meanings while the underlying mathematics remains the same. 