What do eigenvalues and eigenvectors represent intuitively?
I've found a number of ways to teach it over the years. Both Leif Walsh & Ben Cunningham explanations are good ways to think about, so I'll just weigh in with my +1 and additional commentary. The one that I've found that resonates with students is to still go back to the old ellipsoid.  The problem set being take a bunch of data point and find the ellipsoid that best fits the data. The 1st eigenvector will be the semi-major axis and the value is the magnitude and the 2nd one is the semi-minor axis.  From there is is easy to explain how to carry it forward for n-dimensions.   I've also found that people respond quite well to the notion of "explained variance".  I usually explain it along the following path.  Suppose you have points in n-dimensions.  What would be the 1st vector that you should use to describe the points while minimizing the error (in an L2 sense)?  What would be the 2nd one, etc.  These are the corresponding eigenevectors/values.  Good explanations of this can be found under principle component analysis (PCA for stats people), empirical orthogonal functions (EOF for meteorologists). One of the projection (matrix mapping) explanations I like the best is in Numerical Recipes. The other one I really like is in Gilbert Strang's book.  I'd be curious if anyone has a good explanation for the generalized abstracted from (generalized algebra).  The only way I know how to teach this one is from first principles. Updated 212w ago • View Upvotes