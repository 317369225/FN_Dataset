What do eigenvalues and eigenvectors represent intuitively?
I think you actually mean "What are eigenvectors intuitively?" not "What are they exactly?" If you can compute them then you know exactly what they are: they're [math]\vec{x} \text{ such that }[/math] [math] [\mathbf{M}] \cdot \vec{x} = \lambda \cdot \vec{x}, \lambda \in \mathbf{k} \text{ the underlying field}.[/math] The reason we care is that N×N matrices, which have N² entries cataloguing the many interactions among N⇶N variables, can usually be reduced in complexity. Gram Zeppi  called this the pivotal and non-obvious part of linear algebra. (To outsiders it's pure magic.) Eigenvectors are simpler than, yet summarise, matrices. (As do all matrix factorisations.)     There is an intuitive explanation viewing [math]2 \times 2[/math] matrices as linear transformations of the plane. (pic: Flanigan & Kazdan) Eigenvectors point in the same direction before and after the transformation. In few words: "The «stay-the-same» directions." (pic: Wikipedia) In the Mona Lisa shear above, the blue vector is an eigenvector because its orientation does not change upon transformation. I could keep repeating the transform shear, shear, shear, shear, shear, shear, shear and it would stay the same. This is important if you are trying to find out what's stable under repetitions of a linear transform. So the eigenvector keeps pointing in the same direction. But does it stay the same length? The eigenvalue associated to it says how the length changes. This becomes important in for example dynamical systems (or systems of diffeq's) as it indicates the stability behaviour. pic: Wikipedia (The  vertical axis here is sum-of-eigenvalues. The horizontal axis is  product-of-eigenvalues. So points on this "plane" could have come from a  high-dimensional matrix repeating on a system. It's helpful to know, due to theorems, that (1) the sum of diagonal entries = the sum of the eigenvalues, and (2) the product of diagonal entries = the determinant. So neither axis is super hard to compute.) pic: Elmer Wiens Without eigenvalues and eigenvectors it would be hard to talk qualitatively about linear dynamical systems like [math]\mathrm{system}_{\text{next time step}} \ = \ \left[ \mathbf{MATRIX} \right]  \cdot \mathrm{system}_{\text{now}}[/math]. (…and what good is linear? how common could that be? you ask. Well, with calculus many functions can be linearised—at least locally.) Edit: see this comment below for some applications. Please reply there with more! Edit 2: Read Feature Column from the AMS, it's more / better pictures plus a common application. Outside Dynamical Systems it's useful to factor matrices so one complicated one is expressed as a sequence of simple steps. ......shear is equivalent to....   rotation + stretch - See more at: Feature Column from the AMS Updated 20w ago • View Upvotes • Not for Reproduction