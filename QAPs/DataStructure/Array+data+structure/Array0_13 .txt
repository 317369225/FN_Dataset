Why do array indices start with 0 (zero) in many programming languages?I see lots of answers explaining why zero-based indexing is cleaner in some applications, like hash tables, and a few mentions that Pascal allowed the programmer to choose the starting position. Certainly one should be able to index from zero (indexing only from one leads to messy code, as several examples here show), but that doesn't mean zero should always be the first element. It's a shame that Pascal's flexibility in this regard was not kept and expanded in modern languages. I ought to be able to, for example, create a triangular array by providing an indexing function. There is no run-time cost to indexing from an offset other than zero. Calculating the address of a[i] costs exactly the same whether it is the ith or the (i+1)th element of a, because any decent compiler will regroup the address index address(a) + (i+1)*width as (address(a)+width) + i*width, where address(a)+width is a compile-time constant. The same goes for addressing from any fixed offset, so there is no run-time cost for, for example, having an array indexed by months March..August as a subrange of an enumeration of months. 