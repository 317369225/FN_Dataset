What is the difference between a compiler and an interpreter?
You'll be relieved to hear that there is no paradox to resolve, but there is a bit of background, so I'm afraid we will need some vocabulary. Let us begin by telling some lies from the operating system: when a program runs, it has the entire computer to itself the memory allocated to a program is a long, linear array of addresses This isn't anywhere near the truth, but when a native-binary program is launched (thus becoming a process in memory), the operating system loads it into a memory sandbox where it can sit and believe this without any nasty side effects. So what is a program, anyway? It's a bunch of instructions (cleverly called the text segment), a bunch of space for global data (imaginatively named the data segment), a bunch of empty workspace for intermediate calculations (called the stack), and a bunch of empty space to place stuff we don't know the size of before it's needed (called the heap). There's more detail, but those are the principal parts of a process. The program runs by the CPU keeping a counter that points somewhere into the text, the instruction there tells it what to do to the data areas or instruction counter. Afterwards it adjusts the instruction counter (usually just increment it), and repeats with the next instruction it points at. A binary executable file is something like a freeze-dried copy of an unused process image, program included. Here's a rough diagram to put the executable file, the process image, and the CPU into context with each other: Remember, the CPU is dumb as a rock. This is because it is a rock, it just happens to be a rock with interesting electrical properties that look like calculation, if you use a bit of imagination. The instructions in the text segment are fed right into the processor's rock stupid instruction unit, so they can't contain any philosophy. Simplified, they are indices into a table which lists all the places numbers can be kept (like A,B in the figure), and all ways to combine them in one step: stuff like load, store, add, subtract, jump, and other dead easy operations fit for an automatic machine. This sequence of number codes is embedded in the executable file (open one with a disassembler, you can read them yourself), and it is a compiler's job to turn a higher-level program into this string of numbers. When it's finished, you have a freeze-dried process image that your O/S can put right in a sandbox and point the processor at. What it finds in the text segment has already been put into CPU native language after a fashion, so this program can run as many times as you want without any more translation taking place. Now, processors don't have a monopoly on accepting instructions, programs can read instructions too, that's what most of them do in some format. If you write a program that reads a bunch of general program instructions and make them happen one by one while they are being read, you have created a program that acts like a different (and possibly more advanced) processor than the actual one it is running on. Programs for this pretend-processor can look very different from the native instructions, provided you have your pretender program step between and read them every time. By ways of the illustration, the situation looks something like this: This software-made processor is an interpreter. Depending on how abstract and what sort of instructions it reads, people may also call it things like "virtual machine" or "emulator" or such, but the principle is the same. To further confuse the terminology, with things like Java there's a compiler turning Java programs into simpler instructions (bytecode), and storing them in a file read by an interpreter (virtual machine), adding the whole circus once more, just starting from one level up the abstraction ladder. The major difference for compiled or interpreted languages, is that compiled languages need to contain all the information to decide on the instruction stream once and for all. That is, they need to know what all the parts will be, how big are they are, and this and that - the program must contain this information, one way or the other, because the compiler has to translate it into a finalized stream of instructions. Interpreted languages can invent and reinvent things in different ways every time they run a program, so it's easier for languages to admit programs that change themselves, alter data types on the go, detect when stuff goes wrong, and in general behave a little more dynamically in their surroundings. The price paid for this flexibility is that your code isn't running directly on the processor, it's running on top of another program that runs on the processor, and for programs that need to do heavy processing all the time, the added layer will make things run noticeably slower. Putting it a little bluntly, a compiler prepares a program so that the program itself can execute, whereas an interpreter likes execution so much that we put a program in its program, so it can execute while it executes. So to speak. Updated 84w ago • View Upvotes • Asked to answer by Vivek Rajendran, Hadayat Seddiqi, and Tushant Jha