What is the difference between a compiler and an interpreter?
Many many great answers here. I can add only one thing: Compiler - a program Interpreter - a program running a program Also as many have stated that's been muddled by modern technology. Let's use the way back machine to head to about 1980. Early "home computers" often booted and started what is a BASIC interpreter. BASIC was an easy to learn language so it was picked as a way to get home users a system they could program. It stands for Beginners All-purpose Symbolic Instruction Code and you can read all about it on Wikipedia's BASIC page. I disassembled the BASIC compiler on about three of these early systems until I absolutely understand what they were doing. They had a lookup table for converting variable names to memory locations. They had one or more stacks for keeping track of nested loops and parameters so forth. They set aside memory for arrays when you used the DIM keyword. What they did was this: 1) Tokenize the source to internal tokens representing all the BASIC keywords. Some tokenized the whole program and stored it some where in memory, others tokenized on the fly and were pretty slow. Some converted each line as it was entered into the tokenized version and when you typed LIST it de-tokenized it back to readable output (most did this). 2) When variables were used they inserted either an offset from the beginning of variable memory or the absolute address of the variable. 3) Each BASIC keyword had (usually) a small piece of assembly code that performed whatever actions were necessary to do that keyword. Some were straight-forward. When you said PRINT "HELLO WORLD" (my first computer didn't have lowercase), it jumped to some portion of memory that contained the assembly language for the PRINT command (more later) and ran it to output the literal "HELLO WORLD". 4) When you typed RUN, a program written in assembly (the interpreter) loaded the first token from you program, jumped to the proper memory location to perform that action, executed that until it returned, advanced to the next token, and so forth. A lot of work actually happened when you entered a line of code. It had a routine that went like this: wait for a character if that character is the "ENTER" key jump to the parser routine check to make sure the current place in the command buffer isn't  more than a BASIC line could be long (it had a maximum usually about 255 characters) if not put it in the current location in the command buffer add 1 to the current place in the command buffer (moving to next location) jump to 1  When I typed this and hit ENTER on the end: 10 PRINT "HELLO WORLD" the interpreter jumped to the parser and parsed the line, doing something like the following: read a character. check to see if that character is a special character like an operator or the ":" which divided commands. If it is, handle that special character If not consider it part of the current command goto 1 So it would: read "10" and converted that the number 10 ($0A back then $ meant hexidecmal) and put it on a FIFO queue. Parse "PRINT", and compared this one by one to a list of strings in memory that contained the keywords. If it found it, it converted this to a number, let's say 14 ($0e) and pushed this on the queue. The default device is 0 (the screen) so it put this on the queue next. Next it saw the double quote (I didn't have a single quote) and jumped to a routine that would read and store a literal string. This routine often counted the characters and then jumped to a routine that would find room in the memory set aside to hold variables and return this address. Now it copied the characters which were actually their ASCII codes, one by one, from the command buffer to the variable storage location until it hit the next double quote. It output a special character at the end of the string to denote "STOP". Then it pushed the address of this literal onto the queue All the while it was keeping track of how large the tokenized line was. Now, since it stared with a line number it meant it was part of the current program in memory and not something to run right now (called immediate mode). It started reading the tokenized program in memory to see if there was line 10 and if there was checked the length. If it was shorter it copied the queued tokens to the location and moved the rest of the program backwards in memory to close the space. If it was longer then the current line 10 it found room for it in the memory set aside for the program and put it there changing a table in memory that stored the location of each line and then simply replaced the new address with the old address in a table that showed the order of lines by line number and the address of each line in the tokenized code space. Finally it was done and it returned control back to that very first routine. Genreally program space, room for the tokens, grew in one direction (forward in memory) while the program data grew in the opposite (backward in memory). In this manner the same memory can be used to store both the program and the data before you ran out of memory (which I often did). A small program using lots of data was just as bad as a large program using little data. When I typed "RUN" it did the same thing as above but since it didn't start with a line number, it jumped to the execute routine that executed the queue which jumped to the assembly containing the "RUN" command. Here's where it actually ran the program. It went to the table of line numbers and loaded the address of the first line, this contained the tokenized form of the first line. This was a the $0e token of the print command. In memory was an array of all the addresses of the BASIC commands backing assembly routines stored by token number as the index. It jumped to table[$oe] which was the print command. The assembly for the print command, loaded the next token which told it the device number to output. The device output vector routines were in an array with the device number as the index, it loaded this into a specific register. The next token told it there was literal next. The next "token" wasn't actually a token but the address of the "HELLO WORLD" string in the literal table. It started a loop in assembly that loaded the address of the character into a register and jumped to the address of the device output routine stored in that specific register which in this case was character out ("chrout") - an assembly routine that put the ASCII code of the character into the video display buffer at the current location, and advanced the character one position. It kept doing this until it hit the STOP character in which it told the chrout routine to start a new line. Then it jumped back to the very first routine at the top because there were no more tokens in the program storage area. The BASIC program was simply data used by another program to perform the actions that were written in assembly. That base program (the interpreter) had additional data structures it needed like vector tables, tables of strings, variable memory and code to manage it, etc. As you can see the parsing of the language is both reading the code and converting it to data that the RUN command can use to perform the work. A compiler parses code, tokenizes it, and does something similar to the interpreter by converting it to assembly. But instead of being a program that uses data structures to "run" another program, it converts the program to some other form, usually assembly for the native chip and the code necessary to start and initialize a program for the operating system. Lots of time this is an assembly text file (source code) that is assembled using an assembler and finally converted to native machine code for the CPU and operating system. This creates a file on some device. When you  tell the OS you want to run that file, it opens it, loads it in memory, and then executes it. Now at first it may seem that the operating system is a program running a program too, but it's not actually. The code the compiler writes is native to the CPU and it executes just like the operating system itself. Since the operating system controls the hardware, it may stop it your program after a bit, save all the registers, and start another program by loading it's registers, but your program has direct control of the CPU for a bit. That's a compiler. Or so it would seem. Now, for a long time, we've had what a called byte code languages such as Java, Perl, Inferno and  many others. These languages are a hybrid between a compiler and an interpreter. In this case, your Java is tokenized and compiled into byte codes much like the BASIC program above but instead it writes tokens that are like assembly but for another program (called a virtual machine) that reads an executes those tokens like they were assembly language instructions to some virtual hardware. You can see all those for Java at Chapter 6. The Java Virtual Machine Instruction Set. So this is a program, that like an interpreter, runs data as code but does so in a much more efficient method than an interpreter and the language is designed to be compiled while many interpreted languages aren't very easy to compile. Compiling these byte codes to native assembly is often much easier. What's nice about this is, the same file that is compiled on a system running on top of Linux on an ARM and run on a system running Windows on an x86 processor since the virtual machine does all the functionality of the virtual hardware on the real hardware. So this is still a program running a program which is an interpreter. Or so it would seem. A lot of byte codes languages are compiled at run time down to native assembly before they are executed. This is known as just-in-time compiling or JIT. In this case that file of byte codes for a given virtual machine are read and turned into native assembly for the current chip and operating system and ran as native code for the CPU. This is a compiler. If you made it this far in this long answer you must be interested. Here's some links: Comparison of application virtualization software Just-in-time compilation BASIC What to try some BASIC?  Many many many people learned to program this way QuiteBasic - fun, learning and nostalgia Updated 60w ago • View Upvotes