What is a tensor network?
A tensor network is a "network" of tensors. In less cheeky terms, it is a graph whose vertices represent tensors and edges represent the indices of the tensor. Recall that the "rank" of a tensor is sort of like its dimensionality, then rank-0 tensors are scalar numbers, rank-1 tensors are vectors, rank-2 tensors are matrices, and so on. In pictures: Then, you can connect some of these guys together to represent some simple operations. For example, (a) represents a matrix product, and (c) represents a vector inner product. Edges between vertices (tensors) denote a sum. Notice the "open indices" in (a), the edges that don't connect to anything. These represent indices that are preserved, or excluded from the sum. To make sense of that idea, let's write (a) in formulaic terms: [math]C_{ik} = \sum_{j=1}^D A_{ij} B_{jk}[/math] where [math]D[/math] denotes the number of possible values that the index [math]j[/math] can take, or if said in imprecise terms the number of elements along the [math]j[/math]th dimension of these tensors. On the other hand, we would represent (c) like [math]C = \sum_{i}^D A_i B_i[/math] which is just a number, instead of a matrix element like the previous formula. Now it is clear that if you have a tensor product space (a vector space built up by taking tensor products of smaller spaces), it can get big really fast. For example, the dimensionality of a space can go as [math]2^k[/math] if you take the tensor products of [math]k[/math] two-by-two matrices. The point is that maybe you don't need all of that space, and the operations you want to do on the space can be done much more efficiently if you could just operate on the pieces instead. In the example of [math]k[/math] 2x2 matrices, your memory requirements are essentially [math]\mathcal{O}(k)[/math], whereas with the big tensor product of all of them you're dealing with [math]\mathcal{O}(2^k)[/math], an exponential increase in memory requirements!! Clearly it isn't always possible to do this sort of divide & conquer strategy, and sometimes you will need to directly operate on a large vector space. The idea is that you may be able to get away with doing it on a set of smaller tensors. The arguments for why you can do this depend on the application. For quantum systems it is pretty nice because the fact that most physical interactions are local in nature puts restrictions on the entanglement between two randomly chosen subsystems in Hilbert space. In fact, if you focus attention on the low-energy dynamics, you can prove that low-energy states are constrained to be in a particular submanifold of the exponentially large Hilbert space. In plainer terms, you have a huge space of possibilities but because of physics we know that we can ignore everything but a small piece of it. If you are working in machine learning, you might imagine that you can ignore the correlations between two different variables if you have a strong argument for doing so, and in fact this in the same spirit as most dimensionality-reduction schemes like PCA and such, except that the tensor network allows you to represent an arbitrary decomposition, instead of just the eigen or singular value ones. All the figures are from this excellent tutorial paper, which has an excellent introduction that I think would be accessible even to non-physicists: A Practical Introduction to Tensor Networks: Matrix Product States and Projected Entangled Pair States. 