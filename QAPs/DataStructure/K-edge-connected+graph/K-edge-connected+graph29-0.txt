What is an intuitive explanation of spectral clustering in the context of machine learning?
It's not really easy to provide an intuitive explanation of spectral clustering but I accept the challenge, I sincerely hope to find answers better than mine. Clustering in the context of machine learning is an unsupervised problem where you have to split "m" observations in "k" clusters such as: - Points in the same cluster are very similar. - Points in different clusters are very different. Spectral clustering is a clustering algorithm so we might say we have already explained what it is in the context of machine learning however the answer will seem incomplete. So I need to find an intuitive explanation of spectral clustering. For any dataset you can view the data as a point cloud, say "m" points in "n" dimensions. From those points you can create a graph where the points are the nodes and the edges are weighted with the similarity between points. A common way to define similarity is with a Gaussian: For example this is a graph of the famous MNIST dataset. Each color represents a digit. Once we have data as a graph we can create an adjacency matrix where we just put in each cell of the matrix the weight of the edge between nodes "i" and "j". This is a symmetric mxm matrix. Let's call the adjacency matrix W. We can also create a "degree" diagonal matrix that will have in each Aii element the sum of the weights of all edges linked to node "i". Let's call this matrix "G" The Laplacian of the graph can be computed as L=G-W. Notice all rows and columns will sum zero because of the way we defined W and G. There are different formulas for L, this is not the only one. Now that we have the Laplacian of the graph we can compute the spectrum of this matrix, in other words, it's eigenvectors sorted from the most important one to the least important one. If you take the "k" least significative eigenvectors you get a representation in "k" dimensions of each node in the graph which is also a representation of each point in the dataset.Â  The least significative eigenvectors are the ones associated to the smallest eigenvalues. This is a form of non-linear dimensionality reduction. Each eigenvector provides information about the connectivity of the graph. The idea of spectral clustering is to cluster the points using these "k" eigenvectors as features. So you take then "k" least significative eigenvectors and you have your m points in k dimensions. You run a clustering algorithm like K-means and then you have your result. This "k" is not the number of clusters is another parameter that can usually be determined from the plot of eigenvalues. In this plot we can see that the graph is connected, there's only one eigenvalue with zero value so there's only one connected component. From this graph we might decide to use 4 ,5 or 6 eigenvectors. Let's take a look at the eigenvectors As you can see each eigenvector could be used for clustering, in each eigenvector plot some points have similar values meaning they would to to the same cluster. Each eigenvector contributes information about our graph on different levels, if we use 4, 5 or 6 eigenvectors we are using all those levels together to cluster our datapoints. A very intuitive explanation for this is: frequently the least significative eigenvector is good for clustering, but sometimes the 2nd or 3rd eigenvector is the right one, maybe the 4th. If we use them all together then we make sure to get the best clustering result without needing to guess which eigenvector(s) to use. Spectral clustering is deeply related to kernel K-means using a Gaussian kernel so you can also think about it as a clustering method where your points are projected into a space of infinite dimensions, clustered there and then you use those results as the results of clustering your points. I think it can be proved that kernel K-means and spectral clustering are they same algorithm but I can't remember that now. Spectral clustering is used when K-means works badly because the clusters are not linearly separable in their original space. Other methods such as hierarchical clustering or density based methods can also solve this problem. I hope this helps, I tried to be as intuitive as possible while keeping my answer complete, it wasn't easy but I enjoyed writing this. Luis. 