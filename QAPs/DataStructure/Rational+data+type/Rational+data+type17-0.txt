How does mathematics play a big role in programming?
The math you learn up through high school - primarily arithmetic and algebra - is extremely important in computer programming because it is extremely important in practically any industry that uses computer programs (which, nowadays, is all of them). Computers were initially designed to perform exactly this kind of simple math, allowing highly-paid physicists and engineers of the Space Age to spend their time on theory and implementation instead of the "scut work" of calculation. The basics that you'll use every day in virtually any programming job generally lie in the field of "discrete maths". Discrete mathematics is distinct from "continuous mathematics" in that the discipline focuses on mathematical problems that are not or cannot be represented in terms of a continuous mathematical function; they are instead presented in "set" of discrete numeric data. There are many overlaps between continuous and discrete mathematics, but many branches lie wholly within one or the other. Things you'll see a lot of just about anywhere you go include: * Arithmetic: natch. If you don't at least know what the four primary arithmetic operators do, you have no business trying to program. Arithmetic is the foundation of all basic programming, and may well be all you ever need. * Boolean logic: these are the basic principles behind the operation of the modern electronic computer; all binary computation ultimately boils down to a combination of three operators (NOT, AND, OR) that take one or two input bits and produce one result bit. * Set theory: You may not ever get a formal theoretical background in this branch of mathematics when learning to program, but I guarantee you that modern computer programming theory is built on the mathematical concept of a "set", a collection of distinct objects which can be considered a distinct object in itself. * Integer and modular arithmetic: The limitations in the way a computer stores and represents data, in turn fundamentally limited by the computer's finite discrete memory representation of numbers, means that ultimately all mathematical calculations are performed on integers, with two integer numbers representing a rational in different ways (one integer is the "significand", and the other is either the position of the decimal place in that significand in "fixed-point" notation, or the power of two to which the significand should be raised to arrive at the actual number in "floating point" notation). It is therefore fundamental to programming to understand the inherent limitations of performing such calculations in a computer. In addition, these limitations can be exploited and even artificially defined in several ways that have interest to cryptography and other sub-fields of computer science. * Graph theory: A graph is a set of interrelated nodes. Nodes have information about themselves, plus information about "connections" to other nodes (called "edges"). If you've taken formal computer science courses, this should sound very familiar as the basis for linked lists, stacks, queues, trees, heaps and other data storage mechanisms. It also ends up a convenient way to represent networks, from the social kind to the data kind to the human transportation kind. There are several good algorithms for optimizing graphs and the traversal thereof, which have application in everything from the structure and design of the Internet to Google Maps and your GPS. * Discrete calculus: This is, in a broad sense, the study of the application of discrete algorithms to solve problems normally requiring continuous math formulae. It is time-consuming and ultimately self-defeating to try to program a computer to follow all the rules for differentiation and integration of continuous functions. But, we don't have to, because those formulae were derived in part to simplify the amount of simple arithmetic needed to find an area bounded by the curve. Well, simple arithmetic is what a computer was built to do, so we can simply set it to calculate the discrete value of the function at X values separated by some arbitrarily small delta, then calculate the Riemann sum, Gaussian quadrature or similar approximation, and still come up with an answer accurate to within the bounds of the data types the computer uses faster than a human could find the exact answer. These types of calculations, in turn, are pretty much the only way to model things that the continuous functions represent, like a cannonball of mass m flying through the air after being given an initial impulse of force F over time ti at inclination angle a with gravity Fg and drag coefficient Cd. Determining things like the impact velocity of the projectile, the point of impact along the ground, maximum height of travel etc are all complex differential equation problems due to the calculation of drag (which is co-dependent on velocity and thus on time), that can be solved with a practical level of accuracy by just plugging in the variables and having the computer calculate the instantaneous position, velocity vectors and acceleration vectors at arbitrarily short time intervals. That same method is pretty much the only way to model these interactions in real time for things like games. 