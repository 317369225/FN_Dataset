What are some things that mathematicians know, but most people don't?
Strap in, folks. This will be a long answer, with links to many other, long answers. I am going to restrict myself to results and concepts that are reasonably easy to explain (although not necessarily easy to prove). I am also going to try to split by general categories, and I will generally steer away from graduate and post-graduate level mathematics. Even with these massive restrictions, this is not meant to be an exhaustive list. In fact, I will start with exactly that. General Knowledge Mathematics is vast. I think that most people are dimly aware that there is some stuff beyond calculus, but as near as I can tell, very few understand exactly how much. Mathematics has had an enormous explosion in the last two hundred years, to the point that it is no longer humanly possible for a person to be well-acquainted with every field and sub-field of mathematics. Mathematics is not about numbers. I have written before about how the term 'numbers' has actually fallen out favor (Can new kinds of number be introduced in Mathematics?) because it is non descriptive. There are many other types of algebraic structures than the ones that most are familiar with, and the distinction between which are numbers and which aren't is very arbitrary. Mathematics is not about solving equations. Equations also often appear in mathematical work, and there are many interesting results and conjectures about solving various kinds of equations. There are also a lot of very interesting results about when it is impossible to write an algorithm that will automatically solve (some class of) equations, or when some system of equations does not have a solution that can be expressed in terms of elementary functions. Even so, equations are tools, and far from the only ones. Much of topology is written without a single equation in sight (without some liberal interpretation of what an equation means). Mathematics is about the study of structure. Mathematics: What is mathematics? gives a good overview of various mathematicians' opinions of what mathematics actually is about. Objects in mathematics are described not by what they are, but by what they do. This is a major shift in thinking for many people. I would describe it further, but I think I have already given a decent summary here: What are the things that only Ph.D mathematicians know? Mathematics is not constrained by the laws of physics, only by the laws of logic. This is perhaps going to be a slightly controversial assertion, so let me clear about what I mean. I mean that if we discover that space is curved, that does not make Euclidean geometry false as a mathematical theory. It is still as true as when Euclid wrote the Elements---it is just that we have discovered that it is not a good model for space-time. Modern mathematicians do not worry too much about whether there will be a physical model for any mathematical structure that they create. This has lead to significantly more freedom, ability to draw unexpected connections, and productivity. Mathematics is useful. This is very surprising, considering the previous point. However, when you are constructing something in mathematics, you never know exactly how it will connect with everything else, and as a result you never really know where/how/if it is going to be useful. The famous example of number theory turning out to be a valuable tool in cryptography has been mentioned in another answer. Mathematics is fundamentally creative. Hilbert, one of the most famous mathematicians of the last century, once remarked about one of his former students that the young man had not been creative enough to be a mathematician, but now that he had become a poet, he was fine. Tongue-in-cheek though this remark may have been, it has a kernel of truth. Because mathematics is constrained only by the laws of logic, there are very surprising and deep connections that one can find, if one is creative enough to see it. Like it or not, all of us have internal mental models of what various fields of mathematics are used for and what they are about. Seeing that one field of mathematics has something to say about another field is not a trivial exercise. Consider Descartes realization that there was a connection between equations and geometry, leading to the Cartesian plane that we now take for granted. Consider the discovery, which can be traced back to Galois, that there is a connection between linear algebra (which concerns itself with systems of equations, matrices, etc.), solutions to polynomial equations (why is there a quadratic, cubic, and quartic formula, but no quintic formula?), and the constructibility of certain objects in geometry (why can't you trisect an angle with a straightedge and protractor?). Were these not creative acts? Numbers and Number Systems A number is not its representation. A lot of people get confused when they see something like [math] 0.999\ldots = 1 [/math], because here are two different things that they are told are the same. People also sometimes ask questions that are odd to a mathematician, such as whether prime numbers are different in base 2 or base 12. Let me clear: there are many different ways to represent a number. That goes for integers, rational numbers, real numbers, and whatever else you might like to call a number (I have already complained about the ambiguity of this word). In fact, often times, there are many different ways to represent anything. The representation doesn't matter---see point #5 above. Unfortunately, this is understandably confusing, because most people don't learn what real numbers really are! The real numbers are weird. To start with, they are way, way larger than you probably realize (look under the section about set theory for more on this). The way that they are defined is a bit funky---it is a process of 'filling in holes.' You start with the rationals. The rational have a bit of a problem: it is possible for a sequence of rational numbers [math] a_n [/math] to get closer and closer to something (started more accurately, the terms [math] a_n [/math] get closer and closer to each other), but that something is not aÂ  rational number. For example, if [math] F_n [/math] is the [math] n [/math]-th Fibonacci number, then [math] \frac{F_{n+1}}{F_n} [/math] is a rational number, and you can prove that this sequence of rational numbers is closer and closer together as [math] n [/math] becomes very large, but the limit is not a rational number. It is, in fact, the golden ratio, [math] \phi = \frac{1 + \sqrt{5}}{2} [/math]. I have written about the special properties of this number before. To fix this problem of the rationals, you add on elements to them for every sequence like this, thereby 'filling in the holes', or (as is the actual mathematical term) taking the completion of the rational numbers. You might spot the problem that you will have different sequences that should correspond to the same real number (i.e. different sequences of the rationals that should have the same limit). There is a formal way of fixing this, but I will take an informal route (that you could make rigorous): two real numbers [math] x [/math] and [math] y [/math] are equal if [math] |x - y| < \frac{1}{n} [/math] for any positive integer [math] n [/math]. This, incidentally, proves that [math] 0.999\ldots = 1 [/math]. The real numbers are very weird. Most numbers that you deal with in practice are computable: that is, there is some algorithm that exists that will give you this number to whatever precision you want (e.g. you can calculate as many terms of the decimal expansion as you want). [math] \pi [/math], [math] \sqrt{2} [/math], and [math] 1/3 [/math] are all computable. However, the major bulk of all real numbers are not computable! They exist, but you can't write them down explicitly. The complex numbers are not very weird. I personally think that incomputable numbers are much more deserving of the name 'imaginary' than little old [math] i [/math]. Whereas the shift from rational numbers to real numbers added on a lot (and I do mean a lot) of new elements, the shift from real numbers to complex numbers is much, much more straightforward. The process of tacking on the root of some polynomial equation is called an algebraic extension, and it happens frequently in mathematics. There are many ways to understand it. For the complex numbers, this is well covered by this Quora post: Are complex numbers truly indispensable? The complex numbers are enormously useful. There are probably better arguments about this assertion, but I know that I have written about it before on this site here. Neither [math] \sqrt{2} [/math] nor [math] \pi [/math] are particularly weird. For whatever reason, the fact that [math] \sqrt{2} [/math] and [math] \pi [/math] do not have a finite or looping decimal expansion freaks some people out. However, they are both computable numbers, and both of them have some very nice representations. For example [math] \pi = \sqrt{6 \sum_{n = 1}^\infty \frac{1}{n^2}} [/math], [math] \sqrt{2} = 1 + \cfrac{1}{2 + \cfrac{1}{2 + \ddots}} [/math]. Adjoining [math] \sqrt{2} [/math] to the rational numbers in particular is nice, because it is an algebraic extension. Set Theory and Infinity Infinity is not a well-defined concept. To be a little more accurate, there are many different notions that a mathematician might have in mind when they are talking about 'infinity', and they are all quite different. Laypeople often run into problems because they start treating infinity like a number, and conflate different concepts of what infinity is, naturally leading to confusion and erroneous results. The notion of 'infinity' that I will deal with in this section is the idea of 'infinite sets'. Infinite sets come in many different sizes. To be more precise, there is a notion of 'size' that mathematicians attach to infinite sets, which is called cardinality. Two sets are said to have the same cardinality if you can match up the elements of the two sets---for example, you don't need to count the number of legs on a millipede to know that there are as many legs on the right side as there are on the left, because you can match them up directly. Amazingly, the natural numbers, the even numbers, and the rationals can all be matched up like this, and so they have the same cardinality (we say that they are countably infinite). The real numbers, however, have another, larger cardinality. The even numbers have the same cardinality as the integers. I know that technically I already mentioned that in the previous point, but I wanted to reiterate this simply because it is so counter-intuitive for most people seeing the theory for the first time. For a more in-depth discussion about the intuition of this, you might look here: What is an intuitive explanation for why there are an equal number of integers and even numbers? There is no 'largest' set. You can keep building sets of larger and larger cardinality. In some sense, this fundamental problem is what lead to Russell's paradox and forced logicians to reexamine the foundations of mathematics. Functions and Real Analysis A function is not a 'rule'. I have found that many people have a misconception that a function is some sort of rule like [math] f(x) = x^2 + 3 [/math] or maybe [math] f(x) = e^{-x^2} [/math]. That isn't entirely wrong, but it would be more correct to think of a function as a way of matching up inputs and outputs. That is, you have some sort of black box. You give this black box an input, and it will give you back an output---the only real restriction is that given a particular input, the output must always be the same. It can't be 'blue' when it feels like it, but sometimes 'red', and if it is having a really off day, it will return '5.' The set of inputs that the black box will take is the domain. The set of outputs is the codomain. That's it. That is all a function is. For instance, the example I give here is completely kosher. That said, henceforward, I will be assuming my functions to be from the real numbers to the real numbers. Most functions are nowhere continuous. Of course, this depends on how you choose to define 'most', but pretty much any reasonable definition will give you this result. Here is an explicit example of a function like this: [math] 1_\mathbb{Q}(x) [/math] is 1 if [math] x [/math] is rational, but 0 if [math] x [/math] is irrational. There exists a function that is continuous at only one point. [math] f(x) = x 1_\mathbb{Q}(x) [/math] will do this. There exists a function that is smooth at only one point. There exist functions that are smooth but are not analytic (i.e. do not converge to their Taylor expansions). I wrote pretty extensively about this here. There exist functions that are everywhere smooth and nowhere analytic. I gave an example here. In short, Real-valued functions are much more poorly behaved than they have any right being. Complex Analysis Complex-valued functions are much better behaved than they have any right being. Specifically, I mean functions that have complex derivatives. The amazing thing is that if a complex function is once differentiable, then it is twice, thrice... differentiable. In fact, it will be analytic! We call such functions holomorphic. Holomorphic functions are just about the greatest thing ever. As an analytic number theorist, I am probably biased, but it never ceases to amaze me how structured holomorphic functions are. A holomorphic function can be reconstructed completely (or at least, up to a constant) from pitifully small pieces of information---sometimes as little as knowing how quickly it grows and where it is zero. While I have already included this link, it bears repeating that holomorphic functions are tremendously useful: Do imaginary numbers actually exist? Miscellaneous It isn't correct to say the fourth dimension. You should say a fourth dimension. This is something of a pet peeve of mine. I wrote about it here: What is the 5th dimension? Are there any other dimensions beyond the 5th? Let me expand on this slightly to point out that just saying 'dimension' has many of the same difficulties as saying 'infinity.' There are different definitions of 'dimension' depending on context. There is the dimension of a vector space over a field (which is usually the real or complex numbers, but could be many other things). There is the dimension of a manifold (a connected notion, but nevertheless different). There is the Krull dimension. There is the Hausdorff dimension. You name it, we got it. For now, I think I will wrap up my answer here. If I get any bright ideas, or there are specific suggestions/requests, I might add a few things further. 