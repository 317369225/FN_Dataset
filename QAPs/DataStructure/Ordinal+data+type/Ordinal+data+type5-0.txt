Why is the generative/discriminative distinction important in machine learning?
In the most basic way, generative and discriminative learning is the difference in between learning from the distribution of some data set or classes and learning from the categorical differences or boundaries between data sets or classes. A good mnemonic here would be to remember that discrimination divides, and generation condenses. Discriminative learning looks for some explicit or implicit boundary or classification which distinguishes data sets or classes. The usual example here is the generation of data trees, which generate and/or are generated by recursive partitioning. The following image is borrowed from Wikipedia. See: Tree (data structure)                          This data tree is un-ordered, but is hierarchical with definite (step-wise) boundaries, from some parent or kernel and using some set of constraints to generate those boundaries (like the maximizing of explanatory power for the set's contents). Using discriminative methods for learning allows classification, and from that classification stems a way to understand aggregates (like several sets of data or sub-sets created in a data set) as a collection of classifications. Future classifications are possible as a result of learning current classifications, for similar types of data. edited to add: An example of a statistical technique for discriminative analysis would be linear discriminant analysis. LDA uses continuous independent variables to analyze a categorical dependent variable--the independent variables are all treated as objects in the same set, and they are analyzed to see whether or not they belong in the category for which they are being analyzed. Information capture for that category can then be estimated by the proportion of the data set for which the category is able to fit and by the estimated difference between subcategory means. The idea is to maximize both the descriptive power and the discriminatory power for the set being examined. (See: Linear discriminant analysis) example: Suppose you are trying to figure out who, in a group of people traveling, is likely to come down with traveler's sickness. The outcome is y/n, the set is the group of travelers. The predictive variables may be things like: does not wash hands after being in the bathroom, does not drink bottled water, does not wash produce before eating, etc. LDA constructs multiple equations (D) using linear combinations of the predictor variables (X) to examine whether or not a traveler will get sick, looking for as much difference between the groups sick/not sick (p) as possible, in the group of travelers. The equations take the basic form: [math] D_i = b_0 + \sum_{k=1}^p b_k X_k [/math] Let's assume that we find our predictive variables are generally good at dividing people who end up sick while traveling and people who don't. We've learned how to predict who, in a group of travelers, is likely to get sick (and what predictor variables are good for predicting that). What we learn from this can be used to classify incoming data with unknown classes, using samples which are analogous and have known classes. Or, more specifically, we can look at other groups of travelers and engage in predicting their likelihood of getting sick, based on those behaviors. Generative learning is closer to general statistics, in which some set of distribution is created by an examination of the data set as a whole. It's more or less a study of general relations in the data set. A classic example could be regression statistics, and methods like least squares. The following image is borrowed from Wikipedia. See: Least squares As you'll notice, the line of fit attempts to be as close as possible to general trends in this set. The data points in the set are considered relative to some relation between the variables represented by the axes, and treated as comparable without the need for generating distinction--the data is treated as of analogous* type for the purposes of seeing what is generally true of the data. The focus of analysis is on trends, and what is learned is (depending on method) the coefficient which best describes that trend. edited to add: Since the Naive Bayes has been mentioned, I'll do a different method: probabilistic, context-free grammars. PCFG attempt to deal with the problem of modeling natural languages through the use of controlled rules of production for sentences and constructs in the language. These rules allow probabilities to be assigned to productions or structures, predicting their occurrence. Well-designed PCFG have to be fairly generalizable and have to deal well with scalability, accuracy and ambiguity. (See: Stochastic context-free grammar) example: Let us suppose that you, for one reason or another, were monitoring chat logs on a gaming server (I'm sorry in advance; I play TF2), and you wanted to understand the slang being used on the server as an emerging subset of some natural language, in this case English. You would observe these chat logs for some time, looking for the apparent rules for production. Let's say you noticed the word 'dongers' was frequent and decided to use it as an example. The rules you came up with were the following: 1. dongers --> raise dongers | praise dongers | hard dongers | e You could use these rules to produce the following statements: 1. raise dongers 2. raise praise dongers 3. raise hard dongers 4. praise hard dongers.... Once you've produced statements, you can assign probabilities to them. Simply based on observation, I'm calling "raise dongers" the most likely rule for production of statements using the word "dongers." You can then observe chat logs from other nights and test that supposition. If it turns out that this is correct, what I've found is the consensus structure for the production of statements involving that word. And now, for no useful purpose I can think of, I can predict the form of use for that word in future conversations between gamers. I have learned the rules of its production. Some methods (like Bayesean methods) use trend data as a cumulative way to update analyses on types of data. Frequentist methods, like regression, compare trend data across (usually) time instead of updating directly, in order to observe changes. Both use data of the same type and/or drawn on the same populations to update/compare. edited to add: It's been a hell of a day, and I'm going to add an actual answer to the question below. Sorry, I've been distracted. Discriminatory learning allows computer scientists to divide complex data into useful, less complex structures, and allows the more rapid processing of large amounts of incoming data by dividing it into types. Generative learning allows computer scientists to understand trends in complex data, allowing principles and relations to be inductively produced from that data. edited to add again: Today was also a hell of a day, and my edit will be late as a result. _____________________________________________ * Obviously, depending on the method, this may not actually be true. You may, for instance, be considering a relation on nominal and ordinal data. Updated 84w ago • View Upvotes • Asked to answer by Kao Zi Chong