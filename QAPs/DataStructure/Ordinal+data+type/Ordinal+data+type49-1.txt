What is permutation entropy?
Basically it's merely "entropy" as defined simply by that name.  It tends to be specifically associated with time series in the context of complexity theory but strictly even that is called "entropy" in electrical engineering and communications engineering. H = − ∑(pj log2 pj) (j = 1, …, n) Entropy is probably clearest when talking about finite fields in the context of probability (e.g. dice, coins, wheels, digital states in an data stream, etc.).  If you have a set of possible states but only one state is present in a time series, this is low entropy because it's very ordered and predictable.  Similarly if you alternate between a few states of the total possible states in a predicable way in the time series, this will have more entropy than a single state case but it will still have a predictability to it.  If you permute through all possible states but do so in, say, a repeated sequential order, you have more entropy but not maximum entropy.  If the states are completely random and the probabilities of every state are equal (which also means the appearance in the time series of any other state is equally likely), you have maximum entropy. You can also apply this to non-finite states (continuous analog state).  The same basic reasoning applies but instead you have a stochastic probability function, which may or may not be discrete in time. The idea is this can be a measure of "randomness" and say something about dynamical systems that are "in orbit" or "chaotic".    The key difference is this captures something about the "generation function" of state to state transitions in addition to the static probability distribution. Just Google the term - there are tons of links including videos about it. 