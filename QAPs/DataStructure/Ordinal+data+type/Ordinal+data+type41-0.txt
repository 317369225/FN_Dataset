How do I draw an ROC curve?
Receiver Operating Characteristics (ROC) curves are a complete way of reporting performance of a test or a classification technique. These work on binary (2-group) classification problems, such as spam detection. Many times, a non-binary situation can be modified to look at the problem in a binary format. The image below depicts a typical binary situation - where any data you receive, will fall into one of two distributions. The two distributions are the two bell curves below. To continue the spam classification example, let's consider the curve on the left to be emails that are "not spam", and the curve on the right to be emails that are "spam". Left hand curve: "not spam", Right hand curve: "spam", Vertical line: "Threshold" TP: True Positive, FP: False Positive, FN: False Negative, TN: True Negative Sensitivity = TPF = P(TP) = TP/(TP+FN) 1 - Specificity = FPF = P(FP) = FP/(FP+TN) (Image Source: Receiver operating characteristic) The vertical line passing through the curves represents the threshold used by the classifier to define how to classify any data point. Thus data falling to the left of the line would be classified as "not spam", and data falling to the right would be classified as "spam". Because these curves are overlapping, you will see some data that are wrongly classified - both false positives (non spam email wrongly detected as spam) and false negatives (spam emails wrongly detected as non spam) exist. You will note that this decision of "spam" or "not spam" depends only on that vertical line, or Threshold. Thus, depending on where that Threshold line is fixed by us, we would get different values for TP, FP, FN, TN. Deriving True Positive Fraction (TPF) and False Positive Fraction (FPF) across different thresholds, we can plot an ROC curve as shown in the image above. (These are just empirical curves. There are more sophisticated ways of getting ROC information by assuming binormal models - like semi-parametric ROCs - but I am not covering those here) How does this work in a practical classifier? A practical classifier would have some information about a data point, based on which features would be derived, and an outcome variable metric/estimate/probability of some sort computed. The outcome would be numeric - continuous, or at least ordinal in nature (like a count of something). The regression problem is then converted to a classification problem by applying a decision threshold on the outcome variable, which places that particular data point in one class or another. That decision threshold that is applied on the metric/estimate/probability, is the same as the vertical line that helps us separate out our overlapping distributions in the image above. A crude example in the case of spam detection: Say you are using a single feature such as "Percent of Capital letters in the email", such that if the percent of Capital letters is above a certain value, you call it "spam", otherwise it is "not spam". Here, the threshold would be varied across certain values (0%-100%) in steps, the emails classified each time, and the sensitivity and specificity calculated. For each step, you would get a single point on the ROC curve with the coordinates (1-Specificity, Sensitivity). In reality, you would have a multivariate model, with many other features giving you some kind of probability value. I'm not a spam detection expert, I use ROCs in medical applications, so some basic features I can think of would include things like - presence of certain flagged words like profanities, email sender not on contact list, suspicious sender name, etc. In a real world spam classifier, you would be applying some sort of transforms on some of the features, would be using prior data, sophisticated modeling techniques, etc. Key takeaways about ROC curves: Gold standard or ground reality should be established (or assumed) in order to be able to make ROC curves Problem needs to be cast as a binary classification problem Area Under the Curve (AUC) is the area between the curve and the x axis (FPF). It summarizes a particular ROC curve, and is used as a metric to compare various tests or classifiers. In this plot, we come to the unsurprising conclusion based on the AUCs of the two curves that the classifier performs better on the training data than on the testing data. (Image Source: http://www.alanfielding.co.uk/mu...) AUC can be interpreted as being an indicator of specificity averaged across all sensitivities, or sensitivity averaged across all sensitivities. Another important way to interpret AUC is - the AUC is the probability that the test in question will produce a higher value for a randomly selected data point in the positive distribution (spam) than for a randomly selected data point in the negative distribution (not spam). If the ROC completely covers up the entire TPF and FPF axes, AUC = 1, and congratulations, you have a perfect classifier! Chance line, the line connecting (0,0) and (1,1), represents unbiased guesswork, or classification made by a coin toss. If your classifier's ROC happens to lie on this line, AUC = 0.5, and congratulations, your classifier is no better than a coin toss! If your ROC lies below the chance line, AUC < 0.5, congratulations, you have your positive and negative cases mixed up! Just revert your classes and you'll be fine (nah, it's not that simple, figure out the problem in your understanding of your data). (Image Source: http://0agr.ru/wiki/index.php/RO...) Sometimes you will care only about a partial area of your curve. For example, in screening for breast cancer, you will want to maximise sensitivity, so that all possible cases are picked up. In spam detection, you might want to maximise specificity, so that no useful email gets thrown into junk. Thus, ROC curves are useful in reporting the performance of a test or binary classifier. 