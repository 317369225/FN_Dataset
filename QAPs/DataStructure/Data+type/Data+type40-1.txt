When do programmers specify data types?
The difference between them is the amount of memory that they take up. Bytes, for instance, only take 1 byte of data, so they can only store numbers between -127 and 128 inclusive. The next smallest is short, then int, then long. The reason they exist is so that you don't use more RAM then you need, but modern computers have enough ram that individual variables don't take up any worrisome amount of space. For most cases, use int, since most operations use int and anything less is probably gonna be cast up to an int anyway. If you need bigger numbers, like to store the number of milliseconds... (more)