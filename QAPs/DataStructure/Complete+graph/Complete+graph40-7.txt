What are some of the most ridiculous proofs in mathematics?Especially for easier concepts.
Claim: Let H n =∑ n k=1 1k   be the n th   Harmonic number.  Then: H n =∑ n k=1 (−1) k+1 (nk)k   Note that this claim implies the somewhat surprising result that for any n∈N  : ∑ n k=1 1+(−1) k (nk)k =0  Proof: Here's the outline for a ridiculous, but interesting proof.  (A  more direct proof is given in Sridhar Ramesh's answer.) Step 1:  Show that when you repeatedly roll an n  -sided fair die, the expected number of rolls until all n  faces have appeared exactly once is given by E(N)=n∑ n k=1 1k =nH n   .  This step is a fairly standard exercise in probability theory and follows from the linearity of expectation. Step 2: We now solve the same problem for an unfair die (still iid rolls) with the probability of rolling the number j  given by p j   .  This problem is quite a bit harder.  The approach is to assume that the die is rolled at random times according to a Poisson process with rate one.  It follows from some important properties of Poisson processes that the occurrences of a type j  roll will then occur according to a Poisson process with rate p j   for all j=1,2,…,n  , and that all n  of these Poisson processes are independent.  We define X j   as the time of the first roll that results in j  .  The X j   are independent exponential random variables with rate p j   .  The independence allows us to find the expected value of X=max j=1,2,…,n X j   . E(X)=∫ ∞ 0 (1−∏ n j=1 (1−e −p j t )) dt  (Note: At first glance, this seems like black magic.  After all, the roll on which you get your first one is not independent of the roll on which you get your first two - for example.  But by the dark power of the Poisson process, it turns out that the TIME that you get your first one is independent of the TIME that you get your first two.  That's the reason for the completely non-intuitive step of deciding to assume that the time for each roll came from a Poisson process.  I can prove every part of these results, and I somehow still find them hard to believe!)  Step 3:  A simple argument based on conditional expectation allows us to conclude that E(X)  is not only the expected TIME that it takes to roll all j  different faces at least once, but it is also the expected number of rolls for this to occur, E(N)  .  This part of the result isn't really contrary to intuition.  Since the rolls come at an average rate of 1 roll per unit time then it isn't shocking that the numerical value of the expected time is (about) the same as the numerical value of the expected number of rolls.  For example, if the average time were 10 minutes and if we roll an average of 1 roll per minute, we'd expect an average number of rolls of (about) 10. Step 4:  We apply the result for the general "unfair" die to the special "fair" case in which p j =1n   for all j  .  This answer must agree with the answer in step 1. E(N)=∫ ∞ 0 (1−∏ n j=1 (1−e −tn  )) dt  E(N)=∫ ∞ 0 (1−(1−e −tn  ) n ) dt  Using the Binomial theorem (and cancelling out the ones) gives: E(N)=∫ ∞ 0 ∑ n k=1 (nk)(−1) k+1 e −ktn   dt  Interchanging the integral and the sum (which is always justified since the number of terms in the sum is finite) gives: E(N)=n∑ n k=1 (nk)(−1) k+1 k   Equating this result with the one in step 1 gives: H n =∑ n k=1 (nk)(−1) k+1 k   