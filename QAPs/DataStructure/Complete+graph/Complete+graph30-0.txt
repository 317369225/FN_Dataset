How do programming contest problem setters make test cases ?
My test data generation experience comes from my stints as coach for India's Informatics Olympiad training camp (2008,2009,2012) and as coordinator for IIT Kanpur's International Online Programming Contest (2011,2012). The first thing to understand about generating test data is that most of the problems (except some kinds like brute force backtracking, DP over subsets problems) is that the constraints involved are really huge. There is no way someone would be able to generate a graph with 3000 vertices and 20000 edges by hand. So, almost always, test data is generated by a generator program. Now, since you want the generator program to create different sets of test data (to ensure that someone doesn't get lucky on a single case), you use some kind of pseudorandomness in the generator. What the problem setter needs to make sure is that they use the right randomness for the test data. Here is an example: Consider some problem involving strings. If the test data generator chooses each character randomly, there is very low probability that there are going to be long substrings of identical characters. This could result in some kinds of suboptimal solutions to pass. We can prevent this from happening by making the test data to consist of substrings of identical random characters. The tester should be able to catch every possible error. There should be test data which pushes the code to the absolute limits, such as having complete graphs/line graphs in some graph problems and having huge prime numbers in some number theory problems. Also, there are going to be a lot of corner cases, finding which might not be so computationally expensive but a lot of participants could miss them. The tester needs to understand that he is an adversary, and think up all such corner cases and create hand-generated or program-generated test cases to catch such corner cases. Only the perfect solution should pass, 99% is not good enough. One other consideration could be to code suboptimal solutions and ensure that they fail on the test data. This approach is particularly useful for optimisation problems. This is something I learnt from Prateek Karandikar (among a lot of other stuff - he is kind of like my test data guru) : Counting problems are easier to generate test data for compared to optimisation problems. Participants come up with all sorts of wrong greedy algorithms for optimisation problems. You need to ensure that your test data is robust enough to fail all of them, and one way is to think up suboptimal solutions yourself and make sure they fail on the test data. In case you are lucky enough to have the option of creating additional test cases after seeing the participants' codes (this was the case at the training camps), read the code, find mistakes and add breaking test data to the pool. This is something I enjoy doing a lot. And boy, the students at the camp used to love me for it. Here is proof (made by Amartya Shankha Biswas, one of the students I trained): 