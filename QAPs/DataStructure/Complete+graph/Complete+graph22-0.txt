How are  large (complete) weighted graphs built in practice?The problem is to go from a bunch of unconnected nodes to a weighted graph. It looks like if the entire problem is tackled at once, nothing is faster than O(n 2 )  and things are likely slower if there is the need to find shortest paths e.t.c. How do people that deal with large weighted graphs (e.g. Facebook) handle this in practice? Is it just by throwing hardware at the problem?
In case you are talking in terms of data in PetaBytes and companies like facebook, they must be using distributed computing on clusters to handle such large graphs. In most cases distributed versions of algorithms are available, (for example fp-growth algo has a distributed version as well as sequential version). Scaling hardware to solve such problem is not at all cost effective and unfeasible at times. 