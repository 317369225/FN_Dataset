Support Vector Machines:Â Why does SVM work well in practice, even if the reproduced space is very high dimensional?
The classical answer (which you can find in Vapnik's book "Statistical Learning Theory", for example) is that the generalization properties of an SVM do not depend on the dimensionality of the space. You can bound the generalization error by a term depending on the quotient of radius of a ball which contains all the data and the margin realized on that data, but not on the dimensionality of the space. Many extensions exist, but the answer is essentially the same: The generalization does not depend on the dimensionality. An extended explanation is that you can generalize well even in high-dimensional spaces because the data occupies only a low-dimensional subspace of the feature space, and regularization results in the learner dealing only with that subspace. You can see this for yourself if you look at the eigenvalues of the kernel matrix which typically decay quickly, meaning that you can project your data to a low-dimensional subspace with negligible error. So even if you have, for example, a Gaussian kernel, where the feature space is infinite-dimensional, you are actually dealing with an essentially finite dimensional kernel feature space where you are learning a linear decision function, which is statistically tractable. Note that you need to regularize, though. For more information, have a look at my paper http://jmlr.csail.mit.edu/papers..., in particular the discussion on the interplay between regularization, the feature map and generalization bounds. The paper also proposes a method to estimate the intrinsic dimensionality. 