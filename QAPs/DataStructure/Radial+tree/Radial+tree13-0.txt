What would be a good way to organize the field of machine learning into sub-fields, for a new research candidate to gain a good overview of the domain. Basically what are the different streams of research active currently?
Two approaches I would recommend. 1. Read through the proceedings of an ICML conference and you'll get the idea (http://icml.cc/2012/schedule/). 2. Imagine machine learning as being a 3 dimensional space, with the following axes: The type of problem you are trying to solve The way you represent the input and output to the problem The algorithms you use to solve the problem You'll find most machine learning research fits pretty cleanly at a point in that 3 dimensional space. A good way to understand the research opportunities and come to understand the field is to read papers and try to work out where they fit in this model. And -- if you want to be really daring -- which points in the 3 dimensional space have not yet been explored (though by far most of them have). Problem formulation What type of problem are you trying to solve with machine learning? A few of the most popular: Supervised learning: Trying to find a mapping from an input to an output. Unsupervised learning: Trying to find structure in data. Semi-supervised: For some subset of the input you know the output, but not for all of it. Active learning: The algorithm gets to ask questions about some of the input examples. Reinforcement learning: Trying to learn when all you get is a reward telling you if the action you performed was good or not. Representation How do you represent the input and output to the problem? Attribute-value: The input (or output) is presented as a set of discrete and continuous attributes (height, gender, etc), each of which has a value. Relational/structured: You represent the data as individual elements and the relationships between them. Basket: You represent the data as a list of elements (really, this is a subset of relational structured). Probability: You try to represent things as values indicating likelihood. Time series/streams: You represent the data as any of the above, but in a sequence that varies over time. Algorithm families How do you attack the problems? Decision/information-theory/compression: You try to characterize the data as a combination of structure and noise. The more compact the explanation, the better. E.g. decision trees. Memory-based approaches: You try to compare old data to observed data, e.g. k-nearest neighbor Re-mapping the problem into an easier-to-divide space aka kernel methods: Take the problem and find a projection that makes it easy to separate the data. Example: Support vector machines. Looking at and fitting the data's characteristics aka spectral methods/dimensionality reduction: What are the properties of the data? Is there something hidden in there? Example: PCA, ISOMAP. Trying to find a combination of functions that when combined together give the result: How can I combine a set of functions to produce one that matches. Examples: Radial Basis Functions, Gaussian Mixture Models, Neural Networks. Bayesian Probabilistic approaches: Model the world as a set of known and unknown things, and work out the probabilities of correctness. You can pretty much create papers by taking combinations of these. Don't believe me? How about "Applications of support vector machines to active learning of structured output problems"? That's a paper I'd like to read :-). 