Support Vector Machines: Why does SVM work well in practice, even if the reproduced space is very high dimensional?
Mikio's answers are very good. My personal belief is that SVMs work well when a generally smooth function (or equivalently, mostly varying in a lower-dimensional manifold) gives a good performance. I see two other practical and somewhat cultural reasons why SVMs are used a lot: * The optimization problem involved during training is convex (for fixed hyper-parameters). This is appealing and reassuring (but as far as I am concerned, it should not prevent us from exploring algorithms that involve non-convex optimization). Practically, I am also hypothesizing that the better completed optimization that is achieved with SVMs gives right to a cleaner trade-off between regularization and minimizing training error, which is particularly important for small datasets. * The number of required hyper-parameters is small (2, typically, i.e. the regularization coefficient, often called C, and a smoothness hyper-parameter of the kernel such as the Gaussian kernel variance or spread). Since  most researchers are used to perform a manual search or a grid-search (or a combination of both) in order to select hyper-parameters, having just 1 or 2 hyper-parameters is a big advantage. However, there are other alternatives, in particular random (or quasi-random) search can be shown to be much more efficient. My PhD student James Bergstra and I just submitted a journal paper on this question, which will also be presented at the Learning Workshop in April. Random search (or even better, in the future, hyper-parameter optimization) allows one to deal with 5, 10 or 20 hyper-parameters efficiently, freeing us from this constraint. I would add another comment. If the underlying function to be learned really lives in a high-dimensional space and involves a lot of variations, then SVMs (with standard non-learned kernels) do not stand a chance. It then becomes important to 'learn the kernel' or 'learn the feature space', or learn a good representation of the data before applying an SVM (or another classifier) on top. See this paper for a more extensive discussion: Yoshua Bengio and Yann LeCun, Scaling Learning Algorithms towards AI, in: Large Scale Kernel Machines, MIT Press, 2007 http://www.iro.umontreal.ca/~lis... 