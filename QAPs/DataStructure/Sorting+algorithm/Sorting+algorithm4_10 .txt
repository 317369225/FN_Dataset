Which is the best sorting algorithm for large data?该网页不存在附加信息...If you can hold each line of data in memory I would suggest Quick Sort. However, since you say its a very large set of data, I imagine you mean you cannot hold it in memory. In this case I'm not sure what the best algorithm would be. Instead, while this won't teach you about algorithms, if you just want the data sorted, I would suggest using some other tool. Off the top of my head, Use http://redis.io/ LPUSH line_1 <data> | You can read small chunks of your data into memory. Put that data into this Redis list. Drop that data so it is garbage collected, and repeat. SORT line_1 STORE sorted_line_1 LRANGE sorted_line_1 0 1000 : This will return the first 1000 elements of your sorted list, which should be a manageable size. Repeat this with line_2. Update: I thought about this and I have decided that even if you cannot hold the entire line in memory you could still use Quick Sort. 1. If the line can fit in memory. Use QuickSort and return that sorted list in a tmpFile. 2. Otherwise, pick a random pivot from the line you want to sort. 3. Create two tmpFiles; Like you would create two tmpLists in Quick Sort. 4. Read in a small chunk of data from that line. 5. Split/Write the data into one of the two tmpFiles just like you would split a list into two based on your Quick Sort pivot. 5.5 If there is more data loop back to 4. Otherwise, continue. 6. Recursively run this function on the created tmpFiles. 7. Eventually all your tmp files would have lines short enough to store in memory and 1. would sort them using in-memory Quick Sort. 8. As you pop out of your recursive calls concatinate the newly sorted tmpFiles. Again just like you would with Quick Sort lists. 9. Let the recursion end. 10. Move you data to from the final tmpFile to where you need it. 11. Win. I still think the Redis option is better and should probably be used. It deals with all of the optimization and difficulty for you. 