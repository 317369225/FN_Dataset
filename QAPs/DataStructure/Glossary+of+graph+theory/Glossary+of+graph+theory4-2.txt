How reliable is the 100 point scoring system for choosing wine?
Like so many things, your question can be answered thusly: It depends how you use it, and if you know how to interpret its results. Do you have enormous amounts of time on your hands? Then hold on tight, because brevity isn't my strong point on this subject. Wine Tasting is bullshit- or is it? One of the fundamental misconceptions some people have about wine tasting is that they view it as a science, and that science appears to be flawed because it is not consistent. Hence the reason people on Quora consistently cite the following article to "debunk" what they see as flawed science. Wine-tasting: it's junk science Of course, this article misses the point completely. Wine tasting isn't a science at all, and doesn't purport to be. Wine tasting is an art. It's about critically reviewing a wine at a specific point in time from the perspective of a single person, the same way one critically reviews a play or a book. No two people are the same, and the books you read in your lifetime aren't read under exactly the same set of circumstances or in the same moods. Imagine a thought experiment where somebody was able to watch a play and evaluate it. Then their memory of it was erased, and they went to watch it five years later. Then they were asked to evaluate it again. Nobody would expect that the reviews would necessarily rate the play identically in quality- after all, after five years the reviewer's perspective may well have changed. And even if it hasn't, maybe they're just in a good mood that day, or they saw a worse play the previous night. Wine is no different; our tastes change daily, just as they do with food. Some days I crave a fabulous Bordeaux with an elaborately cooked dinner of game. Other days, I'd rather have a cheap Sauvignon Blanc and a peanut butter sandwich. Not only do our tastes change, but so does the wine. Wine evolves and changes in the bottle. Every wine evolves at a different pace- even bottles from the same batch of wine. The play in our thought experiment above doesn't change at all, and we still aren't expecting consistency. Given all this, the notion that anybody would expect wine scores to be consistent is completely absurd. So why do we use them? The rise of the 100-point scale The 100 point score owes most of its renown to US critic Robert Parker, who popularized it in his publication "The Wine Advocate" in the 1970s. The original idea was to attempt to model wine ratings on the scoring system used in American high schools- 90-100 being an 'A' grade, 80-89 a 'B' grade and so on. Until that time the norm in English wine publications was to use either a 20 point scale, or a 1-5 star scale, both of which Parker believed was confusing to the American consumer. Robert Parker's Rating System History seems to have proven Parker correct, because the 100 point scale is now the dominant method of scoring wine. More people understand how it works than any other scale, and it plays a greater role in purchasing decisions than any other point scale currently in use. Not just specialty wine stores, but mainstream supermarkets are using the 100 point scale to highlight their finest offerings. As a result pretty much everybody who buys wine at any point is at least dimly aware of the concept. Most people agree that since the 1970s there has been a huge increase in the quality across the board of wines in practically every region. This is in part due to technological and scientific advancements, but undoubtedly also owes a great debt to Robert Parker. Consumers are better educated, better informed and expect more from their wines than ever before. This has driven winemakers, particularly in the Old World where complacency and lack of ambition was commonplace in the 1970's and earlier, to strive to wring every single bit of quality out of what they have. If they don't, they can't compete. But there's a problem. Or rather three of them I praised Parker for his contributions to consumer knowledge, and those who know my taste in wine are probably going to expect me to excoriate him in this section for his penchant for fruit-bombs. But hold your horses. It is true that Parker's taste leans towards a particular style- big, brash, bold and fruity, high in alcohol and low in finesse. Many wine nerds believe that as a result of this there has been a world-wide "Parkerization" of wines, and believe he is primarily responsible for the late 90s/early 2000s wave of wines made in highly extracted styles. I think this is absolute rubbish, for a number of reasons which would take us on a needless tangent. Primarily though, it's that the fashions these days are drifting in the other direction. Even in the Napa valley- the worst offender in the early '00s- the best wineries are trying very hard to craft more elegant wines, and consumer taste seems to be heading in the same direction. Parker's tastes don't match up with mine at all, but blaming him for the supposed homogenization of wine (and I say "supposed" very deliberately- I think there's a LOT of confirmation bias in this theory), is completely unfair. So why has the 100-point scale become problematic? In my opinion there are three main reasons. Firstly, the inherent difficulty of ranking wines on a numerical scale. I can taste two wildly different wines which both taste wonderful, and find it hard to compare them and say one is better than the other. One wine might go better with food, and one without. Which is better? Who knows. This issue exists with all scoring systems though, so I think we can discount it for the purposes of this answer. Secondly, during its rise questions about the scale were raised about its granularity. "What's the difference between a 91 and a 92 point wine?" people asked. "Surely the twenty point system makes for more consistent scoring!. They had a point; such a tiny difference is pretty arbitrary. But that missed the point in my opinion. The 100 point scale was a trade-off. That lack of numerical exactitude was sacrificed in exchange for a scale which tallied with a "real world" scale everybody knew. In a misguided attempt to preserve the best of both worlds, Wine Enthusiast magazine decided to use an 80-100 point scale. They reasoned people liked the concept of grading wines out of 100, but that a 20-point scale was more reliable. Other publications followed with their own versions. This was disastrous. (apologies for tiny picture- full version at http://wineeconomist.files.wordp...) So now Parker's scale runs from 50-100, as does Wine Spectator, while Wine Enthusiast's scale runs from 80-100. A 90 point wine on Parker's scale should therefore be equivalent to a 96 point wine on the Wine Enthusiast scale. Therefore as a consumer, you need to remember which scale is used by which publication when purchasing. This defeats the entire point of Parker's original idea, which was to have a standard scale everybody could understand, albeit at the cost of some accuracy. There's another huge problem though. The 90-point barrier. It quickly became clear that there was an enormous difference in consumer perception of a wine with an "89" score and a wine with a "90" score. "In a world with so much choice", consumers reasoned, "why should I accept a 'B' grade wine when I can buy an 'A' grade wine?". People tend to make this distinction all the time, which is why we perceive a $9.99 item to be significantly cheaper than a $10.00 item. It makes no logical sense and intellectually we know it. but we still do it. Any winemaker will tell you that a 90 point rating from a reputable wine publication will increase their sales enormously as a result. Those who don't get the high scores tend to hate the 100-point system, whilst those who do seem to quite like it for some reason. This is a unique feature of the 100-point scale. The twenty point scale doesn't have that issue except at the barrier between 9 and 10 point wines (which would both be classed as "undrinkable" anyway) and 19 and 20 point wines (the latter score being virtually unheard of). But there's worse. It's not just the consumers who fall victim to this subtle bias, but the reviewers. The critics are humans, like all of us, and subconsciously are subject to the same effect- namely the perception that an 89 point wine is significantly better than a 90 point wine. If we were to plot a graph of wine scores vs frequency, we'd expect to see a bell curve centered around the 75 point mark. Instead we see this- from Wine Spectator's analysis of their own reviews: In practice, the median score of the wines they review is not 75, as we would expect, but closer to 87. Wine Advocate is similar. The bell curve has been "pulled" to the higher end of the scoring range, and its not unreasonable to imagine the 90-point barrier is at least in part responsible. If a wine is classed as "very good", a 90+ score is intuitively appropriate, even though actually it isn't when you use the scale as intended. The magazines concerned will argue that the higher scores are representative of the higher quality of wines in recent years, but this ignores the fact that the graph above simply does not tally with the original intention of the scale, as described in the link to Parker's website earlier in this answer. So the poor old consumer is once again confused. Is a Wine Enthusiast 90 point wine in any way comparable to a Wine Advocate 90 point wine? And if it is, does that mean it's above average or outstanding? Who on earth can say. So the scale is useless perhaps. Or is it? Workarounds The problems I outline above give consumers a headache for one simple reason- you can't make a qualitative comparison between the relative merits of two wines. But there are a few ways around this problem. Firstly- don't read the scores, read what the reviewer says about the wine. This will always give you a far better idea of what you're getting than any numerical value. Parker himself acknowledges this truth in the explanation he gives of his scoring system. Of course, you need to know what the reviews mean, and this takes a certain amount of knowledge. Will a "structured" wine be to your tastes? You probably won't know unless you drink a good deal of wine and carry a glossary with you to the supermarket. So is there another solution? Well, you could always find a reviewer you trust and stick to their reviews. You'll be comparing like for like, so you won't run into the issue above comparing scores between different rating systems. However you'll be restricting yourself here- not every publication reviews every wine. You're also confining yourself to the tastes of (often) one person. Maybe there's a wine that Wine Advocate loved, but James Suckling despised because it just isn't in his style. You're never going to taste it, even though it's on special offer at the supermarket. Fail. So by reducing the sample size of reviews, we've gotten nowhere. What happens if we increase it? Well, we arrive at probably the most important change in the way we rate wine since the concept of ratings themselves: Cellartracker.com Cellartracker is a website on which people review wine. There are others, but this is the only one you need to concern yourself with, because it's free, and it's the largest, and as I will explain in a moment, size matters. Wines are rated according to the 100-point system, though some people choose either not to supply scores, or to use a different scale and convert it to the 100-point scale. This means everybody's interpretation is slightly different. However, because there are so many reviews, these differences become irrelevant. Take this wine- a 2008 Gloria: https://www.cellartracker.com/notes.asp?iWine=677018&searchId=26F441F4%23selected%253DW677018_1_K77a0376de987803cc60e8eddd872f82a Average score at the time of writing is 88.4 points. But there are 44 reviews- a considerable number. This means that if the distribution of people using different interpretations of the 100 point scoring systems is similar between the wines we search for, we can relatively accurately compare the average figure, thus controlling for all the problems with the system specified above. So let's compare with 2009 from the same producer: https://www.cellartracker.com/no... 91.1 points from 80 reviews. So the 2009 is probably a little better. And indeed I can agree with that assessment having tasted both wines. In addition, the outliers become less relevant. A wine Parker reviews as a 75 will forever be a 75. If I review a wine a 75 and a hundred other people review it as an 89, my extreme viewpoint will be suppressed considerably, so the score is more reliable for most people. This isn't just the fancypants type of wines I picked for the screenshot either- even 2-buck-chuck has 40 reviews for the most recent vintage. Similarly the people reviewing aren't always poncy wine nerds who can't bear the thought of drinking a wine under $1000; they're people such as you and I, drinking wines that you or I might drink. Now, you'll notice I made a significant assumption above, namely that "the distribution of people using different interpretations of the 100 point scoring systems is similar between the wines we search for". It's very hard to estimate if this is the case, and obviously the more reviews a wine has the more likely it is. However if you accept the central idea that wine scoring is at all useful, then is this assumption any worse than the idea of relying on the whim of one man or woman on one day in time? There are also lots of other issues with cellartracker- some people tend to be guided by others' scores (I try to avoid seeing the current score before I review a wine). Some people on the site also don't know a whole lot about wine, though that doesn't really matter for a score; you just have to know what you like. I could go on. But these flaws are minor compared to those with traditional print media. Parker's brainwave has been reincarnated into something genuinely useful for everybody. Try it the next time you buy. Just try to mentally rid yourself of the 90-point barrier first. All of the best value wines I own score under 90 points on CT. A personal note When I started getting into wine, scores provided a very important way of ensuring the wines I was buying using my previous method (completely randomly selecting them) were at least not complete garbage. On a limited budget at the time, this meant I could learn to enjoy wine without too much outlay. For this reason I'll always defend wine-scoring. These days after many years of tasting and writing my own notes, I find scores less and less useful. I tend to spend far more time reading the actual wine review. But I still use cellartracker- why read one review when I can read 150? I've learned to mentally exclude the worst reviews (something I've failed to do with Tripadvisor) and to filter out reviews written by people I know I won't agree with. I still read Decanter magazine regularly, though for the features more than the scores- and the critics who write for it still guide me into new wine regions and concepts, if not perhaps individual wines. But when I'm buying wine at an auction I haven't had time to research, and I have to make a snap decision about whether to buy a bargain lot I know little about, the place I look for a quick score is cellartracker. My money's on the line- that should tell you everything. 