What is the point of doing machine learning, when you have something so robust as the nearest neighbour algorithm?
Here are a few reasons why nearest neighbor isn't perfect/robust: You're assuming we have a reliable distance function ("close"). A lot of the time, that's not true, and especially since in higher dimensions everything is "close" to everything else. Some attributes may matter, others may not from a classification point of view. Depending on the classification problem, simple attribute distance may not be the best predictor of class. Sensitivity to Noise: You're assuming all your class labels are correct, but this isn't always the case. Imagine you have one mislabeled instance in a sea of correctly labeled instances. Despite the evidence, you're going to misclassify instances close to the incorrectly labeled instance. Time/Space limitations: You have a billion examples in 400 dimensions. Do you really want to search through 400 GB of data to find the nearest neighbor when a decision tree might be as small as 20K and give the same result? Yes, you could make that more efficient (e.g. a k-d tree) but it's still a pain to work with. 