What are the techniques used to find nearest neighbor for large scale machine learning?
The approaches which involve one on one lookup are not scalable as you've to do N^2 comparisions - and combine them with dimension of your points. The more feasible solutions create an intermediate space, plot the points there, and search within this reduced space to find the nearest neighbors. A widely used approach uses K-D tree. Think of it like binary search tree for k-dimensional data. Of course searching a balanced tree is quick - and the problem boils down to finding neighbors of a leaf node. Worst case is usually between O(n log2 n) to O(n log n) depending on search comparision method. You'll see that most of these approaches tend to lose accuracy in return for quick search time. These are called "Approximate Nearest Neighbors." The most popular approach is Locality Sensitive Hashing, which maps a set of points in a high dimensional space into bins, and while searching for nearest neighbors, you pick up all the points in the bin. This works perfectly smooth in web scale. Meanwhile there's recent work on distributed/parallel methods for Nearest neighbor Algorithms. Check Fast Library for Approximate Nearest Neighbors(FLANN) which contains MPI based Map-Reduce implementations of nearest neighbor algorithms like KD Tree. Resources: Bentley (1975), Multidimensional binary search trees used for associative searching Aristides Gionis et al (1999), Similarity Search in High Dimensions via Hashing Marius Muja (2014), Scalable Nearest Neighbor Algorithms for High Dimensional Data 