What is the point of doing machine learning, when you have something so robust as the nearest neighbour algorithm?
For most of the problems I work on, NN is infeasible. For instance, I have a name in one document, and other name in a second document. Do they refer to the same person? I need to do this for a large volume of data, so each decision needs to be fast. I can have a machine learning model take the various points of comparison, through them through its equations, and get a score back. I can use that score in relation to other comparisons, and say that its more likely to be in group A than group B. Nearest neighbor won't give me that score, as all of the training data is either going to be true or false, but the granularity is useful for the later algorithm steps. The speed is also much faster, takes up a fraction of the disk space (and since we have to ship the resultant product to other people, and have them use it, both are crucial)  There are also many dimensions that get compared, and it wouldn't be clear how those dimensions would weight for a nearest neighbour. For instance, if one component is "gender", and another is "Similarity of words in the document", the importance is going to be different. Nearest neighbour won't give that, unless you have another machine learning algorithm to learn the weighting on the dimensions for nearest neighbour calculations. As you put it "What better justification/support/evidence  can you give about your inference than saying there is(are) a point(s)  already in the (indexed)dataset which is really close to your query  point?" A lot of the time, nearest neighbor is more like "there is a point in the dataset that is vaugely kind of similar to this point". The justification is really weak, and you don't have any kind of trend to justify your answer. 