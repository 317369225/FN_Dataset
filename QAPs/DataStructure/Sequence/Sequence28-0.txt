Deep Learning:Â Is there an implementation of the LSTM in the Sequence to Sequence Learning paper?
Yes and No. Yes, because there are plenty of RNN and LSTM packages out there. Using Torch, Theano or Caffe, it's relatively easy to implement the model described in the paper. Specifically, if you want to do Neural Machine Translation, which is the experiment mentioned in the paper, lisa-groundhog/GroundHog is a good implementation with more features, for e.g. the attention mechanism. No, because there is no exact implementation as described in the paper. I think most of the time, companies like Google don't publish their research code. They may consider doing so, but only after a long time. If one wishes to implement the method mentioned in the paper, my warning is that the speed reported in the paper is very impressive and not trivial to replicate. 