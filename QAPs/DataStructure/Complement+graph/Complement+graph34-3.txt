At what size does data become "big data"? And is the definition different depending on the industry/statistical test?At university we were taught that n > 30 is a good 'rule of thumb' for starting to see statistical patterns in data. In the messy world of R&D we sometimes joked that it just took two experimental measurements to see a trend in the data!  On a practical basis, when would you say that you had 'big' data in your field of work, and how much data would you consider appropriate to start making decisions...?
I'd like to call out something that Murugan Raj said that I think is critical: When you say "big data" it actually means that data of size that you cannot manage or maintain or process easily. This has very little to do with the number of bytes involved and everything to do with your approach and your resources.  If you are a biologist studying the social behavior of naked mole rats in a custom-built deluxe habitat, 9MB of tracking information gathered over the course of a couple months can be enormous.  (This is a real example and the scientist in question is doing excellent work.) If you are Google then you gobble up petabytes like candy and consider it fairly routine to test your new algorithm by running it on a large chunk of the Web. Consider it this way: you are dealing with big data issues (*lowercase* B and D) whenever you have to structure your entire work process around the amount of information of concern.  The US Census is a good example.  While the final data set is quite compact, the process of gathering, ingesting, curating and cleaning the data is an elaborate enterprise involving lots of computers and several hundred thousand people. 