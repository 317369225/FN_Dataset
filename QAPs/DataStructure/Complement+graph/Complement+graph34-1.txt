At what size does data become "big data"? And is the definition different depending on the industry/statistical test?At university we were taught that n > 30 is a good 'rule of thumb' for starting to see statistical patterns in data. In the messy world of R&D we sometimes joked that it just took two experimental measurements to see a trend in the data!  On a practical basis, when would you say that you had 'big' data in your field of work, and how much data would you consider appropriate to start making decisions...?
Q: What do we call "Big Data" at Google? A: "Data". When I give talks about Google BigQuery, I frequently start by asking the audience "Who here works on big data?". Few, if any, hands raise. Then to make sure I'm talking to developers, I ask "who knows SQL?". Most hands raise. My final question is "Who has ran SQL queries that have taken hours or days to run?" - most hands stay up, and faces reflect the frustrating experiences they've had. For me, big data is not about the size of the data - it's about turning hours and sweat analyzing data into seconds and ease. Look at your watch. If more than 20 minutes have passed since your query started running, that's when you need big data techniques and tools. Updated 9 Sep 2014 • View Upvotes