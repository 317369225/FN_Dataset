At what size does data become "big data"? And is the definition different depending on the industry/statistical test?At university we were taught that n > 30 is a good 'rule of thumb' for starting to see statistical patterns in data. In the messy world of R&D we sometimes joked that it just took two experimental measurements to see a trend in the data!Â  On a practical basis, when would you say that you had 'big' data in your field of work, and how much data would you consider appropriate to start making decisions...?
A long time ago we didn't have the term big data but used the term high performance computing (HPC). At that time, it was considered HPC if the problem cannot be solved with the best state of the art generally available commodity computing technology available. 