At what size does data become "big data"? And is the definition different depending on the industry/statistical test?At university we were taught that n > 30 is a good 'rule of thumb' for starting to see statistical patterns in data. In the messy world of R&D we sometimes joked that it just took two experimental measurements to see a trend in the data!  On a practical basis, when would you say that you had 'big' data in your field of work, and how much data would you consider appropriate to start making decisions...?
Big Data refers to terabytes or petabytes of less-structured data that require Hadoop and/or non-relational databases for cost-effective, efficient processing. It doesn't matter which industry you're in; the term refers to RDBMS + ETL approaches that max out at web scale with less-structured data regardless of industry. Web companies, particularly Google and Yahoo, pioneered these techniques originally for indexing purposes. See Technology Forecast: 2010 Issue 3 for a full description and case studies. Building a bridge to the rest of your data talks specifically about the architecture. HDFS, the Hadoop Distributed File System and the core of the Hadoop architecture, has evolved to the point that it can serve as the foundation for an unsiloed, enterprise-wide approach to general analytics of data in any format, one which complements standard data warehousing. See Data lakes and the promise of unsiloed data for a description and associated case study. What is a Data Lake? PwC, 2014 Some assume that Hadoop requires the use of MapReduce, but that hasn't been the case for years. Most commercial big data analytics platforms support HDFS and there are many different vendors offering SQL-on-Hadoop or R-on-Hadoop, for example. There are also many different approaches available for programmatically querying the data using a range of different languages.  Enterprises who aren't trying to de-silo their data limit themselves to looking under the lamppost for their keys because that's where the light is. A well-managed data lake approach allows an ad-hoc discovery capability across silos and incremental refinement of semantic metadata necesssary to  describe multiple enterprise contexts. ELT, an alternative to standard ETL, refers to an HDFS schema on read + light disambiguation approach that's useful for preliminary data lake exploration. For more formal integration across lakes, scalable integration techniques can be built with the help of of document + graph databases in conjunction with RDF and ontologies. For  more on the enterprise use of RDF and ontologies as a base integration layer, see Technology Forecast: Spring 09. Cambridge Semantics (which has a JBR with PwC) calls their evolution of this approach Smart Data Integration. See http://www.cambridgesemantics.com/solutions/smart-edm/smart-data-integrationfor more information. 