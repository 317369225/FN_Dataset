What is the intuition behind randomized algorithms and randomization in general?
Ah, Randomized Algorithms are certainly very beautiful. They can be often stated very simply, and intuitively, and yet be a pain to analyze. Having done a course in the topic, I can totally empathize with you. At the core of randomized algos, is the analysis. The fact that they can be pretty darn practical and intuitive is all well-and-good, but the minute you get into analysis, that's when everything goes over the head! If you ever feel that its "just theoretical", I'm sure its only because the analysis is fairly complex. Monte Carlo Methods: There are quite a few flavours of Randomized Algorithms. As Devansh Gupta mentioned, there is the Monte Carlo method - which estimates probabilities by running a large number of simulations. Consider this question: Q. Estimate the number of divisors of a given number [math]n[/math]. Ans. Consider the following algorithm... for M iterations, do:    d = random number in 1 to n    if (d is a divisor of n):        increment cntreturn cnt/M * n So here, we have a large search space (size [math]n[/math]), and if we were to try out each and every value, it'd take [math]O(n)[/math] time. But if we did the above, it'd just take [math]O(M)[/math] time. It'd be off, but then the question asks for an estimate. Well, clearly the intuition of the algorithm is fairly straightforward. But anyone would ask the question "But how good is the algorithm? How far from the actual value is the answer? If I were to try out the algorithm with larger [math]M[/math], how much better would my estimate be?" And that's where the analysis comes in. That's where it seems like "randomized algorithms is just about theory". Suppose I were to use the fact that, if [math]d[/math] is a factor, then so is [math]n/d[/math], and just estimate the number of divisors in the range [math][1, \sqrt{n}][/math] and double that? for M iterations, do:    d = random number in 1 to sqrt(n)    if (d is a divisor of n):        increment cntreturn 2 * cnt/M * root(n) Well, fairly intuitively, this second approach is better than the first. But how do we show that? This is just a simple illustration of how different simulation schemes can be used to estimate the same quantity using Monte Carlo methods, and yet one is better than the other. Final word: You are in general trying to answer the question, "What is the least number of simulations I need (M) to ensure that with atleast probability p (some kind of confidence value) I will not be off from the correct answer by more than dx (some tolerance value)?" Sampling: Another kind of area where one could use randomized algorithms is to pretty much just search for a particular kind of structure. The search space may be huge, and one will in such cases try out a bunch of examples, and then either say "found" or "probably does not exist". Also, versions such as "This structure is given to exist. Find it." We can modify our earlier example here too. isPrime(n):    for M iterations, do:        d = random number in 2 to n-1        if(d is divisor of n):            return "is not prime"    return "is probably prime" Now, if we were to compare it with isPrime(n):    for M iterations, do:        d = random number in 2 to sqrt(n)        if(d is divisor of n):            return "is not prime"    return "is probably prime" Some analysis: Let [math]d(n)[/math] = number of divisors of [math]n[/math] other than 1 and n Now, in algo (1) prob of finding a factor [math]p_1(n) = d(n)/(n-2)[/math] So, Probability of being wrong = Prob that n is composite and returns "is probably prime" = [math](1-p_1(n))^M[/math] In algo (2), prob of finding a factor [math]p_2(n) = (d(n)/2)/\sqrt{n}-1)[/math] So, Probability of being wrong = ... = [math](1-p_2(n))^M[/math]. Note, [math]p_2(n) > p_1(n)[/math] Thus, for a given tolerance value, we need run much fewer iterations of algorithm 2! An Example from Competitive Programming: Considering that "Codechef", "Codeforces" and "TopCoder" have been tagged as question topics, I'd like to volunteer this example, as one of "a structure exists - find it". This question appeared in a recent Codeforces round (#192, Div1, C): Problem - C - Codeforces. The quesiton basically asks the following: Given a graph on [math]n[/math] vertices and [math]m[/math] edges, where each vertex has degree atmost 2, find a subgraph of its complement having the same properties (degree bound, number of vertices, number of edges). Due to the fact that the degree of each vertex is atmost 2, for large-ish [math]n[/math], the degree of the vertex in the complement is also large. Hence the chances of finding such a graph is pretty high. What I had done was, for [math]n <= 7[/math], I checked all possible subgraphs of the complement for the  properties. For [math]n > 7[/math], I searched for a cycle of length [math]m[/math] in the complement graph. while true:    pick x[] a random permutation of 1 to n    for i in 1 to m-1:        if (x[i], x[i+1]) is an edge in original graph:            shuffle the permutation from (x[i-1]...x[i+2])            restart checking from i = i-2    if (x[1], x[m]) are connected:        repeat entire procedure    else:        output cycle defined by vertices x[1], x[2], ..., x[m] and quit Note that since I know x[1], x[2], ..., x[i-2] do not cause any conflict, after my reshuffle I need only check for conflicts from i-2 onwards. This kind of "local fix" technique has been used a lot recently as the Algorithmic Lovász local lemma (alas it seems that the wikipedia page is rather complex :() Ashar Fuadi's editorial describes a similar solution (Codeforces Round #192 Editorial - Codeforces scroll down to "nondeterministic solution" 329C), except that the procedure is repeated from scratch as soon as an edge is found. This too apparently works well enough within 100 iterations. 