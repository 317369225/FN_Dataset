Why the current obsession with big data?
The importance of large datasets for allowing more sophisticated statistical models that capture more juice, yield more predictive power, has already been discussed and I agree. But additional elements are worth mentioning: * Many internet-related datasets involve very sparse variables and rarely occurring events: if the things you care most about happen only once in a million user visits, then you need that much more data to capture that phenomenon. For example, in the computational advertising area, with the kinds of big data we are using with steerads.com, the events of interest are clicks, or even worse, when someone buys something in response to seeing an ad. These are extremely rare. * If you are going to use your machine learning device to take decisions that can sometime work for you and sometimes against you (you lose money), you really need to estimate and minimize your risk. Applying your device in a context involving a huge number of decisions is the easiest way to reduce your risk (and this is especially important when there are rare events involved). Of course you need big data to validate that and meaningfully compare strategies. * One should be very careful in assessing the effect of dataset size; it depends on the type of machine learning or statistical model used. The performance of a logistic regression with a reasonably small input size will quickly saturate as you increase the amount of data. Other more sophisticated models (in general going towards the non-parametric, or allowing the capacity of the model increase with the amount of data) will gradually become more relatively advantageous as the amount of data is increased. * The effect of dataset size also depends on the task, of course. Easier tasks will be solved with smaller datasets. However, as mentioned in the above posts, for many of the more interesting, AI-related tasks, we seem to never have enough data. This is connected to the so-called curse of dimensionality: a "stupid" non-parametric statistical model will 'want' an amount of data that grows with the number of ups and downs of the function we want to estimate, that can easily grow exponentially with the number of variables involved (because of the number of configurations of factors of interest can grow that fast). * Advanced machine learning research is trying to go beyond the limitations of "stupid" non-parametric learning algorithms, to be able to generalize to zillions of configurations of the input variables never seen, or even close to, any of those seen in the training set. We know that it must be possible to do that, because brains do that. Humans, mammals and birds learn very sophisticated things from a number of examples that is actually much much smaller than what Google needs to answer our queries or get the sense that two images talk about the same thing. A general way to achieve this is through what is called "sharing of statistical strength", and this comes up in many guises. 