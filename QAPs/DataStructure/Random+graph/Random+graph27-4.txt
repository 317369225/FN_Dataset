Why the current obsession with big data?
Beyond the indeed sickening media hype and all those research proposals / marketing talks to look trendy to get grants, I suggest you listen to the first 10 minutes of this talk by Peter Norvig: Among other things, it presents this figure from Banko, Michele, and Eric Brill. "Scaling to very very large corpora for natural language disambiguation." Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2001. (http://anthology.aclweb.org//P/P...) which basically shows that the best algorithm is highly dependent on the amount of data that you have. I'd also point out to Halevy, Alon, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data." Intelligent Systems, IEEE 24.2 (2009): 8-12. Lastly, regarding what "big" means": Artificial Intelligence: A Modern Approach: In 1992, a large corpus was a 1000-page encyclopedia; today it would be a 100-million-page Web corpus. 