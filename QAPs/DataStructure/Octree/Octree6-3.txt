What is one way programming has helped you look at the world differently?
Things coming up my mind, in no particular order Natural minds are amazing, human minds in particular Well, everyone has a notion of this nowadays. However, as a programmer I often think of tasks we execute in a snap, which are very hard to be coded as a computer program. To make just an example, think of these CAPTCHA boxes that prevent SPAM (CAPTCHA): you have to read characters in a noisy, deformed draw, to prove you’re a real human being and not a SPAM-sending program. The reason why they work is that human minds do the task with just a glance at those characters (well, no, with a massive signal and information processing they are unaware of), while making a computer to do the same is something that, depending on how the CAPTCHA principle is implemented, ranges from fairly difficult, to extremely hard and virtually impossible. Formality, details, precision are often necessary I also realise how much the human brain is powerful when I see how it easily deals with lack of information, wrong information, ambiguous instructions and the like. This makes programmers very used to think of details, formal descriptions, natural language tricks (Collection of Ambiguous or Inconsistent/Incomplete Statements). Example: think of sorting a list of names in alphabetic order. Probably non-programmers would imagining giving a look at the list, look at the ones starting with ‘A’, then ‘B’ etc. I think: what about upper/lower case? Do the names contain numbers, spaces or punctuation too? Which order do these additional symbols have? Which operations can I use to sort, swap positions one-by-one? Numbering multiple names at a time? Given I swap one-at-a-time, which exact steps can I apply? Does one procedure (we say ‘algorithm’) only exist, or more than one? Which one is faster? We love to model, abstract, generalise, factorise it all down to a few rules Back to the sorting thing, think of ‘<‘ as in 2 < 3 and ‘before’ as in ‘Anderson’ before ‘Smith’. Which kind of relationships are ‘<‘ and ‘before’? Do they have something in common? Do they have differences? Do similar relationships exist? Could I represent them in such a way that the sorting algorithm could be defined only once and then used for sorting numbers, names or something else? Which model should I use to do that? That’s about generalising the order relationship out of similar cases. We achieve it by building abstractions, i.e., forget numbers or names, think generically of: 'x less-than y', assuming that, whatever the meaning of x, y, ‘less-than’, certain properties hold (e.g., if x less-than y, it cannot be y less-than x). In writing ‘x less-than y’ I’m also using a model and a formal representation of such a model. Programmers do thinking like this all the time, starting from all sort of day-to-day situations they meet. Like mathematicians, or physicists, they love to hack the whole world into elegant models, small sets of rules and principles, which can be thought once and applied or easily adapted (almost) everywhere.   World needs standards How much mobile chargers do you have around your home? Why can’t we just have the same charger and same pair of plug/socket for each electronic device? Programmers think of this all the time, often vehemently. That’s about standardising: define a set of simple and common rules to show documents to users and to hop through them, let developers use your rules for free and voilà! You have the World Wide Web and the Internet revolution! Open standards are often made through generalisation and abstraction (previous point), we enjoy them, because they simplify our job a lot, but I like them also because they make the world better (e.g., anyone is free to code a new standard-compatible web browser, maybe to help developing countries or blind people).  Computers (still) have nothing to do with intelligence or wizardry, but artificial minds might exist in future We use to say things like ‘the computer thinks it has finished processing this and that’, but it’s just jargon, we never means ‘thinks’ literally. Thinking, in its widest form, is a complex process that involve self-conscience and intuition (see my first point), computers have nothing to do with that, they just turn symbols to other symbols in a mechanical way, no matter how clever we are in making you believe they have a mind. That said, I love movies like Blade Runner or Matrix, or authors such as Asimov, I often think how a true artificial intelligence would (will?) be like in future, what being alive and sentient actually means and how other intelligent living beings, artificial or not, should be, so that we would acknowledge them the same human rights that we have defined for ourselves (by the way, I also love a lot the ‘Planet of Apes’ films). Digital technologies have a lot to do with society and politics Well, everyone realises this nowadays. Programmers realise it in ways that probably aren’t so common. For instance, today I’ve had a chat about the app stores. The main topic was: which software mechanisms can avoid scams or protect children, without a single company manually deciding which app developers have the right to distribute and which ones users have the right to use (OK, I won't mention any company with a fruit logo, but you got the idea...). This is technical stuff, but it’s also about our rights and freedom (whatever you think of the specific issue). Not all the computer geeks care about these things, but I and many others do care a lot and I like to think beyond the technicalities. We’re no-frill I don’t know if this is related to computer science, probably it’s common to all technically-oriented people. Anyway, we don’t like political games, mumbo-jumbo talking, management bla-bla. Definitely, The Dilbert Principle is our manifesto (Dilbert principle). If that’s affected by our computer knowledge, it might be that, if you compare a programming language to a natural language like English, the former is much more straight to the point: simple and sharp sentences, commanding this and that, few synonyms and we don't like synonyms much anyway (more things to learn and more sources of errors), no room for ambiguities, rhetoric or hypocrisy. 