What is an intuitive explanation of gradient boosted trees?
Each tree in boosted model (which is a sum of trees) splits the space into regions. Now when you optimize the loss function at stage m (when building m-th tree) you do a line search for each region separately. The line search gives you a value at a terminal leaf of m-th tree corresponding to each region. However, ideally you would want to just move in the gradient direction of a loss function. Loss function seen as a vector has a component for every training point. You want to do a line search at each training point separately. The regions constraint you from doing that. Gradient boosting is a specific type of boosting when you are trying to mitigate that region constraint. You are still building a tree at each stage but you fit m-th tree to the negative gradients (which are just the differences between actual values y and current model fits). 