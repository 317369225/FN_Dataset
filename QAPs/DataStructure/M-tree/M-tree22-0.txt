What is an intuitive explanation of gradient boosted trees?
I will try to avoid all that math. Imagine, that you are trying to solve a very difficult problem. You give an initial solution, which may be bad, but then you see where you are wrong and you are going to improve that solution. But in every iteration only thing you do is alternating your strategy to correct the mistakes you made in the previous one. The next thing is more interesting. When you think you are done, you don't just throw away all but one solutions - you collect then, estimate the error in each and assign a number to them, which indicates, how important this solution is, according to it's correctness. And in order to have final solution, you just make a certain decision, based on "weak" answers you got. This combination of weak solutions works usually better, than a strong one. 