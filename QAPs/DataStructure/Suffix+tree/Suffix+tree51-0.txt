Is it possible for alternative algorithms to problems with the same time/space complexity as the previous solutions to get published?
Sure, but it should have some advantage over the prior work. I'll give you a real example. In fact, I'm in luck: I can just quote Wikipedia here. In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995. The algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its "on-line" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix. The naive implementation for generating a suffix tree going forward requires O(n^2) or even O(n^3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms. So we have: Weiner's algorithm, which uses linear time and space, published in 1973; McCreight's algorithm, with the same performance, which is simpler and was published in 1976; Ukkonen's algorithm, with the same performance, but with the property of being online (so that after the first N characters have been read, the suffix tree for the first N characters is already ready), published in 1995. 