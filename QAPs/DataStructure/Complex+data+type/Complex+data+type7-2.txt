What are the design, implementation and real world use/experience differences between Parquet, ORC and Trevni?There seems to be a rise of new file formats around the Hadoop ecosystem.  What are the strengths and weaknesses of each?  Why is everyone creating their own new format?  How are they better or worse than the historically popular RCFile?
I might not be able to answer really comparing 3. But below is my experience with ORC. I have been using ORC quite a bit now in Hive. Frankly if I run on huge data sets that comes from opensource tools, with a good processing before inserting in Hive, I saw a very good performance improvement. I can really publish exact numbers but in terms of percentage I would say its 70%(Average) of RC which I had used earlier. Also note that ORC comes with various configs like Stripe size, inner compression etc which is subjective on your data format and size. I did enough research on code and did various testing on multiple sizes. Bottom line is I have seen significant improvement. At the same time, I have some people forcing to use Parquet and see if it could be helpful which I didnt really get time to do. 