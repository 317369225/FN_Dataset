What are the design, implementation and real world use/experience differences between Parquet, ORC and Trevni?There seems to be a rise of new file formats around the Hadoop ecosystem.  What are the strengths and weaknesses of each?  Why is everyone creating their own new format?  How are they better or worse than the historically popular RCFile?
Orc and Parquet allows to save time while reading data. Both of them can skip columns and rows (predicate pushdown) while reading data. This may save much time and makes MapReduce jobs  faster. Orc is implemented in Hive API, while Parquet is independent project. Parquet, Hive and Criteo: the Backstory - Criteo Labs Hive has some problems when dealing with Parquet files: Hive doesn't correctly read Parquet nested types  It's fixed recently, but not in production yet. 