Why did innovation on persistent data structures come from Clojure before Haskell?
Persistent data structures have been around for decades. This shouldn't be surprising: it's a simple idea that comes up as a natural consequence of functional programming implemented with pointers. First, to be clear, let's define what we mean by "persistent data structure": an immutable data structure that reuses ("shares") parts between different versions. Instead of having to copy the whole data structure each time a small part changes, we only copy a subset and maintain pointers back to the rest of the old structure. Since everything is immutable, this is perfectly safe. The thing is, pretty much any immutable data type written in a functional language is going to be persistent to one extent or another. Take the list type, which is probably the simplest functional data structure there is: data List a = Cons a (List a) | Nil This, as it happens, is also the perfect implementation of a persistent stack. If I start with the list ls = [1,2,3,4]: If I pop the top of the stack to get ls' = pop ls, I'll have the following: The "new" list is just a pointer to part of the old list. (My GraphViz skills are a bit lacking I'm afraid :P.) If, instead, ls' swapped the top of the list for something else, it would look like this: Even when we ended up changing the stack, we only need to copy over the single node that changed and could reuse the rest of the list's structure. So the first important point: even an immutable list is a persistent data structure. Usefully so, even, as a stack. This has been around pretty much as long as functional programming. Now, of course, this was a pathologically simple example: the structure we wanted (a list or stack) was naturally persistent. Things get harder as we try to implement more complex data structures persistently. For example, it's not clear how we would implement a sequence where we can efficiently access either end or how we would make a key-value map. As it happens, however, many persistent data structures for this sort of use case have been around for decades as well. The canonical resource for these is Chris Okasaki's Purely Functional Data Structures which started out as his PhD thesis in 1996. A bunch of these efficient persistent data structures feature prominently in Haskell: Data.Sequence, a sequence type with O(1) access to either end  based on 2-3 finger trees Data.Map and Data.Set based on size-balanced binary trees and can work with any ordered keys Data.IntMap and Data.IntSet  based on binary Patricia tries which are significantly more efficient than Data.Map and Data.Set for integer keys As far as I know, all of these have been in active use for a very long time. A lot of the structures in Okasaki's book actually predate Haskell and the book itself has code snippets in both ML and, in an appendix, Haskell. (The most salient difference is laziness which makes it easier to give certain performance guarantees in Haskell.) So there's definitely a fair amount of persistent data structures in active use in Haskell. These have all the properties you'd like of a persistent structure (ie not copying too much) but are just not based on the specific data structure popularized by Scala and Clojure. The main technical contribution from Clojure is the Hash Array-Mapped Trie as an efficient persistent key-value map. This is certainly useful—it performs well and has some good properties—but it's just a single structure rather than the whole idea of persistence! As it happens, there is a HAMT library in Haskell; however, because of technical limitations in Haskell, it is rather slower than the JVM version. There is an interesting article comparing IntMap, JVM HAMTs and Haskell HAMTs which goes into more detail about this. (The technical limitation in Haskell is to do with how much it costs to freeze and thaw vectors inside the HAMT.) The article also has a series of benchmarks which show that, at least for core operations, the binary Patricia trie implementing IntMap performs on par with Clojure's HAMTs: Size IntMap Java HAMT (32K-512K) Java HAMT (512K-32K) 32K .035s .100s .042s 64K .085s .077s .088s 128K .190s .173s .166s 256K .439s .376s .483s 512K 1.047s 1.107s 1.113s The two Java columns have the benchmarks run in opposite orders which presumably affects the JIT optimizations performed by the JVM. So there we have one reason why HAMTs have not really caught on in Haskell: the existing structure performs well enough that any improvement would be minor. It certainly isn't a bottleneck! The two structures aren't exactly comparable because they have different restrictions and operations: IntMap only works for Int keys so it avoids hash collisions and the like, but in turn it provides efficient ordered iteration, merges and intersections. (That said, you could easily generalize IntMap to arbitrary bytestrings like the bytestring-trie package.) The other reason is a bit more cultural and historical: Haskell types are rarely hashable by default because there's no compelling reason, so people are reluctant to reach for anything that requires hashing. On the JVM, of course, pretty much everything is always expected to be hashable so this is not an issue at all. That's not a big cost, but it makes the barrier to entry for a new data structure just that bit higher. The other contribution of Clojure is not technical: it popularized the whole idea of "persistent data structure" as an interesting class unto itself. Before that, functional programmers did not have a good PR story on this; in fact, I'm not even sure they had a handy name to give to this sort of data structure! It certainly wasn't popularly known. Again, this was certainly a useful contribution, but it doesn't mean Haskell was somehow lacking. Rather, it just did a bad job of marketing some of its ideas as useful standalone concepts. And really, this makes sense: from the perspective of Haskell, persistent data structures are just normal so people focused more on the holistic benefits of functional programming throughout than on those particularly. 