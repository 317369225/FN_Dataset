How does a computer distinguish a number and a letter or a string? I mean, let's say we have a bit sequence like 11110100 in anywhere in the memory. How does a computer know if it is a number or a string or a letter?
To expand on the other answers: As they say, it doesn't know unless you tell it. Indeed, in a von-Neuman architecture it doesn't know if it is data (integer, letter, etc) or if it is code & instructions. This can lead to weird things like self-modifying code. But in a Harvard architecture data & program are in different memories. It is possible to extend that: some other architectures do make a specific physical difference. For example, I once worked on an architecture (LINN REKURSIV) where every word in memory had a hardware tag field that described what type of data it was. 