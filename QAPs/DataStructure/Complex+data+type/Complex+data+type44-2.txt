How does a computer distinguish a number and a letter or a string? I mean, let's say we have a bit sequence like 11110100 in anywhere in the memory. How does a computer know if it is a number or a string or a letter?
In general, it can't, and doesn't know. The bit sequence you gave could be the byte-sized integer 244, the signed byte -12, the first byte of a /24 IPv4 network address, the character ô (in Latin-1 encoding), or any number of other things, and the computer fundamentally doesn't have a way to judge that. However, software running on the computer has expectations, and if it is expecting to find a Latin-1 character where it finds that bit pattern, it'll interpret it as ô. If it is expecting to find an IPv4 network address, it'll interpret it that way. If it is expecting to find a floating point number, it'll interpret it that way. There are many a bug and security exploit that come about by accidentally or deliberately messing with those expectations. 