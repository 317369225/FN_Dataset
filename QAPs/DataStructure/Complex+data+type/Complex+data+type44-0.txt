How does a computer distinguish a number and a letter or a string? I mean, let's say we have a bit sequence like 11110100 in anywhere in the memory. How does a computer know if it is a number or a string or a letter?
A computer knows because you tell it.  When you write a program, you declare variable types and data structures, and the instruction sequence of the program guides the computer to understand how to interpret the bit patterns in different places in memory. Sometimes, software bugs cause the program to look in the wrong region of memory with a different type of data, or perhaps a variable type is declared incorrectly.  Then, the fireworks start!  You might see gibberish output, get nonsense results, or have the program just crash outright. If you have a raw binary dump of raw memory, you have to guess what the data might be.  You can sometimes make educated guesses.  For example, if you have a lot of adjacent binary data that looks like a character set (ASCII, ISO-8852,  Unicode), you might conclude you are looking at string data. Complex data structures sometimes embed a Magic number to help programmers find the boundaries of that data structure in a raw dump.  Numbers used in integer and floating point operations usually have a characteristic "feel" and can be scoped out by experienced debuggers. But, a computer only knows what it knows because it is told.  Which brings us the the old programmer's poem: This darn machine is broken I think that I should sell it. For it will not do the things I want But only what I tell it. 