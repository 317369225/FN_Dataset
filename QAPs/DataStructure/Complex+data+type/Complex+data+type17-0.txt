What are the differences between Crunch and Cascading?
I'm in the same boat as Chris K Wensel in that I am not an unbiased party. :) TL;DR The main difference I see is the level of abstraction in the data model and how that level of abstraction impacts the amount of boilerplate involved when writing user-defined functions. If you are working with a simple data model and you can use the built-in functions almost exclusively, then Cascading or Pig can make your life easy. If you are working with more complex data objects that require you to write lots of user-defined functions (UDFs), then I think that Crunch will make you much happier. The Long Answer I started working on Crunch a couple of months after I joined Cloudera in June. I had been at Google for a few years, and primarily used Sawzall and FlumeJava to write my MapReduce jobs. I began working on a project that required a reasonably complex MapReduce pipeline, and I decided to use that project as an opportunity to learn Pig. (At the time, Cascading was still under the GPL license. I believe Chris is planning to release the next version of Cascading under the Apache License, which is great news for the community.) It took me a little while to warm up to it, but I came to like Pig, which uses the "Tuple data model." As Chris pointed out in his answer, both FlumeJava and Crunch have Tuple objects as part of their system, so my referring to it as the "Tuple data model" in the FAQ was not a great choice and I will fix that. For the rest of this answer, I'm going to refer to it as the "single serialization type" (SST) model as opposed to the Crunch/FlumeJava "multiple serialization type" (MST) model. SST systems provide lots of built-in functions for working with their primary type, but I get frustrated with them whenever I come across a problem that requires me to write custom UDFs. On the project where I was using Pig, I ended up writing five complex UDFs that had non-trivial input/output schemas (e.g., nested bags of multi-type tuples), and I spent a fairly tedious day moving from Pig to Java and back again to get everything working together. My next project involved working almost exclusively with binary data records-- primarily time series data that was stored in a nonstandard format that could be organized in a few different ways (e.g., 2D vs. 3D vs. 4D) which I would then need to convert to arrays of floats, perform a series of processing steps on (e.g., a Fast Fourier Transform), and then sort/aggregate according to a series of binary header attributes. There were Java libraries for the individual subtasks I wanted to do, but any given task could use one or more out of about 50 functions, each with different parameters, and the thought of writing many, many Pig UDFs on this and all similar problems made me want to give up show business. Here are a few lines of code to demonstrate what I'm talking about. Let's say you want to write a custom function that takes in two integers, adds them together, and returns the result. Both Pig and Cascading have built-in functions to make this easy, but the example will illustrate my point about what is involved when writing more complex functions. Here's the code for this function from the Cascading User Guide: (http://www.cascading.org/1.2/use... public class AddValuesFunction extends BaseOperation implements Function { public AddValuesFunction() { // expects 2 arguments, fail otherwise super( 2, new Fields( "sum" ) ); }   public AddValuesFunction( Fields fieldDeclaration ) { // expects 2 arguments, fail otherwise super( 2, fieldDeclaration ); }   public void operate( FlowProcess flowProcess, FunctionCall functionCall ) { // get the arguments TupleEntry TupleEntry arguments = functionCall.getArguments();   // create a Tuple to hold our result values Tuple result = new Tuple();   // sum the two arguments int sum = arguments.getInteger( 0 ) + arguments.getInteger( 1 );   // add the sum value to the result Tuple result.add( sum );   // return the result Tuple functionCall.getOutputCollector().add( result ); } } And here's what AddValues looks like in Crunch: public class AddValues extends MapFn<Pair<Integer, Integer>, Integer> { public Integer map(Pair<Integer, Integer> input) { return input.first() + input.second(); } } Almost all of the difference in LOC are due to the different data models. The difference becomes progressively more extreme as the entities we want to perform custom operations on become more complex. Using the AddValues function in Crunch is just as simple: PTable<Integer, Integer> numbers = ...; // Use Avro as the serialization format PCollection<Integer> sums = numbers.parallelDo(new AddValues(), Avros.ints()); I really like the compile-time type checking of the MST model. If I try to apply the AddValues function to a PTable<String, Integer>, or expect it to return a Float, my program won't compile, and Eclipse will notify me of the problem immediately. If I make a mistake and apply a Cascading/Pig UDF to a tuple of strings instead of integers, I won't get an exception until runtime-- and in this case, the runtime isn't a little script on my machine that runs in two seconds, it's in the middle of a long chain of MapReduce jobs that runs overnight on my Hadoop cluster. Static typing is also the preference of the developer who is reading your pipeline code in order to fix it while you are on vacation. Opaque tuples can make it difficult to figure out what exactly is going on in the system without delving into the details of how each function works. I agree with Chris that the SST model is the only practical choice if you are using a dynamically typed language to write a MapReduce job. But Crunch also has the benefit that the underlying serialization model is pluggable: we currently support both Writable and Avro-based serialization models, and I think that it would be pretty cool to run a Crunch pipeline that used Cascading's serialization format. You could then write a Crunch pipeline entirely in terms of Cascading's Tuples, which could be the basis for a MapReduce framework in a dynamic language. (That said, we already have Cascalog and Cascading.JRuby, so I don't think anyone should actually do that.) Chris is also correct that the Crunch/FlumeJava model can get unwieldy if you create very long Tuples. I think that if you have a problem where that happens, you really want to be using a structured format like Avro (software), Apache Thrift, or Protocol Buffers, since records that are made up of lots of fields tend to have even more fields added to them over time, and those systems can help you manage that complexity. At Google, most MapReduces run over data that is stored in Protocol Buffers. Protocol buffers can get pretty complex-- nested fields, repeated fields, etc., etc. Sawzall took the approach of mapping protocol buffers into its own SST format, which I think turned out to be a bad idea. Once your data was in Sawzall's format, it tended to stay in Sawzall's format, which didn't play nicely with the rest of the systems at Google, which do almost everything in terms of protocol buffers. FlumeJava would let you read, write, and operate on protocol buffers directly, which I think was one of the reasons it became so successful: the fact that it was agnostic about data serialization meant that it could integrate well with the rest of the systems at the company, which meant that it was used for everything from aggregating log records to building a search index to cleaning old data out of a BigTable instance. I don't think the Hadoop community needs any more SST systems, and I would discourage anyone who was thinking about writing one. But I think that the benefits of the MST model-- lightweight UDFs, compile-time type checking, complex data types, and easy integration with the rest of the data at an organization-- fills a very definite need. We welcome people who want to help out with Crunch, you can start by joining the developer mailing list: https://groups.google.com/a/clou... or the user mailing list: https://groups.google.com/a/clou... 