What are the design, implementation and real world use/experience differences between Parquet, ORC and Trevni?There seems to be a rise of new file formats around the Hadoop ecosystem.  What are the strengths and weaknesses of each?  Why is everyone creating their own new format?  How are they better or worse than the historically popular RCFile?
I don't think that anybody can say all that much about these new formats since there isn't much runtime on them anywhere.  There are early benchmarks, but these are not the sort of long-term experience you would really like to see. That said, the design of parquet is particularly interesting in that it intends to support nested data well.  This is a big deal in the denormalized world of big data.  ORC files as described in the power point presentation attached to the JIRA issue for them make a comment about complex data types, but this really sounds like the support is not so much going to go the way of Dremel [1], but more the way of Hive in which the complex data elements are stored in a single cell.  As I mentioned, the presentation does not make this clear. This paper [2] provides some insight into what works and what doesn't in column stores.  It would likely be useful to replicate these tests with the new formats to see how things fall. [1] Dremel: Interactive Analysis of Web-Scale Datasets [2] http://pages.cs.wisc.edu/~florat... 