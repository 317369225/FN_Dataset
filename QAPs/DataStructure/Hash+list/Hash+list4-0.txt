Which is the best method or algorithm to compare two large lists of email addresses in a short time?
Your question isn't specific enough to make a single recommendation.  The best answer depends a lot on size and how many duplicates you expect to find. If the lists are small enough to fit in memory, are unsorted and scanning through them completely with a single machine counts as quick, then the idea of building a hash set out of the smaller list and then scanning the larger for matches is a fine one. If one of the lists is small enough to fit in memory and you need to use more than one compute to make scanning the larger list fast enough, then scanning the small list on all machines or scanning on one and passing the hash set around will allow you some speedup, limited by the cost of building the hash set. If the small list is too large, then building partial hash sets on many machines can work.  Distribute the hash sets according to some modulo method.  Each machine would then read part of the bigger list and send the email address according to the same hash modulo method the first list hashes were distributed. You can also read both lists simultaneously sending hashed emails to particular machines according to hash value.  Whenever an email arrives at a particular hash twice, you have a duplicate.  You can also append all values to files chosen according to hash to you don't have to have enough memory all at once.  This last algorithm is essentially a map-reduce program.  The mapper hashes each email address and the key is the hash and the value is the email address.  The reducer counts the addresses. Note that with the last two solutions, you can always pick a large enough intermediate files to make in-memory sorting work.  Since hashes are fixed size integers, you can use radix sort to do the sorting in linear time.  This allows good parallelism and good performance except that you have to shuffle pieces of the intermediate files around which can be intensive network wise.  Hash collisions could still give you n-log n behavior, but only for very intensely large sets.  Note that duplicates in temporary files can be filtered before shuffling which can avoid certain pathological problems. This last implementation is very close to what was used in the MapR record-breaking terasort and minute sort runs [1][2].  The MapR distributed file system is high enough performance that using the distributed file system for the shuffle is a very efficient choice. [1] Breaking the Minute Barrier for TeraSort | Innovation Insights | WIRED [2] Google Teams With Prodigal Son to Bust Data Sort Record | Enterprise | WIRED 