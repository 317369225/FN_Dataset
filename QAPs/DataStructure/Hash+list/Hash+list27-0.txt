Is MongoDB a good replacement for Memcached?
Background: I've implemented a cache for medium-sized image objects (4KB-100KB) on-top of MongoDB. These objects are relatively expensive to produce (~100ms min, ~500ms average, ~1.25s 90 percentile, ~10s max CPU time) and change rarely (usually around once per quarter), so caching them saves significant amounts of CPU time. These images are grouped into sets and I used a collection for each set, which allows entire sets to be invalidated by just truncating the collection. There are anywhere from a few thousand objects in each collection to millions. Pros High-quality drivers in most languages (just like memcached). Persistent, which means if you've got a large set of objects that are expensive to produce, you can still hit disk to get them. In some cases, even slow spindles are cheaper than CPUs. In the case where you have thousands of CPU hours in the cache, that's thousands of CPU hours that have to be recovered if the cache server dies. Replication, so losing a cache server doesn't have the potential to bring your entire application down. Collections support atomic truncation, so groups of objects can be invalidated effectively instantaneously. Structured storage means that adding metadata to binary objects is not a chore. Indexable on multiple attributes, so space efficiency can be increased if values needs to be accessed by multiple keys. The larger your data values, the more space efficient MongoDB will initially be in comparison to memcached. Cons Stores keys in a b-tree instead of a hash table like memcached, a necessity of storing keys on disk, which means you lose O(1) key lookup and retrieval. Uses a universal persistent storage format and access mechanisms for both in-memory and on-disk data. This is a natural result of relying on using memory-mapped files and relying on the OS to manage caching. Ask the author of Redis, who's done extensive research and testing in this area, and his conclusion was that the gains by using specialized structures and access patterns for each type of storage exceeds the benefits of memory-mapped files. RAM is fast and small, disk is slow and big, and that means a database needs to carefully manage memory, using it very sparingly, and disk space can be burned to increase performance. Because MongoDB uses a single structure for the database, it ultimately makes a compromise between these two, and ends up with a less-than-optimized result. For instance, if the data is guaranteed to be in RAM, you can achieve much higher space efficiency by aggressively managing data fragmentation, which would be a disaster if disk I/O was involved. If you have many small values (<1KB) and a data set that is significantly larger than RAM, caching efficiency will dramatically drop over time as these values slowly get fragmented and the minimum cache granularity is 4KB (at least in Linux). If the value cross page boundaries (partially in one page, partially in another), it'll have to cache both pages, now you're up to 8KB. This relates to the compaction issue I mention below. memcached uses much more efficient slab allocation. What I'm going to call as the hot data cliff: As the size of your hot data set (data frequently read at sustained rates above disk I/O capacity) approaches available memory, write operation bursts that exceeds disk write I/O capacity can create a trashing death spiral where hot disk pages that MongoDB desperately needs are evicted from disk cache by the OS as it consumes more buffer space to hold the writes in memory. It's self-compounding because the increased need to hit disk to perform more reads, it will further contend for disk I/O capacity the writes need to flush themselves out. There aren't very many warning signs for this as as far as I know, because there's no way to determine how big the really truly hot part of the page cache really is. If you have a consistent, low write rate (<50% of I/O capacity), this isn't as much of a problem because page cache miss rates are unlikely to climb above the ability of the writes to be flushed to disk. A database with a write-ahead log (Cassandra, HBase, PostgreSQL, InnoDB, etc) is much less likely to crater in this situation because they ALWAYS perform sequential writes first to their binary logs, and then lazily merge these changes back into the main data files when I/O rates have calmed down. When MongoDB deletes a value, it simply marks the area of the data files used by the value as deleted and makes a note of it a list of free space. As of this date, MongoDB doesn't have background compaction, so depending how variable the size of the application's cached values are, the data files will end up looking like swiss cheese and this can wear down performance over time as it will take longer and longer to find one of these holes to insert the data in. The only way to perform compaction is by taking the server offline and running a lengthy repair, which is not feasible for many applications. Further, if your app uses more than half the available disk space, you can actually end up in a situation where compaction will be impossible as the repair operation re-writes the contents of the entire database. To me, the lack of online compaction and the "hot data cliff" are a major caveat of MongoDB for any use case. The most insidious part of these problems is that they won't be revealed to you until long after you've implemented MongoDB in production. IMHO, this should be a "big and bold" and plastered all over their documentation site. One (relatively expensive and time-consuming) band-aid to the compaction issue is to use an N+1 replica set (where N is the number of nodes you need to achieve durability goals), rotating which node is the master, and forcing a full re-sync of each node, which will rebuild the data on the slave from scratch, effectively re-writing all the data in a compact manner. Note that this is not just a MongoDB problem, it's also a problem with Tokyo Tyrant/Cabinet, membase, ehcache DiskStore, and possibly others. Considering that there HBase, Cassandra, and Riak all perform efficient, automatic, continuous online compaction, why not look at one of those instead? All that being said, if you've got tens or hundreds of gigabytes of resource-intensive values to cache, you'll want to be able to expand into disk and have persistence/replication in the event a cache server is lost. The cache can become so critical to performance that your operation will simply fail without it. 