What is the best way to maintain a API wrapper class across multiple languages?Preferably with some sort of automation / generation
Check out Apache Avro or Thrift, and perhaps Google Protocol Buffers as well. All three of these tools allow you to define data types and declare function signatures in a custom interface definition language (IDL) that is then compiled to a variety of target languages, including C++ and Java. The IDLs of the tools are similar and support similar features, although Avro IDL is my favorite. Protocol Buffers (or Protobufs, as they're commonly called) and Thrift require the programmer to explicitly specify a number for each field (e.g., http://code.google.com/apis/prot...), which allows for a flexible schema that changes over time. Avro instead stores the schema in JSON format within the generated source code, and it defines an algorithm for resolving conflicts if two Avro endpoints have different schemas. Protobufs also don't provide native support for map/dictionary types, while Thrift and Avro do (however, the keys in Avro maps must be strings, as is the case with JSON objects). Avro is also unique in that you can specify your schemas in native JSON (albeit with more verbosity), eliminating the need for a compiler. At Google, Protobufs are often used to compactly serialize data. For example, you would define a data type in the Protobuf language, generate the source code for Java classes that implement those data types, create instances of those types within a Java program, and then serialize them as a sequence of bytes in a format understood by Protobuf tools. You can do the same with Avro or Thrift, but these tools are more known for their RPC capabilities. Protobufs do support RPC functionality but the actual implementation, Stubby, is not open-sourced, so you need to find your own. In addition to defining data types, you can declare functions for RPCs in the IDL files. This allows you to generate C++, Java, etc... classes with custom methods. When you invoke one of these generated methods, the underlying RPC framework (usually) transmits the call over the network, serializing its arguments if any are provided, and then a server on the other end will receive the call and handle it. Any exceptions or return values will likewise be serialized and sent back to the caller. In essence, you can call a method in a Java program but have a C++ implementation do the real work, and it's Avro/Thrift/Protobuf-RPC's job to act as the "glue" between these two languages. This is where the IDLs shine; for example, a single file written in Thrift IDL can be compiled to C++, Java, Python, Ruby, PHP, Erlang, etc... and each of those languages can be used within the client (caller) or server (callee). It's not necessary to use a server, however; you can choose inter-process communication instead of sockets or HTTP connections. Thrift is the clear winner when it comes to targeting a wide array of languages; the official site names, "C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml." Avro and Protobuf target the more popular languages, and are appropriate choices if you don't need this degree of cross-language interoperability. Be aware that Python Protobufs often receive complaints regarding performance. My personal choice is Avro, having done some limited work with all three. Not only is Avro IDL relatively expressive, it doesn't require the programmer to explicitly number the fields of a data type. Avro is also the most recent of the three and is a sub-project of Hadoop, and the Cassandra data store is moving (has moved?) from Thrift to Avro. In my eyes, that says it's ready for production. A bit of maybe relevant informal reading: http://news.ycombinator.com/item... 