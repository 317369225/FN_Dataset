online and parallizeable set intersection algorithm apparently this be a distribute system problem . you have ~ 10 stack , each with up to 10 million item at a time , and each stack live on a different machine . here be one simple candidate solution . have a separate `` monitor '' machine , whose sole purpose be to continually compute the intersection . any time one of the stack be modify , the machine store that stack should send a message to the monitor message describe the change . -lrb- these diff can be batch , depend upon you latency requirement . -rrb- now you have all the datum on a single machine , the monitor machine . when run on a single machine , you can completely avoid all of the concurrency issue associate with multithreading or distribute system -- e.g. , the need for synchronization , lock , etc. moreover , the amount of datum be small enough that it could easily be store in the available ram on that monitor machine . -lrb- why ? if each entry be large , hash it first use a good hash function and a large enough hash output that you win t see collision . if each hash value be 128 bit , then store all 100 million hash item take 1.6 gb . if you use sha256 truncate to 128 bit as you hash function , by the birthday paradox , the chance of encounter a hash collision be incredibly small . -rrb- once all of the datum be live in ram on a single machine , the problem become much easier . for example , one approach would be to build a index datum structure : a hashmap that map each key to a 10-bit bitmap that indicate which of the stack it be store on . you can easily update this hashmap as the individual stack change . you can also easily use this hashmap to compute the intersection : just have a doubly link list thread through the entry of this hashmap where the bitmap be equal to 11111111111 -lrb- all 1 s -rrb- . any time a bitmap change from 11111111111 to something else , you remove it from the doubly link list . any time a bitmap change from something else to 11111111111 , you insert it into the doubly link list . the amount of datum you need to transfer from other machine to the monitor machine be very small , especially since you only need to send the hash item to the monitor , not the original item . if each of the 10 machine will do at most 4k push\/pop s per second -lrb- take from you question -rrb- , then that s 64 kb of hash datum per second per machine . if we add another 32 byte or so of overhead -lrb- packet header -rrb- , that s about 200 kb\/s of datum out of each machine , or about 2 mb\/s of datum into the monitor machine -- a very manageable amount . on a 1 gbp ethernet link you win t notice this , and you might not notice it even on a 100 mbp link . you can reduce the amount of traffic by a factor of 2-5x by batch such message and\/or by use a shorter hash . this should provide a efficient algorithm to compute the intersection , and keep it update on the fly as each of the stack change . 