What are analytic functions?I am a high school student. As I was reading the wiki page of the Riemann's zeta function, I found that it has been proved that (1−s)ζ(s)  is analytic. I also read that analytic functions are rather "nice" as they have really "nice" properties. What is so special about them? Why do number theorists want to find more and more analytic functions?
I am not a number theorist, but I can tell you why analytic functions are nice from the perspective of an analyst. First of all, what's an analytic function? Well, if you take an arbitrary function (defined on the real numbers), then you can talk about things such as: continuity: is its graph a connected curve ("continuous"), or does it have jumps ("discontinuous")? differentiability: are there any cusps in the graph? ("non-differentiable") Differentiable functions are quite nice, because you can look at their slope (rate of change) at each point. If a function is differentiable at a point x0, then nearby that point you can approximate the value of the function by a simple linear model: f(x 0 +h)≈f(x 0 )+L 0 h  where L 0   is the "derivative" (i.e. slope) of the function at that point). If your function were a straight line then this would be an exact equality, but most of the time it's just an approximation. That's great, but can you keep going? If your function is differentiable, then you can talk about its slope at each point. This is a new function, its derivative. Well, you can play the same game: Is the derivative continuous? Is the derivative differentiable? If the derivative is differentiable, your function is "twice differentiable". In that case, you can do better than just a linear approximation. You can approximate with a quadratic (a parabola): f(x 0 +h)≈f(x 0 )+L 0 h+12 Q 0 h 2   Now you can just keep going. You can talk about third, fourth, fifth, etc. derivatives as long as they exist, and then you can get higher and higher degree polynomial approximations to your function. "Most" random functions start developing cusps of some sort at some derivative, and then you have to stop differentiating. That's too bad. However, there is a special collection of functions, called the smooth functions, and those are the ones that are differentiable as many times as you'd like. But even smooth functions aren't the best possible. For example, the function f(x)=e −1/x 2    when x>0  , and f(x)=0  when x≤0  is actually a smooth function (plot it on wolframalpha!), and it has the special property that all its derivatives at x = 0 are equal to zero. In other words, the slope, the slope of the slope, the slope of the slope of the slope, etc. of the function at zero is zero. Any reasonable person would guess that the function should be zero everywhere. Alas, it's not the zero function as you can tell from its definition. It's just some funky function. What went wrong? The thing is that smooth functions, as nice as their title may be, aren't the "best" possible as far as this polynomial approximation is concerned. Among those special functions called smooth, there is an even more special collection of functions that are called analytic. Those are such that this polynomial approximation works, and functions such as the one I wrote down for you above are not allowed in this special collection. Very loosely speaking, analytic functions are like infinite degree polynomials. They're very well behaved. If you can show a function you care about is analytic, that's only a good thing for you. By the way, polynomials, trig functions, exponentials, etc. are all analytic. Hope this helps. 