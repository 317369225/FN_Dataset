What exactly is a Huffman coding tree?
First let's talk about coding trees in general. You don't really need coding trees if you want to transmit only a single message. For every possible message, you just come up with some code word, any sequence of ones and zeroes that is not yet used for another message will do, and that's the end of it. However, there is a problem when you transmit a sequence of messages: how would the receiver of your code stream know where one message ends and the next one begins? For example, suppose we use a code where "monkey" is encoded as 110 and "discombobulating" as 11010. Upon receiving 110, you wouldn't know if the code word was complete and message was "monkey", and any subsequent bits should be interpreted as the start of the next encoded message, or if instead you should wait for more bits to complete the first code word. The obvious solution would be to introduce pauses in between messages. This is how it's done in Morse code, for example. But this is actually a truly ugly solution, because now we have a code that uses three symbols: zero, one and pause. And it's not terribly efficient either, because there will be many more zeroes and ones than pauses, so the third symbol is underused. For this reason, it's convenient to design the code words in such a way that the decoder knows exactly when each message ends just by looking at the code words itself. The best way to do this is to use a prefix-free code: a code in which no code word is the prefix of another code word. It is useful to visualise a prefix-free code as a binary tree, that can be used to decode a code word as follows: the code word is scanned from left to right. Starting at the root of the tree, for each 0 we encounter we take the left branch, and for each 1 we take the right branch, until we reach a leaf. On that leaf we then find the original message. So when we reach a leaf we know that the code word is finished and what the message was. This is a coding tree! Huffman figured out a way to construct the optimal coding tree if you know the frequencies with which each message is sent. Shannon had already shown that the number of bits you need on average is equal to the entropy. Huffman's algorithm constructs a coding tree that he proved comes as close to the entropy as possible; the average number of bits used with the Huffman code is at most one bit more than the entropy, and the difference is less than for any other code. So the technically precise answer to your question would be that the Huffman coding tree for a given distribution on messages defines a prefix-free code that minimizes the expected code word length. 