Where does [math]\Theta(\log n / \log \log n)[/math] show up in terms of algorithms?
A function that is [math]\Theta(\log n / \log \log n)[/math] is a function that grows a little bit slower than [math]\Theta(\log n)[/math] -- that is, as a time complexity the first one is a teeny tiny bit better. You can sometimes get such time complexity by shaving off a little from an algorithm that needs [math]\Theta(\log n)[/math]. This sometimes can be done, as in the second example below. There are a few cases whereÂ  functions with the desired growth rate occur in practice. One has already been mentioned by Anonymous in the comments: if you have n bins and you throw n balls into the bins uniformly at random, the expected number of balls in the most full bin is [math]\Theta(\log n / \log \log n)[/math]. As a practical consequence, imagine that you just picked a random hash function (e.g. as in universal hashing) and used it to create a hash table of size n with n elements. Then, suppose that an enemy who can see the hash table gets to choose requests you should process. In this situation, the expected worst-case time complexity is [math]\Theta(\log n / \log \log n)[/math] per query. Another such situation: rooted trees. The depth of a standard binary tree is roughly [math]\log_2 n[/math]. If you want a tree that's more shallow, you can get it by increasing the degree of each node. However, you should not increase it too much, because then processing a node becomes slow. For some algorithms on trees the optimal trade-off is somewhere close to the middle. For example, consider a d-ary tree with [math]d=(\log n)^c[/math] where c is a small positive constant, e.g., c=1/5. The depth of the tree turns out to be [math]\Theta(\log n / \log \log n)[/math], and at the same time the degree is still small enough to process such nodes efficiently. This is, for example, the main idea behind the time complexity of a Fusion tree. 