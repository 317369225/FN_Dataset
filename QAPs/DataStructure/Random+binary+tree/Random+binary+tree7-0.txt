Why is DFS usually more space-efficient than BFS?
I'll only discuss the case of trees but the reasoning for graphs is similar. A DFS will only store as much memory on the stack as is required for the longest root to leaf path in the tree. In other words, it space usage is [math]O(h)[/math] where [math]h[/math] is the height of the tree. A BFS on the other hand will queue every node at a fixed depth before visiting the next depth. If, for example, you have a complete binary tree, the number of leaves will be [math]n/2 = \Theta(n)[/math] where [math]n[/math] is the number of nodes in the tree. Each of those [math]n/2[/math] nodes will be queued up at some point. Many trees you will come across are moderately balanced. Even randomly built binary search trees have expected logarithmic height. In these cases [math]h = O(log\ n) = o(n)[/math] and DFS will therefore use less memory (http://cs.stackexchange.com/ques....) In the case where you come across trees with no comparison metric, it's still typically true that trees are wider than they are deep. For instance, consider the case of doing a DFS or BFS on a particular domain (say a site like The New York Times).Â  Each particular node/page in the graph/website will have a ton of links to other pages in the site. On the homepage, you might easily have 100 links to other NY Times pages. Right at the first step, all those 100 links get added to the queue, and you're only at depth 0 in your search (you could do clever stuff to de-dupe links, but we can safely ignore that for demonstration purposes). On the other hand, I doubt there is a page in the NY Times graph that can only be accessible via 100 clicks through unique pages. Articles are typically accessible between 2-3 clicks, so you can already get a sense of how this website graph is much wider than it is deep, and hence why a DFS would use less memory than a BFS. 