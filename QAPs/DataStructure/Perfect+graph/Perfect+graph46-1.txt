What problems were once thought not to be solvable in polynomial time, but eventually were?
Recently, there has been a push towards provable machine learning within Theoretical CS. Two results that stick out as seminal results in the field are: Proving that finding the exact parameters to a mixture of gaussians model (e.g. not the EM algorithm!) is polytime Proving that non-negative matrix factorization admits a polynomial time algorithm Another algorithm that people thought was 'impossible' (although it is unclear if there are sources that claim that the problem was non-polytime) is Fully Homomorphic Encryption. (see Tarun Chitra's answer to Have there been any new brilliant computer science algorithms in last 10 years?) Mixture of Gaussians A mixture of gaussians is a distribution [math] \rho = \sum_i w_i \mathcal{N}(\mu_i,\Sigma_i)[/math], where [math]\mathcal{N}(\mu_i,\Sigma_i)[/math] is a multivariate normal with mean [math]\mu_i[/math] and covariance [math]\Sigma_i[/math] [math]w_i[/math] is a mixing weight (proportion of samples from this gaussian)The goal is to recover the mixture weights [math]w_i[/math] with polynomially many samples from [math]\rho[/math]. While one can get a very quick and dirty estimate for the weights by running the EM algorithm, these solutions are only local and not global minima of your loss function (e.g. [math]\ell^2[/math] loss function, or least squares) In 2010, Moitra and Valiant [0] proved that this problem is definitely polynomial time. They do this by showing that you can first reduce the problem to inferring the parameters of a set of univariate normal (clearly polynomial time) and then using a hierarchical clustering method to combine the parameters of these univariate normals. There are a lot of degenerate/corner cases that make it very tricky to show that this method works. Instead of handling each case individually, Moitra and Valiant constructed a clever error-correcting algorithm that is run on top of the single-linkage clustering tree. By showing that the overhead of the error correction is polynomial, they were able to deduce the polytime complexity of the whole inference algorithm. Non-Negative Matrix Factorization Non-Negative Matrix Factorization is the process of decomposing a matrix [math]A[/math] into the product of two matrix [math]B,C[/math] such that both [math]B,C[/math] only have non-negative entries. In particular, one can view [math]C[/math] as a matrix of 'probability distributions' (topics) and [math]B[/math] as a matrix of mixing parameters for these topics (the analogy to LDA is correct: NMF is the same as Latent Dirichlet Allocation, see [1]) . This effectively boils down to computing a matrix factorization with a constraint. Since many other constrained factorizations are known to be NP-complete (e.g. tensor factorization, which is related to NMF), most researchers thought that exact NMF was an NP-complete problem. Note that most practical users of NMF use approximations. In 2012, Arora, Ge, Kannan and Moitra [AGKM] proved that non-negative matrix factorization admits a polynomial time algorithm [2]. The techniques they use are fairly complicated (they do a certain type of optimization over semi-algebraic sets that encode the non-negative constraints), but the result was quite surprising to the many in the field.Â  [0] [1004.4223v1] Settling the Polynomial Learnability of Mixtures of Gaussians [1] [1204.6703] A Spectral Algorithm for Latent Dirichlet Allocation [2] [1111.0952] Computing a Nonnegative Matrix Factorization -- Provably 