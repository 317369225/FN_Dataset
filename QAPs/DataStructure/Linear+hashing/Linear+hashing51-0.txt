Given a file of 2 billion 64-bit integers, how can one beÂ  found that appears at least twice?
Use a bloom filter! Ok, lets assume that we have not quite 16GB memory to hold 2B 64bit numbers) and no writable storage so we can't cheat and use the file system as extended memory. The brute force solution to this problem is the O([math]n^2[/math]) for candidate in list  for comparison in list    if candidate == comparison      return candidate Assume that the above pseudo code works by seeking along the file, 64bits at a time, rather than having an in-memory array. 2 billion squared 2^128 (4e+18) comparisons is not going to run very efficiently, but 4e+18 disk reads is also going to be excruciatingly slow. You'll need an extended coffee break, or perhaps a long vacation. However, this is still the right way to think about the problem. What we need is a faster way to check each candidate. 2 billion file reads and comparisons, O(n), is too much work for each of the 2 billion candidates. Enter Bloom filter. Bloom filters are a probabilistic data structure which answers the question "is item X not in the set" by using a comparatively small amount of memory, however it can only say an item might be in a set with some probability p, which improves with the memory size of the Bloom filter. for candidate in list  if bloomFilter.doesNotContain(candidate)    bloomFilter.add(candidate)  else    for comparison in list      if candidate == comparison        return candidate If we had a perfect Bloom filter p=1, then we could check each candidate in O(1), resulting in an O(n) algorithm! However, a Bloom filter is not perfect, so we verify the possible duplicate entry by walking the file again and doing an actual comparison. The runtime of this will be O([math]n + n^2 * (1 - p)[/math]), a tradeoff between memory available to your Bloom filter and runtime performance. An approximate O(n) runtime would require a Bloom filter with a 1-in-2-billion chance of a false positive for 2-billion items. With 1GB of Bloom filter space, we can reduce 23% of file reads. If we did have 16GB, we could reduce 99% of file reads! 