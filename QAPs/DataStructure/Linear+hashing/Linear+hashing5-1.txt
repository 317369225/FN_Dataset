Which is faster, finding an item in a hashtable or in a sorted list? Sounds easy? Think, rethink and comment.
A sorted list is generally not a particularly useful thing, and it's also really time-consuming to generate.  That's because you can't do a binary search in a list: you have to start at an end, because you can't find the next list element without looking at the pointer(s) inside the current element.  So its average search time is O(n). A hash table, on the other hand, has typically O(1) search time. Let's analyze the algorithms more deeply: The steps for the list would be, starting at the end (head if singly-linked, head or tail if doubly-linked): Is value of list node equal to search item?- Yes: return the item.- No: is it greater (less than) the item?    - Yes: return "not found"    - No: are we at the far end of the list?        - Yes: return "not found".        - No: Set current node to next node and go to step #1. Interestingly, this is no improvement on an unordered list, since the chances of matching the item, given N nodes, is 1/N in either case. A hash table's algorithm is roughly this: Compute the hash of the key.Use the hash value as an index into the hash table. There are several choices about what to do next, depending on the particular implementation of the hash table: A hash table that uses lists internally to resolve collisions will do this: Check each item in the "bin" to see if it's a match.Continue until a match is found or the end of the list is reached. A hash table of fixed size has an algorithm to find the next possible slot: Is this item a match?Yes: return the item.No: compute the next possible slot's address    Out of addresses?    Yes:  return "not found"    No: go to the next possible slot and jump to step #1. A "perfect" hash function is constructed from a set of values that are already fully known.  Such a function can be created that will have no collisions. Return the item at the index. So.... The list search will always be of O(n).  Examination of each element involves a memory dereference (of the node address), an extraction of the value, and a comparison. For hash tables, we always have to compute a hash value.  While constant in time, that computation typically involves a loop and an operation on a portion of the key, until the entire key is hashed.  That could take the equivalent time of checking several list elements. Then, we need to do a lookup in the table.  A well-built table will have very few collisions, and the lookup will also be a memory dereference (of the table entry), an extraction of the value, and a comparison.  If we encounter a collision, we'll have to dereference the next location and repeat. A poor hash function could lead to the degenerate case for the implementation that uses lists internally.  All the data in the hash table could be stored in a single list, with the rest of the table empty.  That would constitute an O(n) search, with the added overhead of the hash key computation. Similarly, fixed-size tables with poor hash functions could also degenerate to O(n). A very short list would have a faster lookup time than would a hash table.  I'd guesstimate ten or so items before the hash table lookup would be faster, mainly because of the hash key generation time.  If it were a "perfect" table, the list would "lose" to the hash table at a lower element count. In conclusion, the hash table will "win" when, roughly, the speed of computing the hash becomes faster than the speed of traversing the list. 