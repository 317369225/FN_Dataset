What are the differences & similarities between SVM & Naive Bayes for binary text classification wrt how they are processing the features?
Most of the time you use linear SVM for text classification and multinomial naive Bayes can also be interpreted as a linear model so you often end up with similar decision functions. In practice Multinomial NB can be better than Linear SVM in some situations and svm better than NB on other datasets. Apparently t's possible to combine the two approaches to get a very good baseline: projects:nbsvm - Sida I. Wang You can also try non-linear SVMs but often the quadratic complexity of the SMO algorithm (for instance as implemented in libsvm) makes it not practical on datasets with more than 5000 documents. Instead on prefer to use liblinear than can only train linear SVM on large datasets. It is possible to simulate a quadratic kernel using hashed cross product features as implemented in vowpal_wabbit using the hashing trick (see Feature hashing) and then training a linear model on those non linear features. 