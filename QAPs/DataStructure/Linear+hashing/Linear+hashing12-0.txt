Why do so many CS graduates flunk simple interview questions in algorithms?
Many computer science programs got watered down with less hands-on work  due to a high wash-out rate which was bad for department revenues.  Joel Spolsky reported 40-70%. Supposedly 1/3 of my Data Structures class (assignments were to implement data structures in 'C', grades came from what fraction of TA defined automated tests they passed) failed  although our reject rate hiring graduates went from negligible to about 50% when that professor was no longer allowed to teach it. Without hands-on work students don't internalize the information as well.  Tests may only require memorization which allows students to graduate who cannot apply what they learned even when they still remember.  I can't count how many candidates I've talked to who can rattle off big-O characteristics of all the basic data structures but not pick one for random insertion and sequential or random order removal (even when things are relaxed to allow specification of appropriate STL containers). Selection bias also precludes screening a representative sample of working engineers.  The best candidates tend to get jobs through their personal networks and rarely appear on the market.  The acceptable ones quickly get jobs.  The rest are the 95 - 99.5% having their resumes forwarded to numerous companies.  That's why  internship programs are great - they start with a normal candidate distribution and you essentially have first right of refusal when the students graduate.  Joel Spolsky covers this more eloquently in Finding Great Developers. All the questions are basic.  Only the hashing question requires dredging something out of memory; although that's such a common thing  that an engineer who doesn't know it is almost like a writer who can't use adjectives "Run spot, run!"  Flattening a tree into a linked list wasn't something I thought of until writing Drew Eckhardt's answer to Ordered Tree to Circular Doubly link list: A non-recursive function treeToList (Node root) that takes an ordered binary tree and rearranges the internal pointers to make a circular doubly linked list out of the tree nodes?  and isn't something I'd expect an interview candidate to come up with. 110%.  Double from the n part of n log n.  log 2 1M is 20 for practical purposes which gets you 21/20 for the extra 1.05 multiple. Split the data up into memory sized chunks.  Sort those on the servers in parallel using something convenient.  Use external merge sorts on the results.  Use TCP/IP streams as the inputs for those sorts.  Merge subsets of machines with more levels of merging for really big fleets. Without parent pointers - in order traversals using stacks with O(log n) space complexity.  Skip over interior nodes with one child as a space optimization.  If mutating the trees is acceptable I can  flatten them into doubly linked lists discarding the interior nodes for O(1) space.  With parent pointers - in order traversals using O(1) space. Use some f(x) providing a reasonable output distribution, modulus over hash table size which ought to be prime for fewer collisions.  Using linked lists for collision handling you get O(n) insertions in the degenerate case.As a student I didn't use any libraries beyond libc which is what I'd expect (or the analog in scheme, java, etc) in a computer science program - those should be about the unchanging fundamentals that apply throughout peoples' careers, not whatever simple skills  are popular today.  Even recent graduates should be able to apply those fundamentals. Some jobs don't require more than plumbing together libraries; although those are easily outsourced to engineers living  places where $1500 rents a mansion with servants not half a one-bedroom apartment with salaries that match.   Students only learning enough to compete in that job market aren't doing themselves any favors. Some don't.  I need Merkle trees with only the leaf nodes persisted to make  end-to-end integrity verification work in my online backup product when using parallel processing within a file which makes us viable in the image backup space.  I needed external merge sort to make sync restart work following a hard shutdown for merging the new and old  copies of my client side meta-data cache which uses only sequential disk IO.  A trie was appropriate to track the in-memory subset of the directory tree being operated on.  Four priority queues get things processed efficiently in an order minimizing memory footprint.  State machines were the simplest way to drive the synchronization process given asynchronous meta-data cache input, directory traversals, and directory descendent processing completion. Updated 56w ago • View Upvotes