Which data stream mining tools can handle Big Data?
If you prefer to work within the Java / Apache ecosystem, it should be quite straightforward to reuse the hashing text document vectorizer &  Stochastic Gradient Descent logistic regression classifier impl from Mahout by wrapping them as S4 Processing Elements. You can also use zookeeper to periodically share & average the weights of various SGD instances running concurrently on the same S4 event stream in case one machine is not able to deal with the events rate. Weights averaging should work good enough on linear models. The other approach is to perform feature sharding as WV does in streaming distributed mode. In any case start with the singleton SGD PE instance before diving in a parallelized implementation. SGD updated are really fast and is it likely that upstream feature extraction will be the bottleneck anyway. As for online / streaming clustering, it should be quite straightforward to implement Sequential KMeans as a S4 processing element downstream the mahout hashing text document vectorizer: http://www.cs.princeton.edu/cour... A good trick to know when implementing this is to count the number of times a centroid is activated and discard the less activated centroids from time to time and replace them by random variations of the most activated centroids. It helps kmeans avoid being stuck in bad local optimum. Also kmeans can benefit a lot from whitening. However it is not possible (AFAIK) to compute a streaming online PCA estimatation. It might be interesting to experiment with running the mahout SVD implementation on a  batch of historical data collected from your stream and then use the whitened singular vectors as a fixed (or batched, asynchronously updated) projection basis for preprocessing your streaming kmeans input. 