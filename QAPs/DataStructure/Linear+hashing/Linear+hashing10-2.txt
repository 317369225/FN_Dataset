How can one find out duplicate URLs from a list of 4TB of URLs?
Single machine: Iterate through the data adding each element to a Bloom filter. Before you add each url (make sure that you normalize the urls first), check to see if it's hash pattern is already activated in the bloom filter. If so there is a possibility (but not certainty) that the current url is a duplicate. Record this urls hash pattern in a separate list of potential duplicated hashes. Iterate through the data a second time, this time check each url against the hash patterns in our list of potential duplicates, if so add this url to a new list of potentially duplicated urls. This list will contain all duplicated urls, plus some number of false positives (due to the way a bloom filter works) The list of potentially duplicated urls is hopefully sufficiently small that we don't need to do anything clever to find the duplicates in it. Cluster of Machines: Similar to above, but we construct separate bloom filters on each machine. We can then send these small bloom filter objects to a single machine, and combine them using bitwise OR. The combined bloom filter can now be sent back to each of the machines and used to identify the potential duplicates. Send the potentially duplicated urls to a single machine in order to find the real duplicates. 