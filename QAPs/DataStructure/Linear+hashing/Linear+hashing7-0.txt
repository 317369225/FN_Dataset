What is the best way to computationally estimate cardinality of very large sets?
Up until recently, I thought bloom filters were a pretty clever way to count distinct elements in a set. But a recent comment by Zzzz Zzzzzz pointed me in the direction of Probabilistic Counting, which estimates cardinality by hashing elements to a bitvector, where the probability of hashing to a bit decreases exponentially as the bit significance increases.  The implication is that larger sets include "rarer" elements, and this relationship can be exploited much like the German tank problem ( http://en.wikipedia.org/wiki/Ger... ).  The paper Mateusz pointed out is here: http://algo.inria.fr/flajolet/Pu... So I thought this was pretty great for about a day or so, until Norman Casagrande pointed out an even more effective means of cardinality estimation for very low error rates.  Scientists at Ask.com compared a bunch of different estimation methods both theoretically and practically (on a large corpus of real world search data) and found that Linear Counting was even more effective.  Linear counting also hashes elements to bits in a bitvector, but the hashes all have the same uniform probability, and cardinality is estimated by looking at the probability that a bit in the bitvector is empty, essentially the ratio of zero bits in the bitvector.  Paper is here: http://www.edbt.org/Proceedings/... So by looking at the publication dates of those two papers, according to my own internal cardinality estimation, there's probably an even better technique by now!  I'd also be interested in any good open source implementations of the above techniques. 