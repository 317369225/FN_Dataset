How can one find out duplicate URLs from a list of 4TB of URLs?
The general approach that you'll want to use is known as near-duplicate detection.  The basic idea is that you use a very fast algorithm to identify pairs of items that might be duplicates, and then do an explicit pairwise comparison to filter out the false positives.  This generally involves some sort of locality-sensitive hashing, but the thing that makes this problem special is that your data is relatively short strings.  https://cs.brown.edu/courses/csc... is a recent paper on near-duplicate detection in databases of short strings, and is probably as good a place as any to start looking. 