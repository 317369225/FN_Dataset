Given a file of 2 billion 64-bit integers, how can one be  found that appears at least twice?
As others already mentioned hashing works but it uses 2 billion * 8 = 16 billion bytes which is roughly 16 GB of memory. This is the best in terms of time complexity but 16GB is not an acceptable memory usage most of the times. In case you have multiple nodes available, a Distributed Hash Table could be created and perform the same algorithm in a distributed manner with optimal time complexity. Another solution would be to use external merge sort to sort the values and then traverse the sorted values and see where duplicates appear. A third solution is to partition the input, create for instance 16 buckets, 1 GB each appr. If the distribution of the integers are uniform in the interval -2^63 and 2^63-1, then it is simple to obtain the 16 intervals used for each bucket. If we do not have preliminary information about the distribution of the values, we could pick randomly 16 integers several times, sort them and use their average () as interval endpoints. This way we reduced our memory requirements to roughly 1 GB and we need to traverse the input for each bucket, keep the values from the interval and use the  basic hash-based solution to find a duplicate. After finding the first duplicate we can stop. Of course instead of 16 buckets we can use any number of buckets but we need to consider the tradeoff between memory and time and find values for k (the number of buckets) in such a way that it best meets our requirements. 