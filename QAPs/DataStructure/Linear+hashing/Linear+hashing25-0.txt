How do giant sites like Reddit verify that a username isn't taken so fast?
Here's a simple, although naive answer. I don't have first hand knowledge of exactly how Reddit (or Google, or Facebook) ensures uniqueness, but the general principals of what I'll describe below are probably pretty close to the truth. Linear Searching is Painful Let's start with the simplest approach: all the names are stored in a list, and the software simply checks every name in the list to determine if the user name is already taken. This really is awful: we need to look at every single name every single time. That's not reasonable for the 36 million users Reddit actually has, and completely untenable for the 1.5 billion users that Facebook tracks. Fear not! Computer science is here to help! Binary Searching is Really Clever A better approach is to use a binary search. Generally stated, you keep your giant list of user names alphabetized, then divide and conquer! For example, let's say you're looking for "peat" ... instead of starting with the first name in the list, you jump to the middle of the list (the 18 millionth record in Reddit's case). Let's say the name at that place is "michael", so we ask a simple question: is "peat" alphabetically before or after "michael"? Because "peat" comes after "michael", and we know the list is alphabetical, we can completely ignore the first 18 million records. Woo! How's that for a speed up? Next, we repeat. There are 18 million records remaining, so we split them in half and jump forward 9 million positions. We land on "tina" ... well, "tina" comes after "peat", so now we can ignore the last 9 million records. Two comparisons, and we've already discarded three quarters of the entire list. Cool! Repeat: we divide again, jump back, and we hit "peter" -- now we have a match on the first two characters (lucky us!), but "peter" comes after "peat" so ... we split the list and jump backwards again! To make a long story short, if we repeat this method a few more times and we will come to the conclusion whether or not there is a record for "peat." At the end of the day, using a binary search means that the maximum number of comparisons we have to make on a list with n user names is the binary logarithm of n. For 36 million users, that's only 25 comparisons in the worst possible case! Not bad. Now, let's take a look at something a little harder. Facebook. With 1.5 billion users we're looking at only 30 comparisons in a binary search. Wow. Not that much harder, hmm? ... But What About Hashing? Hashing is a cool trick, and a bunch of people have mentioned it in comments and other responses. Basically speaking, a "hash table" takes a key (like a user name) and runs it through a function to generate an address (or index for an array) where the corresponding value can be found. In our simple use case of identifying whether or not a user name is unique, we would stick the user name into the hashing function, then see if there's a user record at the calculated address. This has the awesome property in that it works in constant time: calculating the address takes the same amount of time no matter how big the data set is! In an ideal computer with unlimited memory, this simple hashing technique would be perfect. In goes "peat", out comes memory address A, and the computer checks to see if there's anything at A. Unfortunately, this simple approach doesn't work out in real life. There are 38^20 possible Reddit user names, so even if we were addressing individual bits as true/false flags, there isn't enough memory on Earth (nor will there be for some time) to store the entire hash table. Doh! So, we can't use an ideal hash table that contains the entire address space for Reddit user names -- but there are a lot of clever ways to make a hash table compact, so that it only has to cover the existing user names. The trade off is that when the data set grows, the table has to be rehashed. In other words, the calculation for where to find a value changes when the data set changes, which means that the data has to be moved around (akin to keeping the list alphabetized in the binary search example above). This highlights one of the biggest problems that engineers have to deal with at scale: unchanging data is easy to deal with and optimize, but for sites like Reddit and Facebook, the name of the game is maintaining high performance while massive amounts of data are being introduced into the system. Just For Fun: Can We Use Both? Now we're on to something! Most gigantic production databases that deal with uniqueness and scaling use a combination of techniques. The engineers are faced with constraints on how quickly these operations have to perform, the volume of requests they have to handle, plus the reality of system limitations (memory, CPU, network speed) and failures (hard drives, networks, even data centers). For example, a very large key-value database spread across a hundred servers will behave like a hash table from the outside. From the outside, you just give it a key (like a user name), and it returns a value (like the user's account info). On the inside, it might use hashing to determine which cluster of servers is responsible for managing that record, then binary search trees or hashing to locate the record. Wikipedia has some really wonderful articles on hashing, binary searches, trees, and other fun topics. Every database in the world uses some variation or combination on these techniques for retrieving data, and each has their own advantages and disadvantages. TL;DR -- Binary searches and hashing are damn near magic, and unholy Chimera-esque combinations are what you'll probably see in practice. (edit 1: that's what I get for writing an answer on my phone at lunch; edited and updated answer for confusing log2 with square root. Always proof your work, kids!) (edit 2: there are a lot of comments about hashing, so I've added a section with some thoughts on hashing, and a bit about the painful reality of building large systems) Updated 20w ago â€¢ View Upvotes