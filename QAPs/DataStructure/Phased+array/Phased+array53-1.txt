Does a radio transmitter use more power when many receivers are tuned to its frequency than when there are no receivers?
Well, the short answer is "no".   A transmitter may put out 50,000 watts, but your radio antenna picks up like well, let's do the math, say it picks up 50 microvolts across 50 ohms, that's voltage squared over R watts, or about 5 times 10^-15 watts.   The rest of the watts are wasted. One very interesting side-question is that of "radiation resistance".  The last I heard it was still a bit of a mystery.  Some theories say that it shouldn't exist if an electron can't act on itself.  Perhaps a good physicist out there can explain. Also physicists have seriously considered that every photon of radio energy might somehow "know" where it is going to end up, wiggling some atom  maybe billions of years away in the future.    That would in a very bizarre way explain "radiation resistance". Updated 73w ago • View Upvotes