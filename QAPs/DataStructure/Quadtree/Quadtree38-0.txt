What kind of statistical analysis can be done of websites and web applications using publicly available data?
From a statistical graphing and visual analytics (exploratory data analysis (EDA)) perspective: You could run a squarified treemap algorithm in R using an open source package for word frequency in a web page. You would conduct a count of the number of instances for each term, then give the terms scores based on the depth of the term in on the page. Frequency of the term would define the size of the square for the term, and the rank of the term based on the depth score would be represented by the color of the square.   You can create a colorized heat map for the frequency of a particular term by domain. your X-axis would be terms, and your Y-axis would be domains, and you would set the color range by frequency of the word count.   You could create an O notation quadtree social node map in Gephi showing the popularity of trending tweets by hash tags as nodes with spatially clustered edges using a linear-linear model, Barnes Hut optimization, adaptive convergence, and logarithmic repulsion scaling. Size of the node could be by frequency of reponses to tweets all sharing the same hash tag (by user) and edges could be O(n) time between tweets (based on time stamp).   From an SPSS perspective: You could pull tons of publicly available data sets from the Amazon web services (AWS) cloud-based centralized repository, codify the data from any number of surveys related to websites and web applications, and conduct statistical analysis on them depending on the the type of data (nominal, ordinal, scale) and what you're trying to evaluate for. If you want to find correlations in interval data between independent and dependent variables, then you could create a Pearson R correlation coefficient matrix with an alpha set to 0.05 at 95% confidence interval to identify bivariate associations (strength and direction) based on the p-value (sig). - You could use the Hopkins Scaling of Correlation Coefficients to determine the strength of the relationship. If the data is categorical then you could check for associations through Chi-Square goodness of fit or difference of means through analysis of variance (ANOVA) using F-statistic and df to check for homogeneity or heterogeneity,  etc...   From a general web traffic perspective: Alexa.com will give you free web traffic and demographic data by domain, so you could do some trends forecasting as well - or from Google Adwords API signals as well if you want to see advertising revenue or volume variance for certain peak and non-peak hours of the day, and peak-days versus non-peak days, etc... 