What is an intuitive explanation of Huffman coding?
We have a document we want to compress: that is, we want to represent it with as few symbols as possible. Intuitively, how might we do this? We would like to exploit redundancy -- we want to make the codewords such that their length is inversely proportional to their frequency. (Note that redundancy plays a strikingly opposite role in compression in comparison to error detection, where we purposely introduce redundancy in order to communicate a message in the presence of errors.) Let's start with a concrete example: We want to compress the message 'aaabbc' with a binary alphabet. We want to ... (more)