How do you construct a Huffman code for a source with probabilities 0.05, 0.05, 0.15, 0.20, 0.25, 0.30?
I'll use list notation to represent the trees, and I'll write the weight of each subtree before the opening parenthesis for convenience. Start: 0.05,0.05,0.15,0.2,0.25,0.3 Combine two lowest-weight subtrees, in this case 0.05 and 0.05: 0.1(0.05, 0.05), 0.15, 0.2, 0.25, 0.3 Repeat! Now the two lowest-weight sublists have weights 0.1 and 0.15: 0.25((0.05, 0.05), 0.15), 0.2, 0.25, 0.3 This time, we combine the list with weight 0.2 with one of the two lists with weight 0.25. Let's take the first one to get a funky tree: 0.45(((0.05, 0.05), 0.15), 0.2), 0.25, 0.3 The last two iterations combine 0.25 with 0.3 and then the remaining two lists, so we get: 0.45(((0.05, 0.05), 0.15), 0.2), 0.55(0.25, 0.3) and then 1((((0.05, 0.05), 0.15), 0.2), (0.25, 0.3)) That's the Huffman tree! If we use 0 to denote left branch and 1 to denote right branch, the code words become: 0.05: 0000 0.05: 0001 0.15: 001 0.20: 01 0.25: 10 0.30: 11 The entropy of the source is H=−∑ p plog 2 (p)≈2.33  H=−∑pplog2⁡(p)≈2.33 bits per outcome. The expected codelength with the Huffman code is E[L]=  E[L]= 2∗0.05∗4+0.15∗3+0.75∗2≈2.35  2∗0.05∗4+0.15∗3+0.75∗2≈2.35 bits per outcome. So this code is approximately 0.02 bits shy of the theoretical optimum (which could be approximated more closely by bunching up source words in larger groups, and generating a Huffman code for that). 108 Views