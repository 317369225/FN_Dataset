What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
In theory there is no difference between theory and practice. But, in practice, there is. --Multiple Sources One of the things that irritates me is developers putting far too much weight on "theoretical/mathematical" constraints rather than practical and empirical evidence.   A good example is people thinking they are smart by being able to analyze algorithms based on the big-O notation.  Then taking this pure mathematical view and attempting to apply it to make programs run faster (optimizing the instruction level complexity)... Without considering the use case or accounting for physical constraints (memory access, instruction pipelining, cache lines, etc.)(i.e. a simple bubble sort will perform faster than a quicksort for small datasets sometimes up to 100 elements). Multi-core instruction pipelined CPUs/ GPGPUs and NUMA architectures has changed the landscape, and people are going to have to rethink what they mean by "efficiency" and get out of their "theoretical/mathematical" ivory tower. (Simpler, "wider" solutions capable of being vectorized/parallelized (with runtime complexity of O(n)) will crush single threaded/"theoretically faster" solutions with run-time instruction characteristics which are O(log(n))... TBH I'm thinking within 5 years people may just abandon this Big-O nonsense as it is becoming less and less practical in looking at actual real-world performance analysis. 