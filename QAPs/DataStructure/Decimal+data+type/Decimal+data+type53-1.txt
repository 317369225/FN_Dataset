What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
Computer programmers have traditionally assumed that hardware will continue getting faster. They also like to think that most algorithms can be parallelized to run much faster on many processors. In reality, this is not only difficult, but hits known limits. And if you get a small speedup, the code may become so complex and hardware-specific that this is not worth implementing and maintaining. Of course, sometimes parallelism is worth the effort. 