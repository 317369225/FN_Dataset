What's the hardest bug you've debugged?
For me, the hardest debug challenges are those where most of the effort is focused on writing the correct debugging facilities. Here is my story... The year was 2005 and a customer had asked me to debug a machine that was running Windows 2000 (SP4). The machine was crashing, displaying a Blue-screen (back then, Blue-screens were common) and wasn’t creating any dump files. To give you a better idea of what I was seeing, here is a snapshot from Welcome to Flickr - Photo Sharing which displays similar symptoms: (Taken from fogindex @ Welcome to Flickr - Photo Sharing) When debugging, it’s always a good idea to understand the history of the bug as well as the current scenario, so I asked the customer to give me more details. That’s where things started to get complicated. It turns out that the bug was presenting itself ONLY when the machine was tested in the field - and when I say field, I mean it literally: a field. The crash was occurring only after the PC was being driven through rough terrain, sometimes a day after and sometimes a week after. To further complicate the situation, when the customer added 4G of memory, things worked much better - the crash appeared every couple of weeks. When I asked what the heck they were running, I received the following reply: a 2GB Visual Basic application(!!). The fact that the bug was rare made it more difficult to catch and frustrating. For this reason, I generally advise users not to attempt their own work-around without first understanding the underlying cause of the problem. Ok, now it was time to get to work. So what did I know so far? There was a standard PC machine, running an unmodified Windows 2000 kernel with a 2GB Visual Basic application, that was crashing every 2-4 weeks while driving in a field. Well, it wasn’t exactly a good starting point. Debugging kernel panic with only four numbers… Looking at the Blue-screen images from previous crashes I saw two types of crashes: KERNEL_STACK_INPAGE_ERROR and KERNEL_DATA_INPAGE_ERROR. In both crashes the second parameter was STATUS_NO_SUCH_DEVICE. Reversing the Windows kernel, it was clear that the crash could have originated from only eight places, and in all of them the kernel tried to do a page-in (load a page from a cache to the memory) and failed. So the main challenge was to debug a crash that was happening once every two weeks and wasn’t leaving behind a dump file or any other debug information. How to debug such a crash? Well, a lot of debugging infrastructure needed to be written. One of the challenges was to display the logging information - since for unknown reasons the machine didn’t generate any dump file. I worked around this problem by replacing the Microsoft Blue-screen window with a “green” screen that displayed dedicated information. Here is a (real) example: Whenever a Blue-screen displayed, I redrew a “green” screen with a stack trace of the crash (Enumerate stack frame using EBP register chain), and displayed information on the active devices to see why I got “STATUS_NO_SUCH_DEVICE”. For the kernel hacker out there, the way I did this was by patching KeBugCheckEx (the kernel function which is invoked for every kernel-panic) using “code patching” techniques. Replacing the assembly bytes of the KeBugCheckEx from their normal function header:   0x55, // push ebp 0x8B, 0xEC, // mov ebp, esp 0x81, 0xEC, 0x74, 0x03, 0x00, 0x00 // sub esp, 374h into a jump call to my function:     0xE9, <Relative address>    // jmp MyBugCheckHandler The new jump invoked my function - “MyBugCheckHandler” - which displayed a green-screen using boot display API (e.g. InbvIsBootDriverInstalled). The function cleared the interrupt flag, avoiding any unwelcome context-switch. Since the function never exited, I could later take a digital camera and photograph my messages. Since “MyBugCheckHandler” is just a function, it can call other kernel API’s. One of the API’s that I have used is Plug&Play API, in order to scan all devices (of FILE_DEVICE_DISK type) and see which device has been removed. The result of the test was that for some unknown reason the hard drive (ATAPI device) was removed from the system, but I couldn’t understand when and why. It seemed like the drive was removed very early but the machine kept running. Only later did I connect the dots: since the customer had increased the memory to 4GB, there hadn’t been any paging activity until much later... So, I modified my humble tool and created “Atamon”. Atamon is a kernel debugger that runs inside the kernel and places breakpoints in strategic places within the atapi.sys driver and logs them for future use: The main purpose of the Atamon was to be able to read the ATAPI registers and display them. There wasn’t any other way to try to understand why the device was essentially committing suicide besides code-patching the atapi.sys driver. Digging in, using the Atamon, I saw that at some point the disk decided to lock and remain locked. No matter what atamon.sys tried to do, and no matter the amount of resets to the controller, the BUSY bit of the ATAPI simply never went down. Changing Atamon to toggle the power line GPIO and forcing HW reset to the controller was the only thing that solved the issue. Furthermore, the Atamon tool could identify, in the field, the exact time of the ATAPI disk crash and helped understand the physical conditions leading to the crash. And that, my fellow programmers, is the hardest bug I have ever come across. It totalled about one month of writing NT kernel-mode debugging infrastructure. Since then, I have fallen in love with my kernel debugger. I love it so much that I use it as a debugging tool for myself. Instead of using WinDbg (which stalls the entire system and can’t gather information on runtime) to solve the problem I have just described to you, using this tool to count/remember to invoke API while debugging proved to be useful. The only problem is that it’s not a generic tool and only I can use it. Hopefully one day I’ll gather the time and energy to release it as an open-source product. 