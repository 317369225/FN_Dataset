What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
Got caught up with the marketing buzzwords. For example like NoSQL. Although it is a highly sophisticated technology, there is still a little application for it compared to traditional SQL database and more often than not, programmers use NoSQL for problems that it is not designed to solve. Same thing with other buzzwords like Big Data and Cloud Computing. 