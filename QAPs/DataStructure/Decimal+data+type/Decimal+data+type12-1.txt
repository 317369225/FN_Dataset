Why does the size of 'a' equal 4 bytes?
In the first case the sizeof operator is being passed a variable of type char and char data type has size of 1 byte, thus the output. In the second case, the sizeof operator is directly being passed an integer value (the int value of 'a' in the ASCII table), which makes sizeof give 4 as the result. EDIT: To make the confusion clear that why isn't the ASCII value being used in the first case. sizeof is a built-in operator provided by the compiler. This means that it is evaluated at compile time rather than run time. Therefore when you say sizeof(a) the compiler knows that a is of type char and its size is 1 byte ( sizeof(char) is guaranteed to be 1 byte). However, when you say sizeof('a'), you are providing the compiler an expression, and the compiler needs to determine the type of the expression. The expression 'a' evaluates to 97 as per the ASCII table, and when you say sizeof(97), it will return 4. Note that since sizeof is a compiler built-in, it is hard to say how it works internally, it must be doing symbol table look ups at compile time to determine the type of the variables and provide the size of the types. However, if you are providing an expression to the sizeof operator instead of variable, the compiler will determine the type of the expression and return its size. Hope it makes more sense now. Updated 30 Apr â€¢ View Upvotes