Why do we need to use a BigInteger class in programming contests when a double data type can hold values almost up to infinity?
No, just NO. This is wrong at so many levels. Firstly, no finite machine can hold values "almost upto infinity", let alone a measly 64-bit or 128-bit data type. Secondly, integer types and floating-point types are conceptually different. They work differently. If your application needs one, you can't just substitute it with the other. That's a bit like saying "I couldn't find my laser pointer, so let me just use my TV instead, after all they both emit light". Consider this: a 64-bit double can hold at most 2 64   distinct values. There's no way you can get around this. Typically, the set of real numbers a double can represent is something like this: pick a sign s, 1 or -1, pick an integer x in a certain range, pick another integer y in a certain other range, and take s*x*(2^y) (this is not exactly true, I'm glossing over infinity and NaN and denormal numbers... but never mind). You'll get large enough integers by making y large, and then you'll have, for example, integers n such that n and n+2 can be represented as a double but n+1 cannot be. If your application calls for exact computation with integers, this is not the sort of behaviour you'd want. 