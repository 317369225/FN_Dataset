What are the differences between the double and float data types?
Originally Answered: What is the difference between a a single-precision floating point number and a double-precision floating point number?On a typical system, a single-precision takes 32 bits of memory, while double takes 64. The double is capable of both more range and greater precision. Range: The single floating point number has an 8-bit exponent part, meaning it can represent as small of absolute values as 2^-(2^7) and as large as 2^(2^7), excluding 0 and infinity. The double floating point number has an 11-bit exponent, so it can go from 2^-(2^10) to 2^(2^10).  That's 3 extra orders of magnitude both up and down that you can represent by using a double instead of a float, at the expense of taking up twice as much memory. Precision: The single floating point number has a 23-bit mantissa. In binary, it means you can get up to 23 significant digits.  In our more traditional decimal system, that translates to about 7 significant digits -- that is, you can represent the difference between 1.000000 and 1.000001, but if the difference is smaller than that you can no longer accurately represent it. (Note:  This isn't perfectly accurate, but it's a "close enough" approximation for the purpose of illustrating the point). The double has a 52-bit mantissa.  This translates to ~15 significant decimal digits. So it can more finely pick out small differences -- 1.00000000000000 and 1.00000000000001 can be distinguished from each other, while they could not in a single-precision representation. There's also necessarily some difference in how the hardware must interpret the two, as for older systems that may have to go through extra steps to do double floating point handling versus single, but I'm pretty sure most modern 64-bit computing systems have dedicated hardware for both so they perform equally. 