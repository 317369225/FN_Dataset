What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
I've been around the computer industry for more than 30 years, and I "didn't get the memo" about this change in terms. Most people don't use the term "kilobytes" anymore, because quantities that small are rare these days, but techies I know all use "megabytes," "gigabytes," and "terabytes," and we understand it to mean a power of 2. A kilobyte is not 10^3 bytes. It's 2^10. This is the first time I've heard the term "gibibytes," and such, though looking at the Wikipedia article on Binary prefix, I see where this is coming from. I have seen the abbreviation "GiB," but I had no idea what it was at first, though I eventually figured it was referring to what I understand as "gigabytes." The naming scheme is arbitrary, much like every other name assigned to technology in the field. See Mibi Mega Kibi Kilo - Decimal and Binary prefixes. All "kibi" is is the first two characters of "kilo" combined with the first two characters of "binary." This is like how the term "bit" was derived, from the terms "binary" and "digit." "Kilo" is from Greek, meaning 1,000, or 10^3. Just renaming "kilobyte" to "kibibyte," and saying "kilo is base-10," and that "kibi" is "kilo in binary," in an effort to be more exact defeats its own purpose, in my mind. 1,000 in base-2 is 0x3E8, and that's not what anyone is shooting for. Just transferring the base-10 exponent to base-2 doesn't make sense, either. 2^3 is not the quantity we're talking about! We're talking about 10000000000 in binary (2^10). I suspect the reason people came up with the term "kilobyte" in the first place is it's roughly what we're talking about when the number is converted to base-10. It's not exact, and wasn't intended to be. I don't think you're necessarily going to get anywhere by being the "language police" on this. The terms as I've used them have been in common use among people in the industry for longer than I've been alive. The term "byte" itself is just a made up term, as far as I can tell, along with "word" (a standard bit length, though usually refers to 2 bytes), "nibble," which is "half a byte," or 4 bits. Even the term "Google" is "misspelled." It came from the term  "googol," which represented 10^100 (and was thought up by a mathematician's nephew). As for anything else computer programmers get "wrong" with technology, the only thing I can think of is people's conception of "object-oriented." Alan Kay has said: Actually I made up the term "object-oriented", and I can tell you I did not have C++ in mind. Java and C++ make you think that the new ideas are like the old ones. Java is the most distressing thing to hit computing since MS-DOS. He's said something similar about Objective C, saying it's not OO. This is a widespread phenomenon, since CS academics have the same notion of OO as a lot of programmers in the field do. The basic problem with the typical notion of OO vs. what he had in mind is that his notion was really about message passing. Objects were just endpoints for messages. The conception of OO that's been promulgated is that method invocation is tightly bound to the point of dispatch. In his conception, method dispatch was "owned" by each object. An object would just be minding its own business until it received a message, and even if it received one, it need not do anything with it, not even call a method. It could refuse, for example, if the message came from a source it did not recognize, or if it did not have permission to access the object's features. An important aspect I'm implying here is that objects and methods were late-bound. In the popular conception, they are bound early. This is not an insignificant difference. In his late-bound environment, it was possible to modify classes while a thread that used them was running, without interrupting the thread's execution. Another popular notion is that objects are the abstraction. That's not what he intended. The abstraction was supposed to be what went on between objects--in the messaging. A third popular notion is that objects are known by tags, which are deemed their types. In his conception, OO could be strongly typed, but the types were interfaces, not tags. Kay has said many times over the years that he's regretted coming up with the term "object-oriented," because people got the wrong idea from it. I think he's said "message-oriented" is more accurate for what he was talking about. The natural question from this would be, "Well, what is C++, Java, Objective C, etc., then?" C++ and Java institutionalize the notion of abstract data types, which is an improvement on procedural modular design. Obj-C/Cocoa does embody the notion of message passing. So in my mind it's a little closer to what Kay had in mind for objects than C++ or Java, but it distracts from the idea by allowing memory references that are unsafe (which can cause a program to crash, and data to become corrupted), like in C, and C++. Obj-C also interferes with the notion that interfaces are the primary types of concern, because it brings in C's notion of types, which are structure tags, and makes them primary. I am being a bit misleading in saying "his conception." This was his conception when Smalltalk was being developed in the 1970s. I've kept up a bit with what Kay has been doing over the last 7 years, and he's indicated he's improved on the concept of objects I've just described by no longer focusing on message passing, but using more of a "pull" model that has objects functioning more like cells in a spreadsheet. People can read more about it at Viewpoints Research Institute. Updated 1 Sep • View Upvotes • Asked to answer by Aaron McClellan