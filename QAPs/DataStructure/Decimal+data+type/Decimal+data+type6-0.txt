Why do we not have a data transaction system that transfers data in forms of decimal systems rather than binary (0 and 1) in computers? What is the feasibility of having such type of data transaction?If we can do so, the processing efficiency increases and also the memory usage reduces.
At a very low level binary is used to represent integers, decimals, and text. One byte can represent the integers from 0 to 255. Two bytes can represent the integers from 0 to 65535. The formats are very easy for computers to understand, but not as easy for humans. There are systems that transfer data in what appears to include decimal the decimal system. These systems include XML and CSV. These systems make more sense to people but in order for computers to understand them they have to read (parsed) and converted into binary forms. So processing efficiency goes down and memory usage goes up due to the parsing. The underlying problem is that on the chips, logic gates (AND, NAND, OR, NOR) work on electric signals and todays computers only have 2 states (high and low). In order to work directly on a decimal system we would have to change computers to work with at least 10 voltage levels. This would require a complete redesign of all chips, circuits, and devices. But it should lead to a 10x performance increase. Which sounds great, but how long would it take? 5 years, 10 years? According to Moore's law (Moore's law) the density of transistors on a CPU doubles every two years, and computer speeds do much the same thing. So we get a 10x performance increase about every 6 years anyway, without a complete re-design. 