Why do computers use base-2 instead of base-10?Decimal computers were once very commonplace, but why aren't they anymore? *Speaking as something with next to no knowledge of computer science.*
Digital computers are inherently binary beasts.  Whether you build them out of relays, vacuum tubes or transistors, the simplest logic circuits to build understand two primary states:  On and Off. There were ternary digital computers:  computers built around three digital states.  Theoretically, these machines provided more optimal compute density.  But, ternary computing never really took off.  (The fact the earliest successful ternary computers was in the Soviet Union may have been a factor.)  You can find links on ternary computing on Wikipedia:  http://en.wikipedia.org/wiki/Ter... Ignoring ternary computers, the rest of the digital computers are all binary.  They represent numbers as combinations of 0s and 1s. What about decimal computing?  The earliest computers were taking over roles previously filled by humans and decimal adding machines.  Prior to the advent of electronic computers, financial arithmetic was performed in decimal.  Any time a value needed to be rounded, it was rounded at a decimal boundary. So, early binary computers implemented decimal arithmetic to fit in to that world.  Binary Coded Decimal (BCD) is one format that survives to this day.  It encodes decimal values 0 - 9 in groups of 4 binary digits.  Machines performed decimal arithmetic on these binary-coded values, rounding at the decimal digit boundaries as would anyone doing the arithmetic by hand. This never actually died. In the modern era, the IEEE 754-2008 floating point standard defines radix-independent floating point, with special attention to decimal floating point.  And IBM POWER processors, if I'm not mistaken, provide hardware support for decimal floating point.  The current crop of GCC compilers also provide extensions for decimal floating point arithmetic. Decimal computers never died.  They live at your local bank, doing fincance computations. The scientific community, however, had no need for decimal rounding.  With a native binary representation, you can represent far greater precision in the same number of bits.  So those doing nuclear bomb simulations, weather simulations and so forth pushed forward the development of native binary floating point computation.  Basically, unless you need to perform rounding at decimal digit boundaries, there's no need to use decimal arithmetic.  So, outside of certain finance calculations, the rest of the world took advantage of more efficient binary representations. And thats why decimal floating point arithmetic is less common than binary floating point arithmetic.  Both still exist and both are still fairly widespread; however, binary floating point is orders of magnitude more common than decimal. 