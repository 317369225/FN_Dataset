What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
In 35 years in the computer field, I've noticed these things that even (and odd) programmers do wrong. These apply to some, but obviously not all. They fail to: Write a grammatically correct English sentence. Trust me, this is a very technical act. Understand that users do not understand the internals of the code.Â  Thus they fail to test behavior that is illogical from the codes point of view. Understand the difference between 'it compiles', and 'it runs without crashing', and 'it works'. Consider and handle all edge cases. Write neat, well-organized, readable code. Initialize variables. Ship the exact same build that was tested. 