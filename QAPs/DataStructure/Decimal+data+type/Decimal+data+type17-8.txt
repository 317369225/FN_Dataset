What are the differences between the double and float data types?
Originally Answered: What is the difference between a a single-precision floating point number and a double-precision floating point number?A single precision floating point number uses 32 bits. Usually of those bits is the sign bit, 8 are the exponent, and 23 are used for the number portion (mantissa/significand).  It actually cheats and gets one extra mantissa bit, because it knows that the value always begins with 1 (so it assumes there's a 1 on the beginning).   A double precision floating point number uses 64 bits: 1 sign, 11 exponent, 52 mantissa/significand (+1 extra bit again). 