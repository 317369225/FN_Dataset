What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
Before giving examples of things programmers get wrong, I just want to point out that the wording of the question betrays  an odd assumption.  "things that EVEN programmers get wrong" seems to imply that programmers are so masterfu that they never make mistakes.  Programmers are only human and make mistakes and get things wrong every day. Generally, debugging is a major part of programming.  This proves that programmers often get things wrong, since they write buggy code. On another note, programmers frequently are wrong about how the user will want to use the program.  Thats why "UX" became a speciality. 