What are some technological things that even computer programmers do wrong?I will get things started. Kilobytes (KB), Megabytes (MB), Gigabytes (GB) are used incorrectly. These are human (base-10) numbers, so each represents 1,000. Computers use binary (base-2), so each should represent 1,024. The proper notation is Kibibytes (KiB), Mebibytes (MiB), Gibibytes (GiB). The same applies for bits.
Treating error processing as an afterthought, something that will work itself out or doesn't need more than a message somewhere. Most software is written mostly blue-sky other than perhaps noting that a file is missing or similar. Unfortunately that's human nature, we rarely swing a hammer on the assumption that it may hit our thumb. But it's distressing, for example, to see a program segfault (exit disastrously and out of the control of the program) because it was fed a file with an unexpected format. Or not be able to back out of a partial computation that can't go on, for example modifying files only to find part way through something is missing or wrong and just exits leaving a mess for someone to clean up manually when it could have been done to temporary files and only committed at the very end. Admittedly the trends towards try { } catch {} finally {} and of course commit/rollback are good when used. It is not unusual for a program to have more error processing statements than computational statements. But many would consider that a mistake, too much effort spent on error handling. That, my friend, would be an error. 