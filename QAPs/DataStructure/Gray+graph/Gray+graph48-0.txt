How does information theory relate to image processing?
How does information theory relate to image processing?Images can be considered as simple channels. So the theory is exactly applicable to images. It is useful in compression analysis, auto-adjustment control (for example, auto-focus and thresholding), blur/noise reduction, image reconstruction and super-resolution algos (and many, many others). In most cases image entropy provides metrics/point estimates (with specific properties like scale/permutation invariance) for use in these algos. if it is useful/common to express image processing in terms of changing the associated information in the imagesObviously. It provides a measure of quality to say the least. Does this mean each filter has an associated information attenuation factor? Yes, it does (in the frequency domain representation of an image). Images have a power value associated with them, and filters have gains (and scalloping factors/losses) - these work the same as in digital signals. if images A and B contain X and Y bits, how many bits would A & B contain? A | B?These are considered under conditional and joint entropies and mutual information operations. For example, the entropy of a grayscale image of N  pixels can be described by the usual Shannon entropy: H=−Σ i p i logp i   , where, p i =N i /N  , and N i   represents number of px with the i th   gray value (p i   captures the same frequency index as that obtained from an image histogram). This is not the only definition though. There's Rényi entropy as well (which generalizes a bunch of other definitions of entropy). [Dave Gordon's answer has nothing to do with information as interpreted under information theory. That answer mistakes interpretive graph/scene semantics extraction and channel content for 'information'. And there's a huge breadth/depth of research on all these topics] 