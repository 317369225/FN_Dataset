Can someone give me an example of a programming problem that you would get in an interview?
How does system noise or jitter affect the critical path of a program? n  nodes communicate in a 1d stencil pattern:  each node i  computes and then sends a message to adjoining nodes i−1  and i+1  , then waits (if necessary) for messages to arrive from its neighboring nodes before computing the next timestep and repeating the process.  The communication network is a torus, so (for example) node n−1  communicates with nodes n−2  and node 0  . Assume communication is instantaneous (but communication cannot start until the associated computation is completed).  Computation takes unit time, but there is an additional amount of "noise" or "jitter" added to the computation drawn from a probability distribution of your choice. Given n  nodes, t  timesteps and the noise distribution p  , what is the distribution of critical path lengths through the directed acyclic graph that defines the execution? As an example, the image above show computation on each "rank" (aka "node") as a horizontal black arrow and the messages are shown as gray vertical arrows.  The red line traces the critical path for this instance back from the last rank to complete through to the first timestep. (I give this question to postdoctoral candidates to get an idea of how they confront problems outside of their comfort area.  I've given it to a few undergrads as well.  Writing a simulator and determining the answer empirically is the easy answer.  I'm still looking for a general analytical answer.) 