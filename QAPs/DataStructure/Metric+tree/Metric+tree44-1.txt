Natural Language Processing: How would you programmatically parse a sentence and decide whether to answer "that's what she said"?
Terminology For the purposes of this answer, I will abbreviate the sentence "That's what she said" as TWSS, I will refer to sentences for which TWSS is an appropriate reply (i.e. the sentences we're interested in classifying) as prompts, and I will refer to the task at hand as the TWSS problem. Training Data The first step is to collect a training corpus.  In my opinion, the best source of data for this problem is probably Twitter for the following reasons: TWSS-prompt relationships are structured.  It's really easy to identify prompts because @-reply tweets are linked to the tweets to which they are responding.[1]  These are precisely the sentences you want to make up your training set.  In the case that a TWSS appears in the middle of a non-reply tweet, then you can usually assume that the text preceding it in the same tweet is the prompt. It's current.  Most existing text corpora are old enough that it's unlikely that TWSS shows up often enough in them to observe any patterns in usage.[2]  Moreover, Twitter is more colloquial and conversational than most other text corpora, which largely use more formal documents like news articles.  One exception I can think of is the Switchboard corpus, which consists of transcriptions of about 2000 telephone conversations[3]; however, it too probably won't work for a task involving TWSS because of its age (early 90s — is that late enough?). It's huge.  There is a crapload of data available, and it's growing all the time. Another option would be to use a recent corpus of instant message transcripts, but I'm not aware of any that are publicly available or easily accessible.[4]  If you did use a corpus of instant messages, you wouldn't be able to pinpoint the prompt as precisely as you can with Twitter, but if you include a few of the messages immediately preceding an appearance of TWSS, I think that should suffice.  (The exact number of preceding messages to include would probably depend on either manual inspection of the data for a prima facie choice or experimentation with increasing numbers that ceases when you start seeing precipitous drops in accuracy.) I disagree with Ryan Waliany's answer that Wikipedia would be a good corpus to use.  Not only does TWSS rarely appear in all of Wikipedia, but it's much more difficult to tell what the prompt is for any given appearance of TWSS because of the prose nature of Wikipedia.  Moreover, because Wikipedia is encyclopedic, it probably has some contrived formal sentences containing words that you would never see near a TWSS in a conversational context, e.g.: "That's what she said", occasionally abbreviated as TWSS, is a chiefly American and Canadian phrase used in response to an accidental double entendre, especially one that is sexual in nature. Typically, 'TWSS' is said in response to another speaker's double entendre, and rarely by the original speaker.[5] The disadvantages of using Twitter as the primary source of training data are: No prompts longer than 140 characters can be used to train.  Whether this matters a lot depends on how long TWSSes tend to be in the wild and on what features we choose to train on (e.g. a model trained on phrase trees will probably not deal well with a set of prompts that are longer than any in the training set, while a model trained on bag-of-words will still do okay), but I think even if performance does suffer for prompts with more than 140 characters, that's a fine trade-off to make for the sake of getting some model working. You'll need to spend time cleaning the data.  Even though the relationship between prompts and TWSSes is pretty clean, the tweets themselves are very likely not well-formed English.  Assuming that what you're attempting to classify in the end is a well-formed English sentence, you'll almost certainly want to strip out all "lol"s, "lmao"s, etc. since they often appear in TWSS-related tweets but wouldn't in any of the test sentences.  There's also a lot of text speak[6] and misspellings. Bad jokes.  People think they're funny, but sometimes their jokes just make no sense.[7]  If all our training prompts were bad TWSS jokes, then the resulting classifier won't perform very well according to what I'd like to think are more refined standards of humour.  Perhaps we can bootstrap out set of prompts by starting with a seed set of prompts drawn from best-of lists for TWSS jokes and finding users who responded with TWSS to prompts that are similar[8], then finding other tweets that that person responded to TWSS with, and iterating.  This would probably take a long time, unless we get lucky and run into several TWSS fanatics who like to drop a good ol' TWSS joke every chance they get.  Maybe it's worth it though — I don't know without manually inspecting the data we would get otherwise. Our training set should also include sentences that don't make good TWSS prompts.  Ideally, the training set would have prompts and non-prompt sentences in roughly the proportion that they appear in the wild, but this may not be feasible or tractable, in which case we can use case-controlled sampling in combination with a compatible learning method. Learning Method and Features Honestly, I think something as simple as training a Naive Bayes classifier with a bag-of-words model will get you most of the way there.  Most TWSS prompts are characterised by use of a small set of tokens that afford double entendres, like stiff, hard, long, big, small, stick, hole, fit, eat, suck, lick, ride, bang, come (and variations thereof, e.g. came, cum, and coming), blow, mouth, hands, and squeeze.  The inclusion of any one of these words in a sentence is usually enough to make it into a reasonable prompt, even if other words have little to do with sexual innuendo.  If your data set is large enough, I'm willing to bet that enough prompts will fit into this category to make training purely on unigram appearances (minus stop words) a satisfactory solution.  If this turns out not to be the case, it's trivial to bump up to bigrams and trigrams to improve sensitivity to context, although this may bring down our perception of the classifier's recall. Why might perceived recall go down?  Well, an interesting side effect of this entire enterprise is that whatever classifier you create will inevitably end up creating new TWSS jokes.  Think about it: the nature of TWSS jokes is to invent new, originally unintended meanings, so to make one requires that the prompt's meaning contain a little room for ambiguity.  A large number of sentences that go un-TWSSed would actually make great TWSS jokes.  Thus, while our model may correctly classify a "non-prompt" sentence in our test set as a prompt, we may look at it and think to ourselves, "Well, actually, this would make a pretty great joke after all!"  As a result, it might actually be a good idea to opt for better recall at the expense of precision since our test set might actually label some good prompts as non-prompts just because no one had yet been clever enough to respond to them with TWSS. If, however, we don't care about this issue, then a good choice for a more involved approach with potential for higher accuracy would be ensembles of decision trees, which will still run fairly efficiently despite the large feature space, although you'll definitely need to apply some pruning to get rid of what is sure to be a huge number of irrelevant features.  You could also try using support vector machines, but it would take fucking forever to run.  One possible strategy is to slice down the feature space by using cheaper techniques like Naive Bayes or decision tree learning to find a set of features that are highly indicative of prompts and then using just those features for the SVM.  You could also get a reduced feature set by running a quick tf-idf on prompts vs. non-prompts. Going beyond just using n-grams as features, I also thought about using POS (part-of-speech) tags, but I don't think you'd get too much of a boost from these.  What might help is if you parse sentences and build phrase structure trees so you can see whether key verbs like ride, suck, or blow affect an antecedent-less it, which I think is a common feature of prompts — the ambiguity of the it lends itself to being repurposed for sexual innuendo.  In fact, just generally training on the object of sentences where the verb is one typical of prompts would probably help a lot.  I'm willing to bet that certain nouns like bananas or popsicle sticks show up more frequently as objects of verbs like ride and suck than they do in other sentences.  The bag-of-words model wouldn't pick this up; bigram and trigram models might, but phrase structure trees would definitely be the most consistent way to catch these connections. Conclusion Anyone want to do this with me and see how much of my idle speculation turns out to be true?  It will be fun and profitable (not really). Endnotes [1] That is, if the replier used the reply feature properly instead of just typing "@screen-name".  In any case, it's easy enough to ignore any @-reply tweets that don't link to a specific tweet. [2] The assumption here is, of course, that our ancestors were a whole lot stodgier than we are.  Feel free to correct me if you have evidence that previous generations were just as vulgar and enthusiastic about puns as we seem to be. [3] http://www.ldc.upenn.edu/Catalog... [4] I think I once heard of an MSN chat transcript dataset that was really awesome, but I can't seem to find mention of it anymore.  Let me know if you know where I can find that or any other instant message datasets.  I know that some IRC rooms get publicly logged — is there a single place where one could grab all of them at once? [5] http://en.wikipedia.org/wiki/Sai... [6] e.g. "slap it 1 mo time 4 me" (visible with its corresponding TWSS at http://twitter.com/#!/Blaxican_B...) [7] E.g.: http://twitter.com/#!/BNicole_/s... and http://twitter.com/#!/DomOfTheYe... [8] Let's assume we have some reasonable metric for similarity.  We could use something like ensuring a minimum edit distance below some threshold or number of non-stop words that two sentences share above some threshold. 