What are the advantages of using a decision tree for classification?
Here's a high level, quick and dirty mental picture to help get your head around the differences: Imagine the labeled training data plotted on a two-dimensional grid.  A decision tree will effectively subdivide the grid into increasingly fine-grained "boxes" around your data.  Some boxes will have label A, others label B. Most other classifiers -- like the most common alternative: the support vector machine -- will effectively draw a line to separate your data.  When I say "line" I really mean some potentially complicated function (thanks to the kernel trick) that could be a parabola, arbitrary polynomial, or even a circle.  But at the end of the day, any data on one side of the line is labeled A, and the other side labeled B. So it really depends on how your data is arranged (which may be harder to visualize in higher dimensions).  If they're arranged in "pockets" or mini-clusters, then a decision tree makes sense.  If they're easily separable by some crazy squiggle, then an SVM makes sense. 