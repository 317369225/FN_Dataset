What are some Machine Learning algorithms that you should always have a strong understanding of, and why?
A lot of the answers in this thread are certainly very good and provide lists of specific algorithms (or propose a pet perspective as the panacea) and this is probably what the OP wants. However, I would like to advocate a different point of view - it is more important to have a solid understanding of some of the theory before getting married to individual algorithms or pet areas. Here is one opinionated list of how to go about having a strong understanding of machine learning - Why the rush after all? Say you want to be a musician - you can whet your appetite and start by mastering a few songs, but sooner or later, you will want to concentrate on the fundamentals. So I'll survey a few disciplines in which it would be nice to have some exposure to. Data is messy, noisy and reality is often unknown. Probability theory is the basic tool to reason under uncertainty. You can get some mileage by learning to construct probabilistic models, understand Bayes theorem and know how to do inference. As a bonus, you can learn about probabilistic graphical models and learn the art of viewing Machine learning problems as latent variable models, algorithms for exact inference and methods of approximate Bayesian inference (sampling, variational methods etc.,). Far too many people stop here and view everything from a strict Bayesian perspective, may be adopt some non-parametric Bayesian techniques for extra brownie points. However, that would be missing out a lot. Understanding generalization deeply is very important. This is because, supervised learning is not memorizing examples, but learning generalizable patterns.  Some rookie mistakes in machine learning stem from an algorithm-first approach and not clearly understanding over-fitting and generalization. I recommend learning the basics of Statistical Learning Theory ( aka Computational Learning Theory) which unites various perspectives in supervised learning. Whatever your weapon is (be it Bayesian methods, convex techniques, applying SVM to every problem, neural nets, topological data analysis etc., ), ultimately in every supervised machine learning problem, you  construct a model and balance model fitting error and model complexity. At the end of it, you will (i) have a richer perspective say when you argue about Naive Bayes vs SVM, (ii) actually be able to quantify how the number of samples needed scales with model complexity etc., (iii) and as a bonus, if you learn Information theory, you can also know the ultimate limits of learnability. To this end, I'd suggest taking the pedagogical approach in, for example, Foundations of Machine Learning by Mehryar Mohri. Also, check out Learning from Data by Yasser Abu-Mostafa for a gentler under-graduate introduction. In the age of "Big Data", it is important to be able to reason about algorithmic complexity and at the least be able to tell scalable (which is almost always synonymous with linear) algorithms from more complex algorithms. You may have a fantastic model, but what if it is computationally hard? Can you develop something that works most of the time. Take a course in algorithms understand how to design and analyze algorithms, hardness and various reductions, greedy techniques, memoization etc., Optimization algorithms are a useful addition to your toolset as a lot of supervised methods easily translate to minimizing an objective function which is combination of a loss function (to encourage model fitting) and a regularization (to control model complexity). When this objective function is convex, you have a lot of theory in Convex Optimization and efficient algorithms at your disposal and you can design and analyze provable machine learning algorithms and provide guarantees about algorithmic and statistical complexity. When it is not convex and even NP-Hard, convex relaxations still work very well and in recent years, we have been gaining an increasing understanding of this phenomenon. I would especially recommend learning about sparsity, L1 minimization, atomic norm etc., So far, we have been concentrating on learning from examples. Finally, unsupervised learning or discovering patterns is another theme in machine learning. This is basically clustering and learning mixtures (and includes algorithms like k-means++, topic models, matrix factorization techniques, spectral clustering etc., that have some new theoretical guarantees) and dimensionality reduction (which is currently more an art than science for the non-linear case). I'd recommend also looking into some semi-supervised methods to exploit both labeled and unlabeled examples, reinforcement learning (to learn from feedback, for ex, bandit algorithms), online learning (to continually learn), active learning (chose what to learn when under a budget of training examples). After learning the fundamentals and gaining a good overall perspective, it doesn't matter whether you lean towards Deep Learning, Bayesian Methods, Optimization or something else. You can do great by picking from any suite, and learn them fairly quickly. Updated 104w ago • View Upvotes