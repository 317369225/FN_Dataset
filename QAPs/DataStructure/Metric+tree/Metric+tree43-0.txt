How do I decide which features to use in a neural network model?
The biggest mistake you can do in variable selection is limiting yourself to only one approach. The first thing you should do, if you haven't already, is picking an evaluation metric. Think about what metric makes sense for this problem (plain accuracy, F1 score, ROC, log loss, precision, recal,...) and stick to it. Then test different variable selection methods and see what works best on a validation set. Remember, you want your model to generalize well, so use a validation set to measure preformance. Now you can try all the different variable selection approaches and compare the quality of predictions side by side. I usually try: -LASSO -variable importance generated by trees (I prefer Random forests) -univariate (chi square, for example) -stepwise selection -dimensionality reduction techniques like PCA or SVD -last but not least, the most important approach, where applicable: domain knowledge At the end, you may consider a combination of those. All of that being said, nowadays there are so many very strong algorithms that are lightening fast and inherently deal with variable selection, correlation, missing data and categorical variables that I usually end up figuring out that those outperform all the other things I listed above. For example, try Gradient Boosting Trees. I use the Python implementation of XGBoost. It leverages multiprocessor approaches, is very quick, and in 90% of the Kaggle competitions I've participated in, outperforms linear models, neural networks, SVM, Random Forests,... 