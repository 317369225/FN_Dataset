What are the advantages of using a decision tree for classification?
I would say that the biggest benefit is that the output of a decision tree can be easily interpreted (by humans) as rules. I wouldn't be too sure about the other reasons commonly cited or are mentioned in the other answers here (please let me know if I am wrong): Ease of coding - I agree this is relatively easier to code - but things do get complicated once you account for pruning to avoid overfitting. And lets face it - no DT algorithm is practical without some means to eliminate overfitting. Even if you look at the original CART algorithm, the pruning mechanism suggested takes some getting used to. Addressing non-linearity, inferring interaction terms etc - We have a bunch of non-linear classifiers available to us today, and in this regard DTs are not special anymore. The only thing really still special about DTs are that they can explain the non-linearity in an intuitive manner - goes back to what I said before about the convenient interpretability of the output of DTs (some interesting points made by Peter Flom in comments) Fast prediction - this may or may not be true or matter: depends on your dataset and alternatives. If you have a SVM classifier for your problem and a kernel that is cheap to compute, then all you would be computing during prediction time are the kernel values wrt each of the support vectors; this may not be much of a drain on resources. I will admit, however, that in most cases DT prediction is very fast. Constructing a DT is fast- this is true. But the trade-off here is DT algos are greedy - they search through only some of the possibilities in a relatively larger hypothesis space. You may have another classifier for your dataset which is not as quick to train, but is more accurate. The one downside DTs have is they are high variance classifiers i.e. the DT learnt is sensitive to the precise layout of points and, if you have less data, can fit to noise. Here is a diagram that shows the performance of C4.5 (a specific kind of DT) vs Naive Bayes, on data with a non-linear separation boundary: Note how till a 1000 examples or so Naive Bayes outperforms DTs because DTs overfit the training set. (Source: Page on washington.edu) Of course, high-variance of DTs can be addressed, and is usually done so with ensembling. Updated 82w ago â€¢ View Upvotes