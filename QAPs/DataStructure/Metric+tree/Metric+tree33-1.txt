When should I use Boosting instead of SVM?
This post includes some minor points and simply adds to what Jack Rae (+1) has already answered. The crux of the answer as mentioned earlier is: Let the data decide. Addendum: 1] Small Dataset Given enough points and a universal kernel, SVM theory predicts very small generalization error. How many points are enough points? Well, if we plug in some reasonable error rate, the number of points given by the theoretical error bounds are something in the order of million points. This is practically infeasible. However, this large number is mainly due to weak error bounds. We know from practice that kernels like RBF kernels do work well even with smaller number of points. However, by using a universal kernel like RBF on a small dataset, we run the risk of over-fitting. Whenever the dataset is very small, it is better to use simpler hypothesis like linear, quadratic or cubic kernels. However, simpler kernels may not play well with the dataset you might have. In such a case, you may resort to Boosting. 2] Class Imbalance SVMs usually don't work well when there is large class imbalance (large proportion of points with label1 versus small proportion of points with label2). libSVM and SVMLight provide class-weight options (which are in turn used to weight the instance constraint errors) to mitigate around this problem. However, it does not necessarily work well in all class imbalance scenarios. Again, in such a case, resorting to boosting can provide benefits. 3] Dataset Shift Consider a scenario of image classification where the images are coming from a security camera. The images from say Day 1 to Day 5 are labeled and are used to classify images from Day 6 onwards. It is highly possible that the joint distribution P(X, Y) of the training data might be significantly different from the test data distribution. This is known as the dataset shift setting and current classifiers like SVM inherently assume that the train and test distributions are identical. Hence, don't usually work well in these scenarios. Again, boosting (after some calibration [1]) can be useful. [1] http://www.machinelearning.org/p... 