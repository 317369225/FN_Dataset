How do random forests and boosted decision trees compare?
see my answer to: When is a random forest a poor choice relative to other algorithms? The prediction power of random forest over decision/regression tree mostly lies in the boostrap samplings in both examples and features during training and ensemble average in the end. The first was also adopted in stochastic gradient boosting, the second has its pros (parallel) and cons (less efficient sampling) compared to boosting. Jerry Friedman did a bunch of empirical studies and theoretical analysis to compare both approach. e.g in his Elements of Statistical Learning book (data mining, inference, and prediction. 2nd Edition.) chapter 15 (page 589-591), it showed the benchmark results on three different data set with boosting winning with large margin on all of them. Â  Also for random forest to perform well, it typically require deep trees (level >=7). While Boosting typical work better with shallow trees (5-15 leaves). Boosting has a clear speed advantage here. 