What are the advantages of using a decision tree for classification?
I'll add that one of the large benefits is that you don't need to worry about normalizing the data. For long-right-tailed predictors, you often need to log it before you can input it into a logistic regression. You don't need to do the same for decision trees since it generally just cares about the order of the predictor data. Additionally, decision trees combined into an ensemble create some of the best binary classifiers. From a paper comparing the predictive power of binary classification algorithms: With excellent performance on all eight metrics, calibrated boosted trees were the best learning algorithm overall. Random forests are a close second, followed by uncalibrated bagged trees, calibrated SVMs, and un-calibrated neural nets. For the rest of the paper, see Page on cornell.edu 