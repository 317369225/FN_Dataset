Why is the fundamental theorem of calculus nontrivial?
A way of presenting the two parts of the theorem, which is essentially done in some textbooks before 1950, is as an equivalence. Assume [math]g[/math] is continuous on an interval containing [math]x=a[/math] and let  [math]f[/math] be a function defined on the same interval. Then [math]f(x) = f(a) + \int_a^x g(t) \, dt[/math]    iff    [math]f'(x)=g(x)[/math] for all [math]x[/math] in the interval. The "only if" part (former implies latter)  is "trivial": its proof requires only a calculation using definitions (definite integral, continuity, derivative—hence limits). The "if" part (latter implies former), which is what the question mentions, is nontrivial. On the face of it, you see that the indirect information about the function [math]f[/math] provided by its derivative at every point, as well as its actual value at a single point, can be used to simply and precisely recover the value of the function everywhere. The proof requires (somewhere, depending on the choice of proof) the Mean Value Theorem, the workhorse theorem of elementary calculus. The standard proof of the MVT uses the Extreme Value Theorem, whose proof is in turn unequivocally deferred in elementary calculus because it requires thinking and tools that are beyond that level. One can say, in short, though, that it invokes the deeper analytic property of the real numbers called completeness. I think it's a stretch to see the relation given in the question as "just the definition of the derivative backwards," though I am not even sure what that means. To characterize it as such would betray either profound intuition coupled with apparent lack of appreciation of definitions or somewhat reckless naivete. So my answer to the question is "no." 