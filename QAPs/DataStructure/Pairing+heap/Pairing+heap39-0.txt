What is the fastest way to find the number of common strings in two files?
It depends on how big the files are. If an appropriate in-core representation is smaller than available memory you can read the files, hash the strings to a pair of bits indicating presence in each source, and output the hash table contents. O(n + m) Otherwise read manageable chunks of each file, sort in-core, and write to a corresponding set of temporary files. Use an external merge sort to reduce the number of files in each data set if necessary so that you don't run out of file descriptors opening all at once.  You'll achieve better performance on machines with hierarchical storage (arrays or hybrid discs with both flash and spinning media) if you intermix this with the temporary file generation and perform merges before data is flushed from the faster storage. Then use a modified merge sort on the intermediate data.  Reference each set of temporary files with a min heap updated (remove the minimum, read the next entry from its file until you don't match the current entry, and re-insert) when the other set's top element equals or passes it.  Compare the top elements and increment your counters appropriately. Ex: sorted word set 1 a c, sorted word set 2 b c.  b > a so a is in file 1 only; c > b so b is in file 2 only; and c == c so it's present in both. O(n log n + m log m) This solution could be spread across  multiple machines if the number of temporary files and intermediate merges became an issue with one machine. Also note that the strings smell like  a 32 bit decimal prefix followed by a 128 bit hex suffix.  Assuming a case like that you're better off treating them as sets of numbers instead of  ASCII strings.   Use tuples of appropriate integer types (ex - a 32 bit int for the prefix and pair of 64 bit longs for the suffix) for your in-core and intermediate secondary storage representations. The complexity won't be any different but your resource usage and run-times should be substantially lower. Tangentially for many situations fast enough is good enough and you'd be better of skipping the optimizations on data representation and hierarchical secondary storage until you have measurements suggesting the gains are worth the implementation costs. Where you run the code once and it finishes while you're home sleeping improvements won't be needed.  Where you have a lot of machines and reducing their number by 1% saves 100K annually optimizations are very worthwhile. Updated 134w ago • View Upvotes