How do I compute space complexity and time complexity, with detailed explanation?
For time complexity Aditya Joshi has written a great answer: Aditya Joshi's answer to What are some easy ways to understand and calculate the time complexity of algorithms? Here I'm writing about Big-O notation and some extra details reference to Aditya Joshi's answer! The simplest definition I can give for Big-O notation is this: Big-O notation is a relative representation of the complexity of an algorithm. There are some important and deliberately chosen words in that sentence:   relative: you can only compare apples to apples.   You can't compare an algorithm to do arithmetic multiplication to an  algorithm that sorts a list of integers.  But a comparison of two  algorithms to do arithmetic operations (one multiplication, one  addition) will tell you something meaningful; representation: Big-O (in its simplest form)  reduces the comparison between algorithms to a single variable.  That  variable is chosen based on observations or assumptions.  For example,  sorting algorithms are typically compared based on comparison operations  (comparing two nodes to determine their relative ordering).  This  assumes that comparison is expensive.  But what if comparison is cheap  but swapping is expensive?  It changes the comparison; and complexity: if it takes me one second to sort  10,000 elements how long will it take me to sort one million?   Complexity in this instance is a relative measure to something else.Come back and reread the above when you've read the rest. The best example of Big-O I can think of is doing arithmetic.  Take  two numbers (123456 and 789012).  The basic arithmetic operations we  learnt in school were:   addition; subtraction; multiplication; and division.Each of these is an operation or a problem.  A method of solving these is called an algorithm. Addition is the simplest.  You line the numbers up (to the right) and  add the digits in a column writing the last number of that addition in  the result.  The 'tens' part of that number is carried over to the next  column. Let's assume that the addition of these numbers is the most expensive  operation in this algorithm.  It stands to reason that to add these two  numbers together we have to add together 6 digits (and possibly carry a  7th).  If we add two 100 digit numbers together we have to do 100  additions.  If we add two 10,000 digit numbers we have to do 10,000  additions. See the pattern?  The complexity (being the number of operations) is directly proportional to the number of digits n in the larger number.  We call this O(n) or linear complexity. Subtraction is similar (except you may need to borrow instead of carry). Multiplication is different.  You line the numbers up, take the first  digit in the bottom number and multiply it in turn against each digit  in the top number and so on through each digit.  So to multiply our two 6  digit numbers we must do 36 multiplications.  We may need to do as many  as 10 or 11 column adds to get the end result too. If we have two 100-digit numbers we need to do 10,000 multiplications  and 200 adds.  For two one million digit numbers we need to do one  trillion (1012) multiplications and two million adds. As the algorithm scales with n-squared, this is O(n2) or quadratic complexity.  This is a good time to introduce another important concept: We only care about the most significant portion of complexity. The astute may have realized that we could express the number of operations as: n2  + 2n.  But as you saw from our example with two numbers of a million  digits apiece, the second term (2n) becomes insignificant (accounting  for 0.0002% of the total operations by that stage). One can notice that we've assumed the worst case scenario here. While  multiplying 6 digit numbers if one of them is 4 digit and the other one  is 6 digit, then we only have 24 multiplications. Still we calculate  the worst case scenario for that 'n', i.e when both are 6 digit numbers.  Hence Big-O notation is about the Worst-case scenario of an algorithm The Telephone BookThe next best example I can think of is the telephone book, normally  called the White Pages or similar but it'll vary from country to  country.  But I'm talking about the one that lists people by surname and  then initials or first name, possibly address and then telephone  numbers. Now if you were instructing a computer to look up the phone number  for "John Smith" in a telephone book that contains 1,000,000 names, what  would you do?  Ignoring the fact that you could guess how far in the  S's started (let's assume you can't), what would you do? A typical implementation might be to open up to the middle, take the 500,000th  and compare it to "Smith".  If it happens to be "Smith, John", we just  got real lucky.  Far more likely is that "John Smith" will be before or  after that name.  If it's after we then divide the last half of the  phone book in half and repeat.  If it's before then we divide the first  half of the phone book in half and repeat.  And so on. This is called a binary search and is used every day in programming whether you realize it or not. So if you want to find a name in a phone book of a million names you  can actually find any name by doing this at most 20 times.  In comparing  search algorithms we decide that this comparison is our 'n'.   For a phone book of 3 names it takes 2 comparisons (at most). For 7 it takes at most 3. For 15 it takes 4. … For 1,000,000 it takes 20.That is staggeringly good isn't it? In Big-O terms this is O(log n) or logarithmic complexity.  Now the logarithm in question could be ln (base e), log10, log2 or some other base.  It doesn't matter it's still O(log n) just like O(2n2) and O(100n2) are still both O(n2). It's worthwhile at this point to explain that Big O can be used to determine three cases with an algorithm:   Best Case: In the telephone book search, the best case is that we find the name in one comparison.  This is O(1) or constant complexity; Expected Case: As discussed above this is O(log n); and Worst Case: This is also O(log n).Normally we don't care about the best case.  We're interested in the  expected and worst case.  Sometimes one or the other of these will be  more important. Back to the telephone book. What if you have a phone number and want to find a name?  The police  have a reverse phone book but such look-ups are denied to the general  public.  Or are they?  Technically you can reverse look-up a number in  an ordinary phone book.  How? You start at the first name and compare the number.  If it's a match,  great, if not, you move on to the next.  You have to do it this way  because the phone book is unordered (by phone number anyway). So to find a name:   Best Case: O(1); Expected Case: O(n) (for 500,000); and Worst Case: O(n) (for 1,000,000). The Travelling SalesmanThis is quite a famous problem in computer science and deserves a  mention.  In this problem you have N towns. Each of those towns is  linked to 1 or more other towns by a road of a certain distance. The  Travelling Salesman problem is to find the shortest tour that visits  every town. Sounds simple?  Think again. If you have 3 towns A, B and C with roads between all pairs then you could go:   A → B → C A → C → B B → C → A B → A → C C → A → B C → B → AWell actually there's less than that because some of these are  equivalent (A → B → C and C → B → A are equivalent, for example, because  they use the same roads, just in reverse). In actuality there are 3 possibilities.   Take this to 4 towns and you have (iirc) 12 possibilities. With 5 it's 60. 6 becomes 360.This is a function of a mathematical operation called a factorial.  Basically:   5! = 5 × 4 × 3 × 2 × 1 = 120 6! = 6 × 5 × 4 × 3 × 2 × 1 = 720 7! = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5040 … 25! = 25 × 24 × … × 2 × 1 = 15,511,210,043,330,985,984,000,000 … 50! = 50 × 49 × … × 2 × 1 = 3.04140932 × 1064So the Big-O of the Travelling Salesman problem is O(n!) or factorial or combinatorial complexity. By the time you get to 200 towns there isn't enough time left in the universe to solve the problem with traditional computers. Something to think about. Polynomial TimeAnother point I wanted to make quick mention of is that any algorithm that has a complexity of O(na) is said to have polynomial complexity or is solvable in polynomial time. O(n), O(n2) etc are all polynomial time. Some problems  cannot be solved in polynomial time.  Certain things are used in the  world because of this.  Public Key Cryptography is a prime example.  It  is computationally hard to find two prime factors of a very large  number.  If it wasn't, we couldn't use the public key systems we use. Time Complexity : How long does this sorting program run? It possibly takes a very long time on large inputs (that is many strings) until the program has completed its work and gives a sign of life again. Sometimes it makes sense to be able to estimate the running time before starting a program. Nobody wants to wait for a sorted phone book for years! Obviously, the running time depends on the number n of the strings to be sorted. Can we find a formula for the running time which depends on n?                         Having a close look at the program we notice that it consists of                two nested for-loops. In both loops the variables run from 0 to n, but the inner variable starts right from where the outer one just stands. An if with a comparison and some assignments not necessarily executed reside inside the two oops. A good measure for the running time is the number of  executed comparisons.[11]                                                                                                In the first iteration n comparisons take place, in the second n-1,                then n-2, then n-3 etc. So 1+2+...+n comparisons are performed                altogether. According to the well known Gaussian sum formula these are     exactly 1/2·(n-1)·n comparisons.    figure illustrates this. The screened area corresponds to the number of comparisons executed. It apparently corresponds approx. to half of the area of a square with a side length of n. So it amounts to approx.1/2·n2.until the program will have terminated! All this is caused by the expression n2. One says: Sorting by minimum search has quadratic complexity. This gives us a forefeeling that this method is unsuitable for large amounts of data because it simply takes far too much time.                                                                                         So it would be a fallacy here to say: “For a lot of money, we'll simply buy a machine which is twice as fast, then we can sort twice as many strings (in the same time).” Theoretical running time considerations offer protection against such fallacies.                         The number of (machine) instructions which a program                      executes during its running time is called its complexity in computer    science.  This number depends primarily on the size of the program's input, that is approximately on the number of the strings to be sorted (and their length) and the algorithm used. So approximately, the time complexity of the program “sort an array of n strings by minimum search” is described by the  expression c·n2. c is a constant which depends on the programming language used, on the quality of the compiler or interpreter, on the CPU, on the size of the main memory and the access time to it, on the knowledge of the programmer, and last but not least on the algorithm itself, which may require simple but also time consuming machine instructions.  (For the sake of simplicity we have drawn the factor 1/2 into c here.) So while one can make c smaller by improvement of external circumstances (and thereby often investing a lot of money), the term n2, however, always remains unchanged.                                                                                                So, if f∈O(g) means that f grows at most as fast as g, asymptotically and up to a constant factor; think of it as a ≤. f∈o(g) is the stricter form, i.e. <. f∈Ω(g) has the symmetric meaning: f grows at least as fast as g. ω is its stricter cousin. You can see that f∈Ω(g) is equivalent to g∈O(f). f∈Θ(g) means that f grows about as fast as g; formally f∈O(g)∩Ω(g). f∼g (asymptotic equality) is its stronger form. We often mean Θ when we use O. Note how O(g) and its siblings are function classes.  It is important to be very aware of this and their precise definitions  which can differ depending on who is talking when doing  "arithmetics" with them.  When proving things, take care to work with your precise definition.  There are many definitions for Landau symbols around (all with the same  basic intuition), some of which are equivalent on some sets on functions  but not on others. Suggested reading: What are the rules for equals signs with big-O and little-o? Sorting functions by asymptotic growth How do O and Ω relate to worst and best case? Nested Big O-notation Definition of Θ for negative functions What is the meaning of O(m+n)? Is O(mn) considered "linear" or "quadratic" growth? Sums of Landau terms revisited What does big O mean as a term of an approximation ratio? Summary : Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. What does that mean? Essentially, that's the amount of memory an algorithm needs. Often, there is a time-space tradeoff in a problem, where we compromise; that is we can't have both low  computing time and low memory consumption, so depending on the algorithm  we choose whether low computing time is essential or memory  consumption. Asymptotic Notations: Asymptotic  Notations are used for expressing the main component of the cost of a  particular algorithm using idealized units of computational work. What do we use it for? A  problem can have numerous algorithmic solutions, but we want to choose  the best algorithm for the task so we take two solutions and judge how  long they will run, whichever has the least running time (least time  complexity) is our winner. Types of Asymptotic Notations: Big-O Notation: It is used to express the upper bound of an algorithm's running time  i.e. tells us the worst case (longest time) an algorithm could take to  complete. It is denoted as                      if and only if there exists constants k and n such that,                      Big-Omega Notation:  The exact opposite of Big-O, it is used to express the lower bound of  an algorithm's running time i.e. tells us the best case (least time) an  algorithm would take to complete. It is denoted as                      if and only if there exists constants k and n such that,                      Theta Notation:  A combination of Big-O and Big-Omega, it has a lower bound as well as  an upper bound. This determines the average case of an algorithm's  running time. It is denoted as                                          if and only if there exists constants j,k and n such that,                                        I hope this will help you. Thank You. 