What are some time-saving tips that every Linux user should know?
First, I suggest using bash rather than any alternative such as tcsh, fish, or zsh.  These are fine alternatives, but bash is the default on the vast majority of Linux and other modern Unix systems and derivatives (including MacOS X, and *BSD systems).  Don't just learn it.  Don't just use it.  MASTER bash! Also master vi (by which I mostly mean the vi compatible subset of vim). You're an emacs fan.  Fine.  You can live in emacs.  But if you're going to spend a significant amount of time at any sort of Linux command prompt then it's likely that you're involved in administering or operating lots of Linux machines or virtual machine instances in a cluster.  (It's primarily in the administrative and operational details where you care whether it's Linux vs. any other Unix-like OS). If you're bouncing around among a multitude of systems then having to stop and ensure that tools like emacs, or zsh are installed will cost you lots of time.  You can invest your time in customizing the systems (and whatever infrastructure is in place to automate such customization) ... and you probaly should.  But you can also invest some of your time in adapting our skills so you're very comfortable using the default shell and the faster default editor. If you find some system starting nano or some other editor when you run commands that try to automatically invoke editors for you then add: export EDITOR=$(which vim) (or which vi) ... to your environment/profile. Combine these previous two suggestions.  Use set -o vi with bash to set your command line and history editing key bindings to a subset of vi bindings.  I also like to add bind C-l:clear-screen C-i:complete ... to regain a couple of settings from the default (emacs mode) bash settings. Also: use the fc (the "fix command") command (it's in bash, Korn shell and probably zsh). fc launches your preferred editor to allow full screen editing of some command or range of commands from your history. This is particularly handy when using cut & paste from web browsers, other terminal windows and editors with complex commands.  (Frequently you have to copy several different excerpts from such sources ... and doing so with separate operations for each is tedious and time consuming). With fc you simply do a "sloppy copy" of the whole passage containing the various bits of text you need and all the cruft you want to remove.  The issue a command like: fc for [Enter] ... paste is the whole mess and then use your favorite editing commands and macros to formulate the rest of your command.  (In this example it would be your most recent for command and you'd presumably be pasting in a list of targets to iterator over, and a body of commands to execute on each, for example). When you save and exit ... the command is automatically executed by your shell. Obviously, when the changes are simple, then you can save keystrokes by using the various old csh compatible ! history operators including ^ (for example changing something like foo to bar in the most recent command using ^foo^bar[Enter]).  But if you spend a lot of time doing ad hoc, complex, multi-line, commands .... fc can't be beat. A common shell pattern I use goes something like this: d='x xxx xxx ... y yyy yyyy ... z zzzz zzzz ... ...' echo "$d" | while read h f r; do echo do_something $h --someswitch "$f" --otherswitches $r; done In other words use a shell variable to hold some lines of text, each line containing some arguments (all usually prepared in my editor in another window usually with creative database or other searches and so). Then iterate over those filling in a command template with my hostnames, ssh commands and switches and so on.  I run that once and look over the output to see if the generated commands look right.  After that I can use ^do echo^do ... to reissue the same command but actually to do_something rather than merely inspecting what the resulting template rendering looks like. One trick there is that you can store a significant amount of arbitrary text in a shell variable (easily over a 100K) and use that instead of temporary files for most purposes.  You can iterate over lines and parse them into variable lists using the | while read x y z; do ...; done pattern.  When parsing (separating the lines into fields into which to associate the variables (x, y, and z here) then your shell's IFS setting will be used (so you handle trivial comma-separated or colon delimited files, for example).  Additionally the read command will respect any quoting in the input line (consistent with the shell's normal quoting and escaping rules).  Also all remaining contents on the line will be assigned to the last variable on the list. Overall I refer to this latter pattern: | while read ...; do ....; done as the pipemill pattern.  You're writing code to "mill" over the output from a pipe.  It's very simple example of the "producer-consumer" pattern and is the most flexible simple shell scripting pattern I've ever found. You can also do stuff like:  ifconfig -a | egrep 'Link |inet ' | while read iface x;read x a x;do echo $iface ${a##*:}; done ... to read the first and second line of each interface description from the ifconfig -a output (the egrep regular expression is set ot a couple of patterns that only occur on the first and second lines respectively).  So we read each of those in our while loop, using the variable x repeatedly to "throw away" the fields we're not interested in.  Then we output just the interface name from the first line and just the IP address from the second line (stripping off the stuff up to the : in that field using a bash parameter substitution).  That's a trivial example but shows that the pipemill is not limited purely to consuming inputs oriented on single lines for each job.  You can also throw away a header line using something like: ps laxwww | {read x; while read x u p ppid x x x x x state x x cmd args; ... done; } ... here we throw away the header line printed by ps and we grab just the columns we're interested in (user, PID, Parent PID, the process state, the command's name or argv[0], and the read of the command's arguments.  This is handing for find zombies and killing their parents for example). The next productivity suggest enhances what I've already said.  Use GNU screen (GNU Screen - GNU Project - Free Software Foundation)   ... or tmux (tmux).  I recommend the former for the same reason I recommend bash over its alternatives.  GNU screen is installed on most Linux and other modern Unix-like systems by default.  However, this suggestion is weighted less heavily since you'll normally only be running text console multiplexer (screen or tmux) on your local workstation or a preferred jump box or "control tower" system. The advantage of these systems is that you can maintain a persistent session with all your shell sessions, connected to various different systems and running various programs all over the place.  You can detach, re-attach, even allow others to connecte and share your session (obviously only your most trusted colleagues for you main session --- though you can also co-ordinate to run separate liaison sessions as necessary).  GNU screen in particular also has some fairly advanced backscroll search and keyboard driven cut-n-paste features and macroing. In my daily usage pattern I maintain a "notes" editor in my first screen window.  It runs vi with a macro for inserting the current date/time stamp using a single keystroke ([F8] in my case).  I almost always just work at the bottom of the file and let it grow arbitrarily large.  Any time I exit the wrapper script around it mails me a copy.  I use this for most of the cases where I would use fc (as described above) ... but I just leave the contents there (notes for the future) and paste the results into my other windows to execute as commands, etc.  When I commit changes to git I paste the output from git log into my notes; when I'm working on a Jira ticket I paste the URL into my notes (where I can go back, right click on it and bring that back up later).  When I edit a file on production system (if it's not under some sort of git or other version control) I paste a copy into my notes (and usually also into a Jira comment).  When I use some web based dashboard or work on something from PagerDuty or from any web based Nagios (Check_MK, Thrux, Opsview)  front end I paste the ReSTful URLs into my notes. Gnu screen uses vi-keybindings for most of its scroll buffer operations and similar features by default.  So using it also builds on the same principles that I mentioned earlier.  Master vi and use those bindings in your shell and everywhere else that the readline libraries allow it (via the ~/.inputrc settings, for example).  I enable them in iPython as well. I also frequently use iPython for munging data into some form I can use for all this other work.  For example on my current contract my boss ends up with various Excell spreadsheets from which he needs to extract data.  I usually just export those to .CSV, read and parse them in an interactive session and write the results out as text.  In one recent case he had to versions of the same spreadsheet (last month and current) and wanted to know which hosts has been added or removed from one to the other.  It only takes a couple minutes to read in both, extract that column from each, storing each in a Python set() and then take the differences in each direction and finally print the sorted results from each (paste into my notes and into the e-mail response for him).  In another case he a long spreadsheet listing every incident of server downtime across our production clusters for the last three monts and he wanted to know which systems appear most often in that list.  (This is, basically, what you'd get by piping just the relevant portion through sort | uniq -c | sort -nr | head -n XX ... but, in this case I did it using the Python collections.Counter class and its .most_common() method).  (I also used iPython's %save command to save that session and whip the history into a script file for him to re-use from now on with similar data). For another case I used data from a text table ([Tab]-delimited in this case) to generate the 40 or so commands necessary to create a new cluster of clusters.  The columns in the table were hostnames (which I'd generated using pattern strings with numbers interpolated into them), OpenStack image and "flavor" IDs (like AWS AMI and type designations), as well as some networking data and other parameters.  I simply interpolated the fields from each row into a nova boot command (analagous to an aws ec2 run command) and wrote the results into a file.  Then I attached the file to the Jira ticket and ran it from the appropriate environment.  (Yes, much of the editing had gone through my notes file as well). The fact that my script is attached to the Jira ticket gives a completely unambiguous "statement of work" for later review and was handy when I was asked to change the capacity of one of the cluster types (I was able to simply paste a copy of the relevant lines from my script into a new script, search and replace witih the bigger "flavor" --- kill the old instances and spin up their replacements.  Adding or replacing nodes is also easy for anyone on the team because they have the exact commands easily attached.  Building a whole new environment (staging for for a different region) is similarly easy. All these work flow practices work together.  Gives me time to amuse myself on quora while looking productive. :) Updated 6 Feb • View Upvotes