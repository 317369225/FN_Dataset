What is the simplest intuitive proof of Dijkstra’s shortest path algorithm?
The idea of Dijkstra’s algorithm is really easy. Suppose we drop a huge colony of ants onto the source node u  at time 0  . They split off from there and follow all possible paths through the graph at a rate of one unit per second. Then the first ant who finds the target node v  will do so at time d(u,v)  seconds, where d(u,v)  is the shortest distance from u  to v  . How do we find when that is? We just need to watch the expanding wavefront of ants. Toward that end, let’s maintain two data structures: A schedule to keep track of the future arrival times of the first ants en route to each node. (Shown as red in my animation. Initially, the ants are only scheduled to arrive at u  at time 0  .) A visited array to mark the nodes that ants have already reached, to make sure we don’t waste following all the ants behind the front lines. (Shown as green in my animation. Initially, no nodes have been visited.) As we observe the march of the ants, we’ll update these structures as follows. Consider the earliest arrival time in the schedule: node i  at time t  . At that time, we’re going to do three things: We’ll mark node i  as visited. We’ll remove i  from the schedule. For each edge from i  to an unvisited node j  , ants are going to start swarming down that edge. (Ants are also going to start swarming back toward any visited nodes, but we don’t care about those ants since they’ll never be the first to find new nodes.) We’ll note on the schedule that ants will arrive at j  at time t+w  , where w  is the length of the edge—unless the schedule already had an earlier entry for j  , in which case we won’t change it. We might, however, replace a later entry for j  .Then we repeat this process with the next earliest arrival time in the schedule, and so on, until the ants finally visit node v  . Since the schedule will contain all the information we need to figure out when the ants will arrive where, we can actually implement this as an algorithm instead of a horrible middle-school science fair project. By the way, you can see Dijkstra’s algorithm happening in nature as a bolt of lightning searches for the path of least resistance to the ground. This is way cooler than watching a colony of ants. To make the algorithm efficient, the schedule just needs to support three operations quickly: remove the entry with earliest time (this happens up to once for each node). add a new entry (this happens up to once for each node), decrease the time associated with an existing entry (this happens up to once for each edge).Such a data structure is called a priority queue, and there are several implementations with different properties. A simple array supports these operations in O(n),O(1),O(1)  time, leading to an O(|V| 2 )  time algorithm. A heap (or a balanced search tree) supports these operations in O(logn),O(logn),O(logn)  time, leading to an O(|E|log|V|)  time algorithm. A more optimized priority queue structure like a Fibonacci heap supports these operations in O(logn),O(1),O(1)  time, leading to an O(|V|log|V|+|E|)  time algorithm. These structures are complicated—if you’re just learning about Dijkstra’s algorithm for the first time, you don’t need to understand them. Updated 27 Nov 2013 • View Upvotes