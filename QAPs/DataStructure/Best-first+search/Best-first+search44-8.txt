Where can I find large datasets open to the public?
Here are some big corpora we use in NLP in addition to the ones already mentioned: ukWaC: a 2 billion word corpus constructed from the Web limiting the crawl to the .uk domain and using medium-frequency words from the BNC as seeds. The corpus was POS-tagged and lemmatized with the TreeTagger. There's also a parsed version called pukWac. Get both at: http://wacky.sslmit.unibo.it/dok... WaCkypedia: a 2009 dump of the English Wikipedia (about 800 million tokens), including part of speech/lemma information, as well as a full syntactic parse. The texts were extracted from the dump and cleaned us... (more)