Under what conditions would a slow algorithm run faster than a fast algorithm?
Frequently this will occur with very small data sizes. Many of the fast algorithms scale much better, but have a much higher overhead than a simpler, more naive approach. Its fairly common to test the size of the data and use the naive approach if its small enough, effectively making a hybrid algorithm, especially if its recursive. Additionally, many algorithms have a worse-case cost that is different from the average case. quick-sort, for instance, has a worse case complexity of n^2. Many measures are taken to avoid this scenario, but if you do hit it, you can do worse than a slower algorithm. Different algorithms may also scale on different aspects of the problem. For instance, a radix sort scales based on the lenth of the numbers to be sorted, while a quicksort scales based on how many. quicksort is n lg(n), while radix sort is n*d, where d is the number of digits. how they compare to each other will depend on the relationship between d and lg(n), and there are properties of the data which will show one outperform the other or visa versa. Or with a graph algorithm, one approach may scale based on the number of edges, and the other the number of nodes. Sometimes one algorithm will be faster for a given set of data on a given hardware. If one takes more memory, and ends up having to use the hard drive instead of purely ram, it will take a huge performance hit that can easily outweigh a big O difference. Or one approach may be disk-bound while the other is cpu bound, so it depends on which resources is more plentiful. 