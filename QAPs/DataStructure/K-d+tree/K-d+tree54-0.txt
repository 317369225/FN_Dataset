What is the optimal solution for the problem: given a list of "threads", which contain two variables (starting and ending times) implement a function that will return all running threads at some time t?
With unstructured data, you can't avoid looking at everything at least once. O(n) is unavoidable, so simply searching through an checking each one for your qualifications is as good as you can get. However, if this will be run multiple times, then having an upfront cost to put it into a better data structure can pay off. A simple approach is to sort the threads by start time. You can then iterate through them until your start time is greater than your query time, and check each one for an end time greater than the time. This helps if many threads start later than your time, but you still are dealing with O(n) threads to look at. One approach is to index on time. First, you choose a time increment; what this increment is will depend on the specifics of your problem, such as how long the run times are and how much overlap there are. You have a list of buckets, each buckeet corresponds to a timeframe. For each thread, you put it in every bucket where its runtime overlaps. The cost of doing this is n*r/t, where n is the number of threads, r is the average runtime, and t is the size of the timestep you used. r*t can be very high, so this may not always make sense to do. Once you have this structure, you can look at the bucket containing your time, and every candidate thread will be in there. Finding the bucket can be constant time. The bucket contains b threads; these threads can be pre-marked into 2 categories; one is complete overlap, where the thread ran for the entire time of the bucket. You can take every thread in this category for free, grabbing a reference to the set containing them. The others are overlapping the bucket, so you have to check if your timestamp actually falls within the structure. This is where a smaller t is beneficial; it makes it so you have more overlaps, and fewer threads to  disambiguate. Best-case, you get the queries own to O(1). Worse case, every thread starts or ends in your bucket, and you still have to deal with O(n). This can also be memory intensive. This is highly dependant on the specifics of the situation to determine efficiency. In the best case, with many queries, and lots of threads with low runtimes, and buckets that align well with the start and end times of your threads, this can be very performant. A variation of this will specifically align the buckets to align with the overlapping regions. For this approach, I'd sort the threads, but the list would contain each thread twice, once for its start and once for its end. (cost O(nlgn)) I would then iterate through the list, keeping track of the current set of active threads. If the next element is a start, I add it to the set, if its an end I remove it. Each time the time changes, you create a new bucket, so each bucket corresponds to every unique combination of running threads. The current set of active threads is put into that bucket. This has a cost of O(s*o), where s is the average number of simultaneous threads(since you have to copy them all) and o is the number of overlapping sections; worse case o is 2n, if every thread starting or stopping creates a new overlap. s can be no more than n, so the total time cost is O(n^2), though better o and s values will give performance closer to O(n). So our total startup cost is lower than the previous if r/t would be greater than n. Once we have this structure, finding the necessary bucket can be done with a binary search, yielding O(lg(o)), and since we have determined o is at most 2n, we have O(lg(n)) as our cost per search. This approach gives us a harder guarantee than the former. If we have 1 queries, the naive approach has a cost of q*n. The simple bucket approach is n*r/t+q  to n*r/t+n*q, and the advanced bucket approach is n^2 + qlg (n) Lets look at the problem from another angle. lets say we graph each thread on a 2d graph. the start time is the x axis, the end time is the y axis. since a thread must end after it starts, all of the points will be above the x=y line. Everything that can be found at a given time has x<t and y>t. This defines a box on the graph, with a corner resting on the x=y line. Everything in that box is a thread we want. So we could use a k-d tree http://en.wikipedia.org/wiki/K-d... building this tree takes nlgn time, so we are just as good as a search. The time to find the points, using a range search, is O(sqrt(n)) this gives us a overall performance of nlgn + qsqrt(n). This is a much smaller startup cost than the customized buckets, and would be much more memory efficient, and so is much more practical, but the per-query cost isn't quite as good, though its not bad. The initial bucket search had a great time. The only problem was the start-up cost, esp if you want to get a good t. We can actually optimize that. Instead of sorting our buckets as a linear list, its a nested tree. At the top level, if we overlap a section completely, we add it to the "complete" set. If not, we drop down a level, and check smaller timeframe boxes. adding it to buckets it completely overlaps, and dropping down to even more precise buckets, etc. This allows us to get down to a very precise time segment, without having to add every small segment along the way. At query time, you just need to get everything from every layer that intersects your point, and the only disambiguation you need to have is if you happen to get the fine grained bucket where something ends This has a construction cost of O(n*l) and a query cost of O(l), where l is the number of layers you use. This gives my final answer, and a total cost of n*l+q*l 