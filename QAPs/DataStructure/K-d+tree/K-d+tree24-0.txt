What are some fast similarity search algorithms and data structures for high-dimensional vectors?e.g. applying Jaccard, Cosine or Hamming distance to find similar items on billions of 100-dimensional arrays of integers.
Similarity search can be generalized as Approximate Nearest neighbor search. A naive linear scan based similarity search algorithms is of O(N∗D)  complexity. There are two ways to improve then: Reducing the number of comparisons made and speeding up individual comparisons. So fast similarity search techniques can be broadly classified as: Partition based: This is one of the earliest methods to speed up similarity search. Technique involves branch and bound based search in the data space. The indexing structure is usually a tree data structure. Some of the common indexing techniques are: k-d tree: Binary space partitioning based data structure, where the data space is recursively divided by successive dimensions.  A similar approach is Quadtree which partitions the space into four at every level. There are many variants with randomized KD trees being the recent scalable one. R-tree: This indexing is well suited for neighbor search in 2D space. The indexing technique involves hierarchical overlapping rectangles. There are variants like R+ tree, R* tree and X-tree. Metric tree: Indexing structure defined on metric spaces, utilizing some of the properties like triangular inequality. Popular one is M-tree For an exhaustive treatment on multi dimensional indexing techniques, one of the best references are Foundations of Multidimensional and Metric Data Structures by H. Samet. Space partitioning techniques are in general not immune to the curse of dimensionality and when the dimensionality size increases beyond a point it is as worse as a linear scan. Approximate distance: A complementary approach is doing a linear scan with approximate distance measure. This can be generalized to the family of Locality-sensitive hashing, a probabilistic approach to compute approximate neighbors. LSH can be used as a partition technique too, but the trend is on using the LSH hash-code to approximate distance, instead as an index. During the indexing phase, the database vectors are reduced to hash codes using LSH and stored as index. Given a query point, its has code is compared against all the database points in code space. Thanks to the advancement in modern processors, this "code comparison" is very fast (few instructions), compared to vector based distance. Thus it can be scaled to even millions of points. Some common LSH distance approximations are Jaccard Distance: MinHash Cosine Similarity: Random Projection + Sign based quantization Similarity estimation techniques from rounding algorithms Euclidean distance: E2LSH Locality-sensitive hashing scheme based on p-stable distributions Learning Based approach: This is also known as Learning hash functions. Part of the data is used to train hash functions, which will be used to index the data. Again the learnt hash function can also be used to approximate distance if the hash function is in hamming space. Learning Binary Hash Codes for Large-Scale Image Search and Small codes and large image databases for recognition give a good overview about hash function learning. Some of the popular approaches are: Semantic hashing: This is one of the first popular approaches on using supervised learnt hash function to transform data into hamming space. They trained a deep belief network to map real vectors to binary codes. Spectral Hashing: They applied spectral techniques on similarity graph to get hash codes. This is one of the widely compared techniques. Product Quantization: This is an unsupervised learning technique, where in dimensions are grouped and clustered separately using K-Means. A data point to be indexed is assigned to these cluster groups separately and the cluster-ids from these groups are concatenated to get a global compact code. 