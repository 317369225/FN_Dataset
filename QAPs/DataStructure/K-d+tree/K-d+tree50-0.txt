How do I go about creating an object recognition system?
In computer vision there are two main systems to consider when developing an object recognition system: Instance-level object recognition: This is the easiest to achieve as it is just about recognizing an instance of an object in a scene. It involves finding corresponding points between an object model in the database and that in a scene. Category-level object recognition: This involves classifying an object as belonging to a particular known category of objects. This is more difficult and requires special feature selection and learning algorithms. Instance-level object recognition: There are 4 main stages: Feature detection Feature matching Feature integration Model fitting and verification Feature detection: What is a feature? A feature in computer vision is an easily localizable structure in an image, that is, a feature must be easy to find in a given image. It must have a well defined pose i.e position that is easy to recover from measurements done on the image. Repeatability is one important attribute of a feature and means that the same feature must be detectable even when an image/object undergoes different transformations such as projective distortion, scaling, rotation or translation. Feature detectors such as the simple, complex and hypercomplex cells are also found in the primary visual cortex. The simple cell responds to an oriented edge-like stimuli at a precise 2D location on the retina. The complex cell responds best to an oriented moving bar, the orientation tuning is specific but the position tuning is loose, hence making complex cells tolerant to geometric distortions. The hypercomplex cell is end-stopped, that is, it can only respond to bars of a certain length, any larger and the response drops. As the visual signal moves up the hierarchy of visual processing, the kind or type of features to which neurons tune to become more complex and specific. Borrowing from the primary visual cortex, feature detectors are heavily used in computer vision to speed up image analysis. One easily identifiable features are edges: Edges indicate an existing boundary between an object and the background, in terms of computer vision an edge is a region of high gradient magnitude, usually an abrupt change in gradient magnitude indicates edge presence. Early computer vision systems needed explicit edge detection phase but newer systems don't do explicit edge detection but rather work directly with image gradient information or raw pixel info. Corners are points where two edges with different orientations meet. A more proper definition is, a corner point or region is a region which shows sharp change in appearance within it's immediate neighborhood in all directions. This means that a corner is localizable, that is, it is easy to find a corner point location consistently in different image settings, a very important attribute, thus corner detectors are widely used in matching or aligning images but are not scale variant. Blobs are regions which are brighter or lighter than their immediate neighboring region. They are usually complementary to corner detectors and blobs are also highly localizable in terms of 2D location and scale. Thus blob detectors are heavily used in scale invariant object detection and recognition such as in Scale Invariant Feature Transform (SIFT). Ridges are not so widely used in computer vision and mainly detects elongated objects or features. Good features to keep in mind are corners and blobs because they fit the definition of a feature very well. Feature matching: When a feature point is detected, a descriptor is extracted around that point to summarize image appearance in the immediate neighborhood. Descriptors are usually real-valued vectors that are normalized using L2-norm but they can also be binary. They are placed in an indexing structure like the  k-d tree or many others such as locality-sensitive hashing so that observed descriptors can be matched to their corresponding look-alike features in the database. Feature integration: As previously seen multiple features are detected and matched per given scene. There is need for multiple features consistent with a particular object pose to be grouped into a singular perception. This can be achieved with a generalised hough transform approach were each feature votes for an object center, scale and orientation. The hough voting bins with enough votes i.e 4 and above are considered as object instance hypotheses. Model fitting and verification: The object instance hypotheses are analyzed by fitting a model to the observation. This is done by RANSAC - Random sample consensus algorithm. When fitting a homography motion model matrix 4-point correspondences are sufficient to give a good solution. Model verification looks at a detailed probabilistic analysis of inliers and outliers, to accept or reject a hypothesis. Category-level object recognition: Category-level object recognition is accomplished in a different manner to feature-based methods described above. Feature descriptors are heavily affected by lighting and appearance changes that occur in category-level problems. Merely using distance measures such as squared euclidean distance or sum of absolute differences is not effective for highly variable category-level objects. Thus category-level recognition method employs a non-straight forward approach to recognition, learning. The sliding-window method is heavily used here, given an MxN image, a much smaller window of size mxn where n << N and m << M is swept across the MxN image and at each location a classification function F is evaluated. The classification function F  can range from a simple template to a complex deep neural net. The learning algorithms are vast and currently deep-learning convolutional neural nets are state-of-the-art especially in category-level object recognition. In cases were locating an object is not important, it is not necessary to have a smaller sliding window hence n == N and m == M, this is usually the case in deep learning were F = convolutional neural net. Conclusion: Like in overcoming any hurdle in life, it all boils doing to tenacity and consistence. I built my vision system from ground-up in a space of 4 yrs and I would love to share this link not for marketing purposes but as a demonstration. The app has a state-of-the-art instance-level object recognition system. If you wish to build something like that or better, then you have to work on your passion consistently and with purpose you will get there, read a lot of journals and literature on computer vision. I have added a reference section for you, so that you can read some literature I was frequently referencing while developing my vision system. There is a lot to talk about on object recognition system. Hope this helps and feel free to comment so I can improve the answer. REFERENCES: Computer Vision: Algorithms and Applications Publications by David Lowe Richard Szeliski's Publications Josef Sivic imagenet: CNN image-net.org Scale-invariant feature transform Speeded up robust features Updated 7w ago • View Upvotes • Asked to answer by Anonymous