What is the point of doing machine learning, when you have something so robust as the nearest neighbour algorithm?What better justification/support/evidence can you give about your inference than saying there is(are) a point(s) already in the (indexed)dataset which is really close to your query point.
As Leonid says, it is machine-learning, there are many parameters to tune given a problem, like, picking the appropriate radius, or the appropriate k. Then, in regard Waleed comments, there are workarounds for the issue of "too many samples" for instance it may be possible to use condensation (a form of id-tree edition). Sensitivity to noise? pick a bigger radius or k. Use cross-validation and optimiceÂ  your parameters. Neural Networks virtually take no space, and can be even faster to run. Kd-trees are generally heavy stuff to have in memory compared to neural networks. There is no Silver bullet. 