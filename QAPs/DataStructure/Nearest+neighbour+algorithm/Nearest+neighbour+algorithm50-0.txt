Classification (machine learning):Â How divergence is used in binary classification problems and can it outperform logistic regression?
This is akin to Fisher's linear discriminant. The intuition is as follows: imagine two clusters in 2-dimensional space. Let one of them belong to class 1 and the other to class -1. Consider two classifiers -- one that passes through the centers of each cluster, splitting each cluster into two halves, and the other that is orthogonal to the first one such that the clusters are neatly separated. Which one is a better classifier? Clearly the second one. What does the divergence formula above say? Intuitively, the decision surface splits the data into positive and negative points. You view each of these as a cluster, and find the centers (E(g) or E(b)) and radii (Var(g) and Var(b)) of these clusters. The numerator is the distance between the centers, and the denominator is the mean of the radii of these clusters. So well separated positive and negative points will have a large distance between the centers and small radii, thus a large divergence. Use the example above to see that divergence is larger for the second classifier. There is no correct answer to when is it better than logistic regression. Different classification algorithms do well for different problems, and there are no "rules" to help you find the best one. But you can refer to this answer for some pros and cons of logistic regression: Prasoon Goyal's answer to How can we compare the advantages and disadvantage of logistic regression versus k-nearest neighbour? 