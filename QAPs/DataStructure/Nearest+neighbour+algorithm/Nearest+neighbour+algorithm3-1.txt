What are the state-of-the-art in nearest neighbour learning algorithms?
Nearest neighbor problem The exact nearest neighbor problem can be stated as follows. Given a metric space [math](M,d)[/math] and a query point [math]q\in M[/math] find a point [math]p\in M[/math] such that for all [math]x\in M[/math] we have [math]d(q,p)\le d(q,x).[/math] Relaxed variant, called [math]\epsilon[/math]-NN, or approximate nearest neighbor problem, replaces the above inequality with [math]d(q,p)\le (1+\epsilon)d(q,x).[/math] I think there is currently no way to efficiently solve the exact k-NN problem. (There's an [math]\Omega(n)[/math]-time lower bound for general metric spaces.) However, if one is willing to relax the problem to approximate k-nearest neighbors, there are a number of approaches available:  - locality sensitive hashing (Piotr Indyk & Rajeev Motwani),  - randomized kd-trees (David Lowe & Marius Muha),  - cover trees (Alina Beygelzimer, Sham Kakade, John Langford),  - [math]\epsilon[/math]-nets (Ken Clarkson),  - and probably more. I suggest you check out Ken Clarkson's publications on his homepage for several survey papers on the problem. Since you are asking for the state of the art, let me point out a very recent paper from November 2014, presented at NIPS 2014: [1405.5869] Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS). It extends the LSH framework --- a practically very successful approach to approximate nearest neighbor problem --- to get "fast" algorithm for maximum inner product search problem. I recommend you check what people have been publishing recently in relevant conferences, such as:  - SoCG,  - FOCS,  - STOC,  - SODA,  - NIPS,  - KDD. Classification and regression with k-NNNote that k-NN classifiers are not really learning --- they essentially store the data and, when asked to classify a new example, just report the majority label of the k nearest neighbors (or some other function, like the mean, of the k nearest neighbors). The problem is that finding k-NNs is slow --- it is exponential in the dimension of the space. (Sometimes this is referred to as the curse of dimensionality.) This means that each prediction will be slow! In practice, you are usually willing to invest some time to learn the model but expect that, once built, prediction will be efficient in the sense that "it won't take much time". k-NN are the other way around --- learning is trivial, while prediction can be slow. Second, it is hard to understand why k-NN classifies as it does as opposed to, say, decision trees, where the structure of the tree explains why the example was classified as it was. However, you are right, k-NN-based models tend perform extremely well in practice. Updated 42w ago • View Upvotes