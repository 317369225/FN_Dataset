What is the k-Nearest Neighbour algorithm? What type of problems can be solved by this algorithm? What type of math is required?
The problem is very simple: You have a set of points in n-dimensions. Given a new point (let's call it query) you need to find the k-closest points to the query. KNN can be used to solve many problems, in classification for example you can classify a new point just by examining the class of its nearest neighbors. You can also use KNN to find the most similar documents to a given document for plagiarism, finding mirrors, etc.Â  In recommender systems you can use KNN to find the items that are most similar to an item a user hasn't reviewed and then calculate if the user will like it or not. You can use it in clustering algorithms and there are many many more applications. You need some form of metric distance for this, there are several options the most common is the traditional euclidean distance but you can use Manhattan, Hamming, Jaccard, Levenshtein, etc etc. A naive solution is to linearly search all the original points computing the distance to the query point. In low dimensional spaces you can use some form of spatial index like kd-trees, quadtrees or r-trees to find the points in sub-linear time. Unfortunately spatial indexes perform even worst than a naive linear scan when you have many dimensions. This is known as "the curse of dimensionality" To fight the curse of dimensionality you can try an approximate solution. This is known as ANN (Approximate near neighbours). One way to do that is using LSH (Locality sensitive hashing) where you create a hash function that will map two points that are close to each other to the same bucket. So you just hash the query and scan the points the bucket for the nearest neighbour. There are other interesting solutions to approximate KNN like the Greedy Walk algorithm. 