When is a random forest a poor choice relative to other algorithms?
Random forests tend to be surprisingly flexible -- they can be used with large number of attributes, small or large datasets, they are easy and quick to evaluate. The only two weaknesses I would see are: Results of learning are incomprehensible. Compared to a single decision tree, or to a set of rules, they don't give you a lot of insight. They are hard to make incremental. It can be done, but there's no natural algorithm to do so (in the same way, for instance, that it's easy to tweak the parameters of naive bayes after you add an instance, or to add a new instance to a nearest neighbour classifier). 