Why is dimensionality reduction useful?
An example that demonstrates the use of dimensionality reduction very well is nearest neighbour search. Problem: given a set of d-dimensional datapoints (vectors), find the closest neighbour of a given datapoint according to some distance metric (e.g. euclidan distance). There are a variety of widely used, very clever data structures to anwser that kind of query (which is very common in machine learning). Some examples are ([1] is an awesome overview): KD-Trees and KD-B-Trees Quad-Trees BSP-Trees BBD-Trees R- and X-Trees They all have one thing in common: query running time complexity grows exponential with respect to the dimensionality d of the vectors! That's the (un)popular curse of dimensionality. But there is a solution: dimensionality reduction. According to a theorem called the Johnson-Lindenstrauss lemma [2] you can get the following: Given a set of n datapoints. Fix a deviation epsilon of distances you are going to tolerate for your nearest neighbour querys. Reduce your dimensionality from d to roughly log(n)/epsilon^2 by applying a transformation called random projections to your datapoints. Run your query in the lower dimensional space. That is extremely powerful! We had several algorithms that were exponentional in their dependency on the dimensionality. We then applied a dimensionality reduction which reduced the dimension to a logarithm of that. We end up with and algorithm that is now polynomial instead of exponential! That often is the difference between an algorithm that is utterly unusable and one that actually works! I have goosebumps. Side note: there are a few algorithms for nearest neighbour querys that aren't exponential in the first place, for instance Locality Sensitive Hashing [3] and Random Projection Trees [4]. [1] http://dl.acm.org/citation.cfm?i... [2] http://en.wikipedia.org/wiki/Joh... [3] http://en.wikipedia.org/wiki/Loc... [4] http://cseweb.ucsd.edu/~naverma/... Updated 74w ago â€¢ View Upvotes