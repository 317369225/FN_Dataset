What is the mean return time on the n-dimensional boolean cube, if the stochastic process is choosing one coordinate at random and flipping it?
It's 2^n. I'll give a quick intuitive argument for why we should expect this result, then a complete proof that hopefully is clear. First off, number the vertices from 0 to 2^n - 1. We're looking for the expected time that it takes to get back to vertex 0 after starting at vertex 0. (It's all symmetrical, so the answer would be the same for any other vertex.) Here's the intuitive argument: suppose we run the process for T steps and record the whole history. After some number of steps (the "mixing time" for the process), it doesn't matter where we started -- the probability of being on vertex 0 at step t is nearly equal to 1/2^n in any case. So when T is large, many times the mixing time, we expect to see vertex 0 about T/2^n times. That means that we made about T/2^n round trips leaving vertex 0 and returning, in a total of T steps, so the mean time for a round trip was about T / (T/2^n) = 2^n. Take the limit as T goes to infinity and you get the mean return time. Tricky exercise in probability (I think): turn that argument into a complete proof. Now here's a proof. Let v_i for each i be the expected time before hitting vertex 0, starting from vertex i, with v_0 = 0, and let v be the vector consisting of the v_i. This vector tells us the mean return time -- starting at vertex 0 we take one step, and then we're at a neighbor of 0, so the mean return time is 1 plus the value v_i has at each neighbor of 0. So we'll study this vector around vertex 0. Let A be the adjacency matrix, so A_{ij} is 1 just where vertices i and j are adjacent, and 0 elsewhere. Let L = I - A/n; this is my favorite matrix, the Laplacian of the graph. The great thing here about the Laplacian is that it expresses the conditions on v. In particular, for each i other than 0, [math]v_i = 1 + \frac 1 n \sum_{j \sim i} v_j = 1 + (A v)_i / n[/math] (because starting at vertex i, you take one step and then you're at one of the neighboring vertices uniformly at random), and if you rearrange the terms you get [math](L v)_i = 1[/math]. So the product Lv has the value 1 at every vertex except 0. This is actually enough to solve for v, if we wanted to, to find out exactly how long it should take to get back to a 32-bit string of zeroes when we have 6 bits wrong, etc. But we can finish with a shortcut. We're looking for the mean return time r, and we know it's 1 plus the value v_i has at all the neighbors of 0 -- which we could write as [math]r = 1 + \frac 1 n \sum_{j \sim i} v_j[/math]. Because v_0 = 0, this rearranges to [math]r = 1 - (L v)_0[/math]. So Lv has the value 1 at every vertex other than 0, and if we can find the value at vertex 0, we're done. Now here's the really magical thing the Laplacian does for us: the sum of the components of Lv is zero. Why? Well, let [math]\vec 1[/math] be the all-ones vector; then [math]\vec 1^T L v[/math] expresses the sum of the components of Lv. But take a look at the row vector [math]\vec 1^T L[/math], or equivalently its transpose [math]L \vec 1[/math]. Each vertex has n neighbors, so [math]A \vec 1 = n \vec 1[/math], so [math]L \vec 1[/math] is actually the zero vector. That means [math]\vec 1^T L v = 0 v = 0[/math], so the components of Lv add up to 0. But 2^n-1 of them are 1, so the remaining component (L v)_0 is 1-2^n, and r = 2^n, QED. Traditionally I'd write that proof with a lot fewer words, but I hope it's clearer this way. I've written this for the Boolean n-cube, but the same thing actually works for any Markov chain at all -- for a vertex with equilibrium probability p, the mean return time is 1/p. If you run through the intuitive argument again, it's pretty clear how it all goes the same way in that general setting. Exercise (relatively easy): adapt the proof to work for any Markov chain. 