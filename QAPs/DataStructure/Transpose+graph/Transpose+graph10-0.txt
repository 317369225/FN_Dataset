What kind of interesting research is going on at Berkeley in the field of Computer Science?
Lots of great stuff. I'm mostly familiar with systems research (Amp Lab and Par Lab), but some of that is just ridiculous. Not to say that there isn't any cool non-systems research; this is just what I'm the most familiar with. Communication-Avoiding Algorithms The basic idea is simple: these days, doing some computation on the CPU is much faster than accessing memory. So we need to update our fundamental theory of algorithms to account for communication. "Communication" covers accessing caches, memory, other cores or even information over the network. The group (BeBOP) has developed a theory for modelling the amount of communication an algorithm does. This allows them to do things like prove lower bounds for the number of memory accesses needed to solve a given problem. This is a much better theoretical model of an algorithm's runtime than normal big-O because it accounts for the increased time cost of accessing memory, as well as the "shape" of the memory (multi-level caches, network latency...). This theory allows the group to develop algorithms close to the communication bound. This has resulted in speedups for, well, pretty much all linear algebra algorithms. Yes, really, all of them. Including dense matrix multiply and Strassen's. Another example speedup was in breadth-first search which let its author place 18th on the Graph 500 rankings with a single node. Boy, did that get noticed: now the majority of the top entrants use his algorithm. Performance improvement from "communication-avoiding parallel Strassens" (CAPS), from Grey Ballard's dissertation talk. That's an improvement for dense matrix multiplication! Many of these algorithms are improvements because they perform asymptotically less communication. This group is producing asymptotic improvements to some of the most fundamental algorithms in CS. (Yeah, the logo is pretty weak.)   Apache Spark™ is a framework for running large distributed computations developed at the AMPLab. It's like Hadoop but better in literally every way. And I'm not be facetious either: it's not a trade-off, it's strictly better. It's: fast: up to 100x faster in memory and 10x faster on disk flexible: the core abstraction (Resilient Distributed Datasets or RDDs) is more expressive than map reduce, allowing more complicated processing like streaming computations easy: Spark has a sane interface in Scala, Java and Python including support for interactive development. I've used Hadoop and its interface is... hideous.On top of the framework itself, the Amp Lab has developed some interesting frameworks for more specific tasks: Shark: distributed SQL engine Graph X: efficient large-scale graph processing Blink DB: a database the can approximate SQL queries with bounded errors, making it much faster over large amounts of data Spark is especially impressive in having significant industry buy-in almost immediately after being published. Companies are already using Spark commercially, with more considering it. The project has also spawned a startup: Databricks. Here's a nice (if slightly sensationalist) article about it: Welcome to Berkeley: Where Hadoop isn't nearly fast enough. SEJITS Selective Embedded Just-In-Time Specialization (SEJITS) is a set of techniques for enabling scientists and domain experts (who are not necessarily expert programmers) to easily write high-performance parallel code. It is based on designing domain-specific languages (DSLs) that embody a set of common parallel programming patterns the group identified. The idea is to combine "productive" languages like Python with highly optimized parallel kernels for core algorithms. The SEJITS framework specializes high-level code at runtime to be efficiently executed. The system supports a specific subset of the host language and produces code that is specific to the general algorithmic task performed and the specific parallel architecture the code is running on. A particularly interesting feature of the system is that the specializer is implemented in the host high-level language. This makes it easy to extend the SEJITS system with new specializers for different tasks or architectures after the fact. Here's an implementation of SEJITS: ucb-sejits/ctree. Program Synthesis Program synthesis involves automatically generating code according to some specification. It's a special kind of automatic programming. Essentially, this involves searching through the space of possible programs. Of course, these spaces are mind-bogglingly large (I wrote an answer about this), so we have to be pretty clever about how we search through them. There are two fundamental ideas developed here for program synthesis: CEGIS and synthesis by sketching. This research is mostly led by Ras Bodik. CEGIS Counter-example guided inductive synthesis (CEGIS) is a general algorithm for efficiently synthesizing a general-purpose program. The goal is to synthesize a program that matches a logical specification: for example, to synthesize a program which has identical behavior as a slower program or a program in another language. In practice, it resolves around two insights. The first is using some way of synthesizing a program given a set of input-output pairs: this can be efficiently done with modern Satisfiability Modulo Theories (SMT) solvers. Given an SMT solver, we can synthesize a program that conforms to a particular set of inputs and outputs. But how do we go from there to fully implementing the logical specification? The next part of the trick is using the SMT solver for verification; if our program does not match our specification on some input, the SMT solver returns this input. This gives us another input/output pair which we can plug back into the synthesis system. The CEGIS algorithm, from Armando Solar-Lezama's PhD thesis. Sketching The second big idea is synthesis by sketching. For certain kinds of code, the synthesizer is much better than a human. However, many other things like boilerplate and the general structure of an algorithm are very difficult for the solver but basically trivial for a programmer. We want some easy way for the programmer to provide insight for the solver. Enter sketching. The idea is actually really simple: we let the programmer sketch out the skeleton of the code, leaving holes for the synthesizer to fill in. For example, if you're writing a transpose function, you might know that you need to use the shufps SIMD instruction and two arrays, but not know any of the specifics. You could write a sketch like this: int[16] transpose_simd(int[16] M) implements transpose {  int[16] S = 0, T = 0;   repeat (??) S[??::4] = shufps(M[??::4], M[??::4], ??);  repeat (??) T[??::4] = shufps(S[??::4], S[??::4], ??);  return T;} The synthesizer would then automatically fill in the ?? holes for you, giving you a result like this: int[16] transpose_simd(int[16] M) implements transpose {  int[16] = S = 0, T = 0;   S[4::4]  = shufps(M[6::4],  M[2::4],  11001000b);  S[0::4]  = shufps(M[11::4], M[6::4],  10010110b);  S[12::4] = shufps(M[0::4],  M[2::4],  10001101b);  S[8::4]  = shufps(M[8::4],  M[12::4], 11010111b);  T[4::4]  = shufps(M[11::4], M[1::4],  10111100b);  T[12::4] = shufps(M[3::4],  M[8::4],  11000011b);  T[8::4]  = shufps(M[4::4],  M[9::4],  11100010b);  T[0::4]  = shufps(M[12::4], M[0::4],  10110100b);  return T;} That would have been a pain to do by hand! I wrote more about program synthesis in general here. My section on program synthesis is the longest because that's also what I've been working on. So I'll take the chance to plug that project as well: Chlorophyll, a synthesis-aided compiler for low-level spatial architectures. The idea is to show how we can use program synthesis to easily implement an optimizing compiler for a really weird architecture (GreenArrays). The compiler also uses synthesis techniques to automatically partition and distribute code over multiple cores. I think it's pretty neat. Parallel Web BrowsersFinally, something of an honorary note for a project from the recent past: parallel web browsers. In particular, there was a focus on efficiently implementing a parallel layout engine using attribute grammars and program synthesis. I don't think this project is active any more, but it's interesting because it heavily influence Mozilla's Servo project. This included several of the students from the Berkeley team interning at Mozilla. I think it's neat to see research like this being put into more active development for a real project like this. Updated 54w ago • View Upvotes