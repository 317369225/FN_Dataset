Linear Algebra:Â What is the information exposed by the eigenvalues vector in a distance based graph matrix?
Very nice question. If you know the basics Jump to the conclusion: 1. Lets assume we have 2 classes. 2. Say each point is in 10000 dimension (a 100*100 image) 3. Say your train data is another 1000 (100 classes) images and now you have a test data of around 100 images. What do you do? You try any algorithm without optimizing and you would be out of computation even on a 32 GB RAM. (eg. Basic KNN,DAG etc) You main aim is to reduce the test data and here comes the role of Eigen vectors and Eigen values. PCA: "You find mean, then co-variance matrix and then subtract and then you reach equation A [dot] 'A transpose' and use some inbuilt function and find Eigen vectors and Eigen values" Lets rewind to your question what do they say. Eigen vectors are small representation of you big data. How come? Because we choose Eigen values corresponding to Eigen vectors with higher values. Why so? High Eigen values represent high variance in that direction(Direction of Eigen vector) Conclusion: Select Eigen vector for classification corresponding to high Eigen values because high Eigen values represent the vectors which we need the most, ones in dorection of maximum variance. 