Is linear algebra generally less intuitive than other mathematics?
Yes, many people find linear algebra less intuitive than calculus, at least at first. I think there are two main sources of difficulty. The first is high dimensionality. Whereas a lot of interesting calculus takes place in the theory of functions of one variable, which are easily graphed on a sheet of paper, the theory of 1-dimensional vector spaces is utterly trivial, and is therefore skipped over immediately. So the starting point of linear algebra is typically the study of linear transformations between 2-dimensional spaces — and already, the graph of such a transformation lies in a 4-dimensional space, which cannot be drawn or easily visualized. There are more complicated phenomena, too, that arise only in even higher dimensions. So beginning students, who have been trained since childhood to visualize functions almost exclusively in terms of their graphs, are immediately deprived of this crutch. Thus, they either have to come to terms with less direct forms of visualization, or rely on algebraic manipulations. The former requires significant paradigm shifts, and the latter is almost tantamount to giving up on intuition — a disastrous strategy in the long term. The second difficulty is a bit harder to describe, but call it a kind of conceptual overload. Most introductory courses on linear algebra use the same basic notation — matrices — for almost everything. This gives students the impression that they are studying the same thing over and over, so they are completely unprepared for the harsh reality that matrices are used to describe several conceptually disparate objects. For starters, I mentioned linear transformations above, but there are also bilinear forms, which are often thought of quite differently. A symmetric matrix, to take just one example, may denote a symmetric bilinear form on a bare-bones vector space, or it may denote a self-adjoint operator on a real inner product space. Without a conceptual understanding, the student is left studying various notions of "equivalence" of matrices, some given by conjugating by invertible matrices, some by multiplying by one matrix on one side and its transpose (instead of its inverse) on the other, some by both at the same time (that is, conjugating by orthogonal matrices), some by multiplying by possibly unrelated orthogonal matrices on the left and right, and so on. This is all quite bewildering and unintuitive, and unfortunately, the best way to clear up all this confusion, ultimately, is to introduce a wealth of abstractions and distinctions — dual vector spaces and dual transformations, tensor products and powers, inner product spaces versus vector spaces considered without additional structure, endomorphisms versus transformations between different spaces that merely happen to have the same dimension, and so on — which can only be intimidating to the beginning student. 