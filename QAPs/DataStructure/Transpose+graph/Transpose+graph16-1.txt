What is the mean return time on the n-dimensional boolean cube, if the stochastic process is choosing one coordinate at random and flipping it?
Experimentally, [math]2^n[/math]. We can encode the position on the n-cube as elements of [math]\{0,1\}^n[/math]. Wlog we start at [math]S=(0,\ldots,0)[/math]. If [math]X_k[/math] is a random variable denoting the number of timesteps required to return to [math]S[/math], we are interested in [math]1+E[X_1][/math]. Consider a sequence with [math]k[/math] mistakes. With probability [math]k/n[/math] we fix one mistake, and [math](n-k)/n[/math] we make another one. Hence, [math]E[X_k]=1+\frac{k}{n}E[X_{k-1}]+\frac{n-k}{n}E[X_{k+1}][/math]. Let [math]M[/math] be the zero-indexed [math]N\times N[/math] matrix with [math]M_{i,i+1}=(n-i-1)/n,0\leq i<n-1[/math] and [math]M_{i,i-1}=(i+1)/n,1\leq i<n[/math]. If [math]x[/math] is a vector with [math]x_k=E[X_k][/math], [math]1+Mx=x[/math], so [math](M-I)x=-1[/math]. Carrying out this computation hints that [math]E[X_1]=2^n-1[/math] so the mean return time is [math]2^n[/math]. Relevant code: import numpy as npN = 6M = -np.eye(N)for i in xrange(N - 1):    M[i, i + 1] = (N - i - 1.) / Nfor i in xrange(1, N):    M[i, i - 1] = (i + 1.) / Nprint np.linalg.solve(M, -np.ones((N, 1)))[0,0] + 1 