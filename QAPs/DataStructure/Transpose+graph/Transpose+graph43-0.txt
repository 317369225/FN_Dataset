What is an intuitive explanation of the Hashimoto non-backtracking matrix and its utility in network analysis?
TL;DR: The Hashimoto non-backtracking matrix is a representation of the link structure of a network that is an alternative to the usual adjacency matrix. The matrix can be used to identify non-backtracking walks on a network, which means that the walks do not proceed from a node [math]i[/math] to a node [math]j[/math] only to immediately return to node [math]i[/math]. This property of the Hashimoto matrix makes it useful for defining centrality measures that correct for the potentially exaggerated importance given to high-degree nodes in common metrics like eigenvector centrality. Other applications are outlined below.   A few quick notes before diving into details: The non-backtracking matrix was first defined by Kiichiro Hashimoto in 1989 [1].  I haven’t been able to find a copy of Hashimoto’s original paper, and my discussion is instead based on papers that apply the matrix to various problems in network analysis [2, 3, 4]. The applications that I summarize near the end of my answer are primarily from the physics literature. There is also a large body of work in the mathematics literature that uses the non-backtracking matrix. This work is less accessible to me, so it would be wonderful if someone else could complement this answer with some exploration of that literature. This ended up becoming a rather long answer, probably the longest that I’ve written on Quora so far. Sorry about that. Definition of the Matrix Let G be a graph with [math]N[/math] nodes and [math]M[/math], potentially directed, links. Let’s first review the structure of the usual adjacency matrix. This is an [math]N \times N[/math] matrix [math]A[/math] with:   [math] A_{ij} = \left\{   \begin{array}{lr}     1 & \text{if link } j \rightarrow i \text{ exists}\\     0 & \text{otherwise}   \end{array} \right. [/math]   So the adjacency matrix tells you where you can go in a single hop from any given node [math]i[/math] in the network. If your network has undirected edges, the adjacency matrix is symmetric; if your network has directed edges, the adjacency matrix will not generally be symmetric.   By contrast, the Hashimoto matrix is an [math]M \times M[/math] matrix with one row and one column for each link in the network. The matrix is defined for directed links, but it’s often applied to undirected networks by replacing each undirected link between [math] i [/math] and [math] j [/math] with a pair of directed links [math] i \rightarrow j [/math] and [math]j \rightarrow i [/math]. The matrix encodes information about sequences of links that you can follow in a walk through your network. More specifically, if you just traversed the link [math]i \rightarrow j[/math], the Hashimoto matrix tells you what links [math]k \rightarrow \ell[/math] are permissible as the next steps in your walk, explicitly excluding the possibility of immediately backtracking from [math]j \rightarrow i[/math] (hence the term non-backtracking). Mathematically, the Hashimoto matrix [math]B[/math] is given by:   [math] B_{k \rightarrow \ell, i\rightarrow j} = \left\{   \begin{array}{lr}     1 & \text{if } j = k \text{ and } i \neq \ell \\     0 & \text{otherwise}   \end{array} \right. [/math]   Visually, the non-zero entries of B correspond to link pairs that are oriented like links 1 and 3 in the following diagram:   ​   Meanwhile, the non-backtracking condition sets entries to 0 when links are oriented like links 1 and 2. A Simple Example Let’s now consider a simple example network with three nodes and four directed links: ​ The adjacency matrix for this network is given by:   [math] A = \begin{pmatrix} 0 & 1 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0\end{pmatrix} [/math]   Meanwhile, the Hashimoto matrix is:   [math] B = \begin{pmatrix} 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0\\0 & 0 & 1 & 0\end{pmatrix} [/math]   Powers of the Hashimoto matrix generate non-backtracking walks through the network. Diagonal elements of powers of [math]B[/math] correspond to closed walks that return to their starting point while fulfilling the non-backtracking condition. Let’s compute the second and third powers of [math]B[/math] to see how this plays out for our simple three-node network:   [math] B^2 = \begin{pmatrix} 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\1 & 0 & 0 & 0\end{pmatrix} [/math]   [math] B^3 = \begin{pmatrix} 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{pmatrix} [/math]   The matrix [math]B^2[/math] indicates that there are no closed, non-backtracking walks of two hops in this network. Indeed, the only closed walk of length 2 (link [math]1[/math] followed by link [math]2[/math]) violates the non-backtracking condition. Since no walk of two hops can be both closed and non-backtracking, this is generally true in any network. On the other hand, the matrix [math]B^3[/math] indicates that there are three closed, non-backtracking walks of three hops in the network. These are the three walks that begin at links [math]1[/math], [math]2[/math], and [math]3[/math] and circle the triangle.   It’s worth momentarily considering a slightly more complicated example network of [math]4[/math] nodes and [math]7[/math] links:   ​   If you construct the [math]7 \times 7[/math] Hashimoto matrix for this link and compute its powers, you’ll find that a closed walk of the form [math]1 \rightarrow 3 \rightarrow 5 \rightarrow 6 \rightarrow 2[/math] contributes to the first diagonal element of [math]B^5[/math]. This is notable because the walk does eventually traverse the antiparallel links [math]1[/math] and [math]2[/math]. This still satisfies the non-backtracking condition because those two links are not consecutive in the walk. Some Applications I now want to discuss a few recent applications of the Hashimoto non-backtracking matrix to give a better sense of why it’s useful in network analysis.   Localization and Centrality in Networks T. Martin, X. Zhang, and M. E. J. Newman. Phys. Rev. E. 90, 052808 (2014). [2]   This paper discusses:   pathological behavior of eigenvector centrality in certain classes of networks with undirected links how the Hashimoto matrix can be used to resolve this pathology. That being the case, let’s review the usual definition of eigenvector centrality. This metric is based on the intuition that a node’s importance in a network is determined by how important its neighbors are. We can imagine initializing the centrality of each node in a network to [math]1[/math] and then iteratively updating the centrality of each node to be the sum of the centralities of its neighbors’ centralities. For an undirected network, this iterative update is given by:   [math]x_{n+1} = Ax_n[/math]   where [math]A[/math] is the adjacency matrix and [math]x_n[/math] is an [math]N[/math] dimensional vector containing the centrality estimates after the [math]n[/math] iteration. Eventually, the relative weights of the centrality measures in [math]x_n[/math] should converge to those of the leading eigenvector (i.e., the one with the leading eigenvalue) of [math]A[/math]. We can visualize the first steps of this process for a simple undirected network here:   ​   (As an aside, note that this method of beginning with all centralities equal to [math]1[/math] and performing iterative updates is called the power method. It generally works for undirected networks, because of the nice properties of a symmetric adjacency matrix. It doesn’t generally work in the directed case, where the adjacency matrix can be asymmetric. In either case, you can calculate eigenvector centrality by explicitly finding the eigenvector corresponding to the largest eigenvalue. Note also that, in the directed case, you should find the leading right eigenvector of the transpose of [math] A [/math] - or the leading left eigenvector of [math] A [/math] - to accumulate contributions from incoming links.)   The example illustrated above begins to hint at the problem that Martin et al. note with eigenvector centrality. The central node in this example passes its centrality along to its neighbors, and then, in the next step, that centrality gets reflected back to the central node. This repeated “reflection” can result in a heavy, and perhaps unjustifiably large, accumulation of eigenvector centrality near high-degree nodes in a network. (Note that this problem can occur in directed networks too, but it’s most acute in undirected networks where every link can carry centrality back and forth.)   Concretely, Martin et al. consider a random network of [math]N-1[/math] nodes and mean degree [math]d[/math] to which they add a single hub node of degree [math]c[/math]. They then show that, if [math]c[/math] is sufficiently large, nearly all of the eigenvector centrality gets concentrated on the hub node and its neighbors. The typical node in the network gets no weight as [math]N \rightarrow \infty[/math]. The authors argue that this is a pathological consequence of the “reflection” mentioned above.   The solution proposed by Martin et al. is to find the leading eigenvector of the Hashimoto matrix instead. Let’s call this leading eigenvector [math]\vec{b}[/math]. The idea here is that the element of this eigenvector corresponding to the link [math]i \rightarrow j[/math] can be interpreted as an eigenvector centrality of node [math]i[/math] that neglects any contribution from node [math]j[/math]. With this eigenvector in hand, we can compute the full centrality of node j by summing over all neighbors [math]i[/math] of j:   [math]c_j = \sum_i b_{i \rightarrow j}[/math]   According to Martin et al., this actually better captures the intuition that we actually hope to capture with eigenvector centrality: when we ask what importance our neighbors pass on to us, we really want to consider how important our neighbors would be in our absence. The authors also show that the centrality defined in this way avoids the “localization” that affects eigenvector centrality in certain regimes.   Spectral Redemption: Clustering Sparse Networks F. Krzakala et al. PNAS 110, 52 (2013). [3]   This paper concerns the question of community detection in networks. Suppose the nodes in the network belong to one of several communities that exhibit homophily: nodes in the same community are more likely to link to one another than to nodes outside the community. Suppose the community labels are hidden from us and we want to infer them. How can we use information encoded in the link structure to achieve this?   While the leading eigenvector of the adjacency matrix gives the eigenvector centrality of the nodes, there’s another eigenvector that’s typically correlated with the community structure. This eigenvector can, therefore, be used for “spectral clustering.” When homophily is strong, the eigenvalue associated with this eigenvector is clearly separated from the bulk of the eigenvalue spectrum, which approximately follows a Wigner semicircle law. As such, it’s easy to identify the community-correlated eigenvector and infer communities. However, when homophily is weaker, the relevant eigenvalue disappears into the bulk of the spectrum, making community detection impossible (when there are just two communities) or very difficult (when there are more than two). There’s a theoretical limit beyond which this is inevitable, called the detectability threshold. Spectral algorithms based on the adjacency matrix (or other related matrices like the modularity matrix [5]) tend to fail before reaching this theoretical limit though. The reason is that the edges of the bulk of the spectrum are "smeared" by the influence of hub nodes, making it easier for the special, community-correlated eigenvalue to get lost amidst these hub-dominated eigenvalues. Here's a numerical example from reference [3] of a spectrum's deviations from the semicircle law and how those deviations swamp the theoretically expected location of the community-correlated eigenvector (indicated by [math]\lambda_c[/math]): ​ Krzakala et al. show that a spectral algorithm based on the Hashimoto matrix works in this intermediate regime and allows for community detection down to the detectability threshold. There’s an approximate eigenvector of the Hashimoto matrix that’s highly correlated with the community-correlated eigenvector of the adjacency matrix. Using the Hashimoto matrix redeems spectral clustering because the non-reflection property reduces the influence of hubs, and thereby reduces the smearing of the edges of the bulk of the spectrum.   Percolation on Sparse Networks B. Karrer, M. E. J. Newman, and L. Zdebrova. Phys. Rev. Lett. 113, 208702 (2014). [4]   This paper is about identifying the percolation threshold for random networks. Imagine a network of [math]N[/math] nodes and [math]M[/math] potential, undirected links, where each potential link is activated randomly with probability [math]p[/math]. Above some link probability [math]p_c[/math], there exists a component of nodes connected by activated links that occupies a finite fraction of the nodes as [math]N \rightarrow \infty[/math]. Below [math]p_c[/math], no such macroscopic cluster forms as the size of the lattice grows. The authors of this paper show that you can get a a good estimate of [math]p_c[/math] by taking the inverse of the leading eigenvalue of the Hashimoto non-backtracking matrix for the network.   The idea here is to compute the probability [math]\pi_i(s)[/math] that a node [math]i[/math] belongs to a finite cluster of size [math]s[/math]. This can be expressed in terms of the probabilities [math]\pi_{i\rightarrow j}(s_j)[/math] that [math]s_j[/math] nodes are reachable from [math]i[/math] via the links [math]i \rightarrow j[/math], for each of the potential neighbors [math]j[/math] of [math]i[/math]. In computing the number of nodes accessible from [math]i[/math] in this way, the main approximation that we need to make is that the structure of the lattice (of potential links) is tree-like, so that the same nodes are not reachable from two neighbors [math]j_1[/math] and [math]j_2[/math]. Continuing with this tree approximation, [math]\pi_{i\rightarrow j}(s_j)[/math] can itself be expressed in terms of [math]\pi_{j\rightarrow k}(s_k)[/math] for each of the neighbors [math]k[/math] of [math]j[/math], excluding [math]i[/math]. The Hashimoto matrix, whose definition also considers links leading from a node to each of each of its neighbors but one, emerges naturally from this reasoning. See the text of the paper for details.   Although the approximations used by Karrer et al. are strictly valid only for tree-like networks, the authors show numerically that, in practice, the inverse of the leading eigenvalue of the Hashimoto matrix works well for real-world networks. In the following figure from [4], Karrer et al. show the performance of the method based on the Hashimoto matrix (solid lines) compared to direct numerical simulation (red circles for the fraction of the network occupied by the largest component, blue squares for the average size of subleasing components) for Erdös-Rényi random graphs and two empirical networks: ​ [1] K. Hashimoto. Zeta functions of finite graphs and representations of p-adic groups. Adv. Stud. Pure Math. 15, 211–280 (1989). [2] T. Martin, X. Zhang, and M. E. J. Newman. Localization and Centrality in Networks. Phys. Rev. E. 90, 052808 (2014). [3] F. Krzakala et al. Spectral Redemption: Clustering Sparse Networks. PNAS 110, 52 (2013). [4] B. Karrer, M. E. J. Newman, and L. Zdebrova. Percolation on Sparse Networks. Phys. Rev. Lett. 113, 208702 (2014). [5] M. E. J. Newman. Networks: An Introduction. Chapter 11: pgs. 375-380. Updated 32w ago • View Upvotes