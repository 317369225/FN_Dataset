Why do Hash Tables need to store the Key of a record?
This turns out to be a surprisingly involved question, with multiple answers depending on the question. But essentially, the question that you're asking is: What do we actually need to store in a hash table entry? We're going to have to get deep into some maths here, because the answer to this question requires being precise about the word "need". I'm going to make a few simplifying assumptions: The "keys" for your hash table are integers chosen from a universe of n-bit integers. There are, obviously, [math]2^n[/math] possible integers that you might want to store. Note that if your keys don't have this structure (e.g. strings), then things automatically get more complicated. The set of strings is unbounded, and they don't even have the same size. Clearly if you're trying to shove a subset of an infinite set into a finite table, you're going to need to store some key material. The number of keys that you want to store in the hash table is [math]2^m[/math] for some integer m. [math]2^m[/math] is also the hash table size, so the load factor (i.e. the average number of keys per hash slot) is 1. It should be obvious that if don't have a power of two keys to store, you could just round up to the next power of 2. The subset of all possible keys which will be used for queries is not known in advance.That last one may surprise you, but there's actually a whole literature on "perfect hashing", including "minimal perfect hashing". If the set of keys that you're planning to query the hash table with is known in advance, you can construct a hash function which ensures that if [math]k_1 \ne k_2[/math], [math]h(k_1) \ne h(k_2)[/math] (that is, the hash function is injective). If you can arrange that, then clearly you don't need to store keys, and so the hash function is "perfect". If the hash function is also surjective, then it's "minimal perfect". Note that this also means that the hash function is invertible, but you don't need to "know" the inverse if all you're trying to do is test keys for membership. As an aside, in practice you can get pretty close to "perfect" using cryptographic hash functions. Many cryptographic hash functions have a very large codomain (i.e. so much bigger than any set of keys you're going to throw at it that this won't be an issue) and no known clashes (i.e. there are no known cases of two strings hashing to the same value). "Pretty close" here means that you can rely on it it for most security purposes. All right. Now let's get back to investigating what you can do given those simplifying assumptions. Let's start with a simple hash table, with [math]2^m[/math] keys stored in [math]2^m[/math] hash slots, and some mechanism for storing multiple keys in a hash slot (e.g. separate chaining). Assuming you have a good hash function, then how many keys do you expect there to be in any given hash slot? Clearly, the average is 1. But the distribution should follow a Poisson distribution. I'll spare you the details, but it means that roughly [math]\frac{1}{e}[/math] of the slots will have no keys, [math]\frac{1}{e}[/math] of them will have one key, and in general, [math]\frac{1}{e k!}[/math] of them will have k keys. As you probably know, [math]e \approx 2.71828[/math], and so [math]\frac{1}{e}[/math] is a bit over a third. So a bit over a third of the slots will have no keys, a bit over a third will have one, and a bit over a quarter will have more than one. That means that a lot of hash slots are effectively being wasted, and a quarter of the queries will involve checking multiple keys. Looking at the inverse problem, how many hash slots would you need for there to be a pretty good chance of no slots having more than one key? The answer, and this is a good rule of thumb that every programmer needs to know, is that if there are [math]M[/math] pigeonholes, and you start randomly assigning pigeons to pigeonholes, there's a 50% chance of more than one pigeon ending up in the same pigeonhole after assigning [math]\sqrt{M}[/math] pigeons. (The badly-named "birthday paradox" is basically the same idea. In any room of [math]\sqrt{365} \approx 19[/math] people, there is a 50% chance that two people share a birthday. At around 23 people or so, it's close to guaranteed.) But all is not lost. Cuckoo hashing is a relatively new technique which lets you use close enough to all of a hash table, by judicious use of multiple hash functions. I'm not going into the full details (there's a lot of literature on it), but the point is that in practice, we can get very close to an ideal hash table with load factor 1: exactly one key per hash slot. So let's add the further assumption that this is what we really can do. To recap, we have a hash table with the following properties: Keys are n-bit integers, that is, the universe of possible keys has size [math]2^n[/math]. We have [math]2^m[/math] keys stored in [math]2^m[/math] hash slots, with no collisions.Before we go on, I might answer a question you might have: How often in practice do you need to hash n-bit integers for some n? The answer is "surprisingly often". Operating systems often hash addresses, and in bioinformatics you're often hashing fixed-length DNA sequences. So yeah, it happens. All right. That was the easy part. Now let's get into information theory. Suppose you have an array of N things. Suppose you want to tell someone else where in the array some specific element is. How much do you actually have to say? Well, you have to specify a number between 1 and N (or between 0 and N-1, since we're computer scientists). On a binary computer, storing a number between 1 and N needs [math]\log N[/math] bits of storage. (All logarithms are base 2 unless otherwise specified, because once again, we're computer scientists.) This is also the amount of information that you need to transmit to tell someone else. This is the definition of "need" that we're going to use. Incidentally, this also gives you a bound on how efficient certain algorithms can be. Suppose you are trying to find an element of the array, and only have a "probe" operation which takes some stuff and returns a boolean true/false value. (Note that comparing two elements, say using less-than or less-than-or-equal-to, is such an operation.) A boolean value is one bit of information. You are trying to discover [math]\log N[/math] bits. Therefore, you need to do at least [math]\log N[/math] "probes" to find any element. You cannot do better than this! Interestingly, binary search achieves this lower bound exactly, so in that sense, binary search is provably optimal. Just so you get the idea, let's also try sorting. We assume you have an array of elements where the values are all different. You have a "probe" operation which returns one bit of information (again, say, a comparison). What you're trying to discover is a permutation of the array. There are [math]N![/math] permutations of the array, so you are trying to find a number between 1 and [math]N![/math]. So you need at least [math]\log N![/math] bits of information, which means you need to do at least [math]\log N![/math] probes. Using Stirling's approximation, you will find that [math]\log N! = O(N \log N)[/math], so if comparison is all you have, you need to do at least this many comparisons. A sort algorithm which uses [math]O(N \log N)[/math] comparisons is optimal. Note that if you have a probe operation which gives you more than one bit of information, you don't need as many probes to get the same amount of information. This is why radix sort uses fewer probes: the probe operation (radix extraction) gives you more than one bit. OK, that's all the information theory that we need. Now let's analyse our hash table. Abstractly, you have a space of [math]2^n[/math] possible elements, and you want to transmit to someone else a subset of them with size [math]2^m[/math]. How many bits of information do you need to transmit? There are [math]{2^n \choose 2^m}[/math] possible subsets. So you need to transmit a number between 1 and [math]{2^n \choose 2^m}[/math]. That is, you need to transmit [math]\log {2^n \choose 2^m}[/math] bits of information. Again using Stirling's approximation, a little thought, and a bit of algebra, you will find that if [math]2^m[/math] is much less than [math]2^n[/math], you need roughly [math](n-m) 2^m[/math] bits of information. So for any data structure, hash table or otherwise, you need at least that many bits to represent the set. Can we do it? Yes, we can. And the resulting data structure shows that your intuition was kind of correct. Under some reasonable assumptions, we don't need to store the key. Moreover, under those assumptions, you only have to store information that's actually required. Let me explain what I mean by that. Suppose that you have a key k (which, you will recall, is an n-bit integer). Let's hash it, and we will suppose that the hash value is also an n-bit integer (the reason for this will become clear soon). There are [math]2^m[/math] hash table slots, so we can simply take the m least significant bits of the hash value and use that as the slot number. Now suppose we want to store the hash value in the slot. We don't have to store the whole thing! Only hash values whose m least significant bits are i will be stored in hash slot i. We only have to store the other n-m bits in the slot. So, there are [math]2^m[/math] hash slots, and we only have to store [math]n-m[/math] bits in each... and that's [math](n-m)2^m[/math] bits. So if the hash function is invertible, you can arrange things so that the key never needs to be stored in the hash table. If you can compute the inverse, then you can even enumerate all of the keys in the hash table even though they are not stored. Moreover, it's an optimal representation, in the sense that there is no data structure which is smaller. So that's why we made the hash value an n-bit integer. Invertible perfect hash functions from n-bit integers to n-bit integers are surprisingly easy to construct. For example, you could use a Feistel network with one or two rounds to turn any non-invertible hash function into an invertible one. We finally got there. The answer to your question, do we need to store the key in a hash table, is "under certain common reasonable conditions, no". And the answer to the question you didn't ask, what do you actually need to store in a hash entry, is "not as much as you think, and it actually can be done". Final question: Is it actually a practical hash table? Well, as always, it's a tradeoff. Evaluating a perfect invertible hash function is invariably going to be more expensive than a decent modern general-purpose hash function like Murmur or CityHash. But on the other hand, if your hash table is so big that the extra compression means that it now fits in memory, that's a clear win. It has been put into practice. Succinct hashing is used in several bioinformatics systems for storing fixed-size DNA sequences, such as Jellyfish and Gossamer. (Full disclosure: I worked on Gossamer.) Final thought: Of course, this isn't the only "optimal" representation. You can do it using techniques other than hashing, too, and still retain roughly constant-time access. But you asked about hash tables, and this is the answer. Now aren't you glad you asked? 