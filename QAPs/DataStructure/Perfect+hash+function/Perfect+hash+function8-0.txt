How is the size of hash table determined?
Speed is usually more important than memory. Empty cells in the hash table cost you only a little. Program with an average of 5 items per hash table cell will, on average, be about 3-4 times as slow as a program with just 1 item per cell, because of the additional comparisons it has to do. You want a hash table that is larger than the number of keys you are going to store, so that the expected number of items per cell is less than 1. The particular hash function does not matter much if the input is guaranteed to be English words -- many functions will behave nicely enough for those. That being said, the simplest solution for your problem is to stop reinventing the wheel: if you can, just use an existing solution, such as the unordered_set in C++11 with its default hash function for strings, and everything will work just fine. Of course, if you know the set of words beforehand, the optimal solution is to use a Perfect hash function with no collisions at all. There is even a tool to find one for you: gperf(1) - Linux man page. 