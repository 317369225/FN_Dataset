Could advanced machine learning put data scientists out of the business?
On the surface, this question seems perfectly rational in the context of computer science.  As examples, compiler optimization has largely eliminated the need for handwritten assembly, and the hash functions in java are unknown to most developers, which is fine for most applications.  It begs the question: How far does machine learning (specifically modeling) have to go to put feature extraction, feature engineering, model selection, etc. out of business? The answer is: really really far. First of all, practical machine learning applications break nearly all of the fundamental assumptions of machine learning (independent & identically distributed features, independence among all training examples, etc.).  Much time is spent trying to transform the data in a way that better validates these assumptions.  There are many techniques for doing this and they are difficult to mechanize. Second, there are many types of models (linear regression, decision trees, neural networks, etc.), and how to best transform your data to work with any of these models requires an understanding of the model optimization loop that is difficult to quantify. Last but not least, there is a human element to machine learning that will always require some level of data science.  The function that one is trying to optimize is never entirely objective.  For example, the business might want to optimize revenue but not at the expense of the company's brand.  Understanding how to quantify abstract concepts such as "brand" requires human evaluation and tweaking of the objective function.  Most of the challenge from machine learning comes from attempting to quantify what the true goals of the system are, and this challenge will almost certainly involve one or more data scientists. 