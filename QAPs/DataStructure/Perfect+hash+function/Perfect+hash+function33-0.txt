Why would anyone like to use a red black tree when a hash table can do the job perfectly?
The points mentioned so far for Red Black tree over hash function are: Maintaining order (knowing first/next, iterating in order) Better chance of maintaining locality between consecutive insertions: less I/O hits (a hash table would swap in all elements in a bucket for lookup efficiency, plus there is the 'random' angle to hashes due to which it is difficult to preserve locality) Table Resizing issues: sudden performance drop and memory management issues I'll branch from Tim's answer to add another point, so you might want to read that first. The lookup operations cease to be O(1) if the bucket in the hash table has many entries. E.g: if I had a series of n inputs to the hash table, all of which hashed to the same bucket then that one bucket will have O(n) lookup time, which adds to insert time in the same bucket, since you first need to verify that the element you are trying to insert is not already present. This depends on the actual choice of the hash function. It's assumed that the chosen hash function will "randomize" your inputs well enough but good hash functions require more computation as well. Choosing a weaker hash function might give issues with some set of input patterns. A DOS attack on Btrfs based on CRC32 hash function (which seemingly has hw support) does a good analysis of this: A hash-based DOS attack on Btrfs I think there is an underlying theme of maintaining order vs. randomizing, and someone with more clarity could tie these things perhaps. Thanks for asking the question, I had recently come across the Btrfs piece and this sounded like the perfect place to share it. 