How does beam search work in seq2seq RNN models? In seq2seq models, the decoder is conditioned on a sentence encoding to generate a sentence. To that end, words of the final sentence are generated one by one in each time step of the decoder¡¯s recurrence.

You should note that at each time step, decoder¡¯s view of what should be generated is a probability distribution over your vocabulary via the softmax layer. Therefore at time t, the decoder needs to make a decision on what word to generate as the t-th word of your final sentence. The problem here is that, you do not know the exact sequence of words you would need to generate in order to maximize the overall conditional probability distribution. A locally greedy choice i.e. choosing the word with highest score at time t is not necessarily going to give you the sentence with highest probability given your conditioned sentence encoding.

For that reason, people usually use beam search. Beam search has a width of m such that at each time step it takes the top m proposal and continues the decoding with each one of them. You can imagine the search tree structure that this would produce. A special case, and something I have already discussed in the previous paragraph, is that when the width of the beam search is 1, we simply call it the ¡°greedy search¡±.