Is a rigorous formal understanding of the big-O/theta/omega notation necessary to apply it successfully in computer science?
Depends if you mean "computer science", as in the theory and what is done in schools, or you mean "computer programming", as in what is done in the real world. I would call myself successful in the latter. I know what big O means, and I know the big O costs of the normal stuff like hash lookups, binary searches, most of the major sorts, and the worst case costs of them as well. I can figure out the big O cost of something new by devolving it into something I already know. That's good enough for me. 