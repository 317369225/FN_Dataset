Is a rigorous formal understanding of the big-O/theta/omega notation necessary to apply it successfully in computer science?
There exists, of course, a fully precise definition of all of these notations, and this is worth knowing if only to understand their limitations. However for the most part, particularly in runtime analysis, the notation is intuitive: it ignores multiplicative constants and terms which grow asymptotically slower than the main term, so 8x 3 +2x 2 +7  is Θ(x 3 )  . Outside of rigorous algorithmic analysis it usually suffices to consider the algorithm and 'count' how many times each statement is executed in relation to the input size, and the fastest-growing term is your asymptotic runtime. 