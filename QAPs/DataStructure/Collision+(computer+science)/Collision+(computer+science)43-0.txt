How can we collect big data from the internet in general?For economists, the prevalence of negative words like “inflation” or “unemployment” might signal a recession. Epidemiologists can predict a widespread influenza outbreak in a similar fashion. How can we search the whole internet (just as Google does) so that analysis can be done? Does there exist a cheap/free program for this purpose?
Edit: This question was originally entitled: how can we collect big data from the entire Internet in general? My answer was in response to that. Searching the entire Internet is impossible - even Google only indexes about 12% of the internet, according to best estimates. There are entire regions that are inaccessible to crawlers - HTTPS-protected websites, for instance, take extra care to ensure that search engine crawlers can't get in, while the Dark Web, as the giant unindexed portion of the web is called, is a hotbed of illegal criminal activity and quite frankly dangerous to wander around on - you never know what sort of malicious virus you might accidentally download. How Data Collection Works - and Why What You're Asking is Impossible Since you've asked how to collect information from the internet in general, here's what you have to understand about how crawlers, the workhorses of search engines, work to understand why what you're asking is beyond our ability: Crawlers are simple: they are programs that start on a webpage, collect all the URLs on the webpage, visit each URL in turn, collect all the URLS on that URL, and keep repeating. How they do this is not that interesting - there are a lot of tools, such as cURL, that can get pages for you and tools like BeautifulSoup that can 'read' pages to a crawler. They may scrape content from the webpage, or they may simply be happy with keeping a list of URLs - in either case, it's easy to imagine it using this graph: Each node/value in this graph represents a webpage (say). One way a crawler can operate is as follows: a crawler starts at the webpage marked 'World', and notes that it has URLs pointing to Italy, Zimbabwe, Australia ,China and Honduras, respectively. Suppose the crawler is designed so it always chooses the first URL it encounters to visit - in our graph, that would correspond to Italy. Fine. It goes to Italy, sees it has links to Venezia, Florence, etc., visits them all, and only once this entire branch has been exhausted does it finally go back to the beginning and try to do the same with Zimbabwe. In computer science, this is called Depth-first search. (Another way a crawler can visit web pages is to visit Italy, then China, then Honduras, and then move on to Venezia, Florence ... , Hazare, Hwange, .. Shanghai ... etc. Basically, it can visit all the nodes at each level before moving on to the next level. This is called Breadth-first search, and is the preferred way for sensible developers to build their crawlers, for reasons explained below.) At every level, a crawler needs access to a database of collected URLs. This collection of URLs should sensibly be built in a way so that it can scale pretty quickly and easily to host what you need. Now, suppose you wanted to build crawlers that could scrape the entire Internet. We've been working thus far with a very small and finite graph - but what do you do when the graph becomes extremely large? Google claims to be able to index at least a trillion webpages - but if that's only 12% of the internet, that means there are about 8.3 quadrillion pages out there. To be able to host this sort of capacity, you need a lot of storage space. If every website is only 1 KB, that means you need 1 exabyte of space to store it somehow.  In other words, you need 1,000,000,000 gigabytes of space. To put this in perspective, the largest data store in construction is IBM's data store, which can host 120 petabytes - a tenth of an exabyte. (120 petabytes: IBM building largest data storage array ever). Perhaps there are ways around this, though? Maybe you've heard of cloud/distributed computing, where you employ other people's computers remotely for extra memory and space. This allows you to expand your memory limitations - instead of one large memory bank on one computer, you use a little memory from a lot of other computers that together add up to a larger memory. Google is perhaps a world leader at this sort of thing, where working with big data is part and parcel of daily life. Even with that sort of power, however, ArsTechnica claims Google can only process 20 petabytes a day. (The Great Disk Drive in the Sky: How Web giants store big—and we mean big—data). There are other challenges. In order to be relevant, you'd have to track changes on a website every single second. That means you need to run crawlers millions of times just to be able to see what changes have occurred. That eats up a lot of overhead, and time - people have worked around this in multiple ways, mainly by being clever and using something called a Hash function (or just a hash) to be able to avoid crawling through the whole list of indexes again, but you still need a large number of crawlers visiting each page individually to check for changes. Hashes basically allow you to jump straight to a visited node instead of starting from a root node and working your way down. If you have a large number of crawlers, you can generally do this fairly quickly - but doing this for the 8.3 quadrillion URLs is impossible. Most computers allow you to run one or more crawlers at once using something called Multithreading (computer architecture) or multiprocessing, where you assign more than one process to another core of your computer, but there is an upper limit to how much more performance you can squeeze out of an additional process. I don't have numbers with me, but I can safely say that a quadrillion crawlers is impossible to have simultaneously running without sever performance issues. Another challenge is intelligent crawler design. It's very easy to ignore a problem like: how do we prevent a crawler from getting trapped in an infinite loop? Say Venice has a link to Zimbabawe, which also has a link back to Venice. When our crawler visits Venice, it goes to Zimbabwe, where it sees Venice again, where it goes back to Zimbabwe again, and - well, you get the picture. The key again is a hash function, but designing a good one isn't simple to do naively (the core of a hash is in its ability to prevent collisions - basically, two hashes must not point to the same URL, and it's not that straightforward to do on big datasets), and Google has gone through several iterations of its hash function (it open-sourced its early version of such a function cityhash - The CityHash family of hash functions - Google Project Hosting a long time ago, and has moved on to much better sources). Crawlers can also really cause problems for other people. If you have a lot of crawlers going to a webpage lots of times, the server hosting the webpage can become overloaded with requests and simply refuse to respond (this is a distributed denial of service attack, the easiest way to shut down a web page). People have had problems with Google crawlers rapaciously slowing down their website's response times, and I know of at least one case where Google's crawlers really did en up inadvertently causing a DDoS event. Another last issue is that crawlers are best designed for visiting websites that aren't protected by an authentication protocol. This means a lot of sensitive websites - bank websites where you can access your account, for instance - don't let crawlers in, so you can't scrape or collect information. This also means a large number of Facebook pages can't be accessed, because it is also HTTPS-protected, alongside a number of other websites (for instance, you can't access other people's Gmails). Summary To scrape the entire web, you need to know how to work with multithreaded crawlers that can operate either on shared or distributed memory, come up with an intelligent hashing functions that guarantees no collisions for the entire internet, have an architecture large enough to sustain several hundreds of millions of simultaneous programs that don't eat at memory or cause your computer's response time to go down, and above all be sensible enough not to overload servers. Your system would have to be able to continuously expand as more and more web pages are added to the net, and have to somehow exceed the abilities of the best and the brightest at Google and IBM combined. Finally, you'd have to pay for all of this - none of this is cheap to implement or run. Now do you see why a cheap/free program is impossible? Anybody who could pull this off would not have the money to be able to afford keeping this free for any sustained period of time. What People Usually Do Instead In talking about all of this, you have to ask:: is crawling the entire web useful? Not really: it might be useful for a search engine, where information needs to be found, but to the end user - you - whose stated aim is to, say, find out how trends in usage of terms like 'influenza' affects the world, a lot of the data is useless. You might be maintaining a list of websites that talk about gazelles, which has nothing to do with influenza, and which are utterly irrelevant to what you're looking for. Good search is often accompanied by a strong understanding of the limitations of your method - if you want to get good results, you have to have a better methodology. So people instead use publicly available data collected from social networking sites - sites where people actually go to talk about or discuss events. Twitter allows people to collect and harvest tweets from lots and lots of people, which Facebook also allows external web applications to collect (with appropriate permissions from the user) status texts posted by people. It's much more sensible to start with what you're looking for and then go hunting in places more likely to have that than to go hunt everything, and then come up with something to look for. To my knowledge, there are no free cheap and easy tools to be able to query Twitter and/or Facebook at once. Twitter and Facebook provide APIs - interfaces - for your application or program to collect the data - there are no dashboards or consoles you can type your query in and get an instant response from. You have to devote the bare minimum involved in learning how to  build such tools that connect to Facebook. So How Would I Build Something Like This? I've outlined the basic computer science theory and issues behind building a good data scraping programming - any specific implementation will require deep knowledge of a programming language and adequate access to tools that can do the rough work for you. Some advice? Use a combination of Python, Spark and Amazon AWS for best results. Python has the requests module, which can fetch web pages for you, and BeautifulSoup module, which is incredibly useful for parsing a web page (not to mention Python is easy to pick up and can be optimised for really fast work). Spark is a distributed engine that lets you multiply the number of crawlers you use and comes bundled with a good framework for storing memory and expanding it when necessary. Finally, Amazon AWS lets you run a large number of computers at once (a cluster), which Spark divides memory between and assigns each to handle a different URL. You will need a good database - I would ordinarily recommend MongoDB, but I have it on good authority that it can't really scale to meet the challenges of scraping several million webpages, so you'll have to look at different options and figure things out yourself. If you wanted to simply connect to Twitter's database/ Facebook's database using their API, however, that's much easier to do. There are lots of questions and StackOverflow questions that can guide you through that process. Good luck! Updated 15 Apr • View Upvotes