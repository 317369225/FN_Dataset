Is a rigorous formal understanding of the big-O/theta/omega notation necessary to apply it successfully in computer science?
Beware of using Maths stuff without understanding what it means. (Statistics is often used wrong, for example.) Big-O is quite easy to apply wrong. For example, consider recursive quicksort. With constant input it certainly performs in O(n). Otherwise, by induction, you do O(n) work thrice, which is O(n) together. Also, the constants may creep up, so in practice the best Big-O may not give the fastest thing (even if we disregard worst case not happening). For example, anyone can find a SHA-1 collision in O(1) time. The number of SHA-1 outputs is constant, so trying just 1+2^160 different inputs will produce a collision for sure. 