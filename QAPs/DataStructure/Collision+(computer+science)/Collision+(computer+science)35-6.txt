Is a rigorous formal understanding of the big-O/theta/omega notation necessary to apply it successfully in computer science?
