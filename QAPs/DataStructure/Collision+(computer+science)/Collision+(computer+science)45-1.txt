Why do we have the hierarchical principle in adding interactions to a model? What is the significance of it?This is specific to modelling. And in general linear regression models. Textbook says that even if the main effect is insignificant, one should add it if there is an interaction term with that. Why?
I'm assuming for this answer that your application of hierarchy is consistent with the data in question as it appears out in the world, since hierarchy effects the expression of data. Hierarchy has a number of purposes in statistics--in a very general sense, the hierarchy imposes a structure on the data. Deciding on a hierarchy allows the data the be structured so that interactions are considered relative to the effect of independent variables on some outcome. As I'm sure you know, a lot of linear statistical methods measure not the actual values of the independent and dependent variables, but the structure of changes in the specified relation (X --> Y). Order matters in this relation for a variety of reasons: the ability to analyze (statistics require categories for comparison in some relation) coefficients change based on the presumptive directionality of the relation directionality is based on dependency (data is nested in some inherent order which effects the expression of variables) an implied relationship to time (event --> consequences) an implied relationship to causality* (is the independent variable necessary in the relation) The use of linear methods requires hierarchy. Linear models are absolutely dependent on some set, specified relation in the same way that the line equations they're modeled on require X and Y to maintain a set relation in order to calculate tendencies between data points. x=ym+b≠y=mx+b  In computer science, hierarchy has some general, additional uses for creating/handling data. As I'm sure you know, speed and organization are (major) issues in computer science. Assigning hierarchy is a tool allows the following: simplification of execution speed of access (by limiting the necessary iterations, instructions or components) discretization of problems through assigning structure** the use of parallel processes for speed the creation of analyzable categories and assignment of qualities for analysis In terms of interaction detection, these qualities become considerably more important. Interaction detection is something of a general term, in my reading, used to describe anything from detecting and modelling packet interference due to conflicting assumptions in complex protocols, to detecting collisions between rendered objects and generating trees to model those collisions. In a very general way, hierarchy allows the following for interaction detection: speed of detection (by reducing the necessary steps to detect interaction) simplification of detection discretization of the detection process (which is a necessary step in modeling) the use of parallel processes for detection the creation of analyzable categories for interaction and assignment of qualities for analysis Coming back to statistics and creating graphs and models, hierarchy allows you to assign relationships to the interactions you're modelling. This creates a direction of relation, allows you to usefully truncate the data based on the hierarchy and implied relation (discretization is the formal term, but what you're essentially doing is truncating the data for the purposes of creating structure), allows you to engage in comparisons, and allows you to model using time.*** What do I mean by truncation, here? Think of a set of data as a potentially chaotic set, with the formal definition of set (no implied relation or internal order, discretizable objects). In order to make heads or tails of the information you have, that set has to have some relation imposed on it--this is usually imposed by looking for common elements in the set. The process of looking for common elements is considerably faster when the set can be broken into smaller subsets. If the objects can be broken into smaller subsets, looking for common features can be aggregated through looking at those subsets. This is what linear statistics is doing. The hierarchy is imposed, and allows you to make subsets and aggregate their relations looking for similarities. Statistics programs usually run thousands of subsets in the aggregation process for the sake of dealing with potentially problematic (unrepresentative) variation in the set. This is why you can run the same data in the same program and get different results when you run it. It's also why you usually run the data repeatedly to get a sense for consistency in your results. The resulting structure, after that aggregation process, has eliminated superfluous relations down to only the relations shared during the aggregation process. The program creating the graph or model does get rid of information for the sake of creating a simplified relation as it aggregates, but it does so in the service of removing dissimilarities. The tl;dr version: you can't make any sense out of data without structure and the ability to aggregate relations thorough looking for common elements. Hierarchy is a structure that is useful for making sense out of some data because it simplifies the process of looking for relations. ___________________________________ * Hierarchical modeling is not sufficient by itself to determine this, but it may suggest it, to be confirmed with other methods. ** This does cause some information loss when imposing the structure on complex problems. *** Modeling without using time is, to the best of my knowledge, impossible. 