Why is the stack of RBM's after learning not a deep Boltzmann machine (DBM)?If a bunch of stacked RBMs aren't a DBM, then what is? See Page on toronto.edu section 3.1. Hinton at el. are pretty explicit that a stack of RBM's are not a DBM.
According to the Deep Belief Network paper,  a Restricted Boltzmann Machines(RBMs) is an infinite stack of sigmoid belief networks with shared weight. You can understand this concept by  comparing Gibbs sampling process of an RBM with the Gibbs sampling process of unrolled Belief networks.  For example, think about what is a Gibbs sampling. Gibbs sampling is a way of sampling method that generate samples with the desired distribution after a certain number of burn-in steps(mixing rate), which mainly depends on the shape of the target distribution. So, when looking into the two step blocked Gibbs sampling for RBM, you gonna find that p(h|v) = sigmoid(Wv +a ) ,p(v|h) = sigmoid(W'h + b). By iternating sampling p(h|v) and p(v|h) indefinitely , you gonna have samples from the target distributions p(v,h), in this case this will be the distribution of the given RBM.  This process resembles ancetrial sampling of infinite stack of sigmoid Belief Networks as an directed graphical model by alternating hidden layers  of the unrolled RBM as follows:   h1 -> W -> v1 ->  W' -> h2 -> W -> v3 -> ....    -> h_{inf} ->W -> v_{inf}  so, samples from  p(v_{inf},h_{inf}) of infinite stack of sigmoid belief network are equivalent to the samples from  p(v,h) of RBM.  If we have the same Gibbs sampling process of two distributions, they will have the same distribution in the end. In a perspective of learning parameters of infinite stack of  directed networks with shared weights, its gradient of likelihood is the same as a RBM because reconstruction probability of a t layer is the same as posterior probability of a t+2 layer, which is mainly due to the fact that hidden layer of infinite stack of directed networks  has RBM as a specific complementary prior, which leads  factorized likelihood p(v|h) and p(h|v) with shared weight.   Deep Boltzmann Machines(DBMs) is a undirected graphical model has a graph structure shaped as a stack of RBMs. However, it encodes a probability measure of a configuration of whole random variables of multiple layers.  If we learn DBMs as an stacks of RBM, we will learn  infinite stack of Belief Networks with a top RBM as a complementary prior, which means that we are learning the Deep Belief Networks instead of DBMs.  So, even though we can initiate parameters by learning each two layers as an RBM, we need to learn the DBM as a whole just as described in the DBM paper.   Happy Deep Learning! Updated 25 Apr • View Upvotes