Why is the stack of RBM's after learning not a deep Boltzmann machine (DBM)?If a bunch of stacked RBMs aren't a DBM, then what is? See Page on toronto.edu section 3.1. Hinton at el. are pretty explicit that a stack of RBM's are not a DBM.
Each RBM layer itself is an undirected graphical model, but stacking them and training them in a particular way can change whether the entire graphical model put together should be directed or undirected. In the case of the deep belief network, where we stack RBMs and train them layerwise by starting at the base layer and move up, this is a directed model because there is no feedback from higher layers to lower layers (well, the top layer RBM is undirected, but that's it). On the other hand, a deep Boltzmann machine is a completely undirected graph, so there is feedback from higher layers to lower layers at every stage of training. This makes a direct sampling method more difficult. If we trained using contrastive divergence for example, we would have to modify it to update the weights of every layer in each Gibbs step. Part of the reason contrastive divergence even works is because it is from a model that isn't very complicated (in terms of the graph topology), and even with extremely simple bipartite graphs like for RBMs its performance deteriorates throughout training. This is probably the reason why you wouldn't use contrastive divergence to train a deep Boltzmann machine directly. There are alternative methods, but the main point is that the sampling becomes more difficult if you include extra structure in the hidden layers. Those methods are more complicated, but in the end it should provide for a better model. 