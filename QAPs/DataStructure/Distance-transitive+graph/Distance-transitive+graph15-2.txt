What are the challenges of automatic text summarization?
There are various approaches and challenges for an Automatic Text Summarization System . Some of them are discussed below : Sentence Centrality and Centroid-based Summarization : Extractive summarization works by choosing a subset of the sentences in the original documents .This process can be viewed as identifying the most central sentences in a cluster that give the necessary and sufficient amount of information related to the main theme of the cluster. Centrality of a sentence is often defined in terms of the centrality of the words that it contains. A common way of assessing word centrality is to look at the centroid of the document cluster in a vector space. The centroid of a cluster is a pseudo-document which consists of words that have `tf×idf` scores above a predefined threshold, where tf is the frequency of a word in the cluster, and idf (inverse document frequency) values are typically computed over a much larger and similar genre data set .In short, the objective is to identify the essential, or central, sentences in a document. One way of modeling a document is as a graph, with each sentence of the document represented with a node and the relationships between those sentences represented with weighted edges. Sentences are modelled as bags of words, and the strength of interaction between two sentences as being the similarity between their respective word-sets . Then the edges are normalized according to the edge weights, using standard metrics like Jaccard similarity and Hamming distance , such that the out-degree of each node sums to one. Page Rank is then computed for each sentence in the document. Higher scoring sentences are more central and more typical of the document. Then the sentences are sorted by their scores and the top n sentences are selected and reordered by their order of appearance in the original document. This approach is used by Flipboard for the summarization. Machine Learning Models : There are various machine learning models for text summarization . Some of them are: Naive Bayes Methods : Let s be a particular sentence, S the set of sentences that make up the summary, and F1, . . . , Fk the features. Assuming independence of the features: Each sentence was given a score according to the equation , and only the n top sentences were extracted. To evaluate the system, a corpus of technical documents with manual abstracts was used in the following way: for each sentence in the manual abstract, the authors manually analyzed its match with the actual document sentences and created a mapping (e.g. exact match with a sentence, matching a join of two sentences, not matchable etc.). The auto-extracts were then evaluated against this mapping. Rich Features and Decision Trees : This technique focusses on the importance of a single feature, sentence position. Just weighing a sentence by its position in text, which the authors term as the “position method”, arises from the idea that texts generally follow a predictable discourse structure, and that the sentences of greater topic centrality tend to occur in certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse structure significantly varies over domains, the position method cannot be defined as naively. Hidden Markov Models : The basic motivation for using a sequential model is to account for local dependencies between sentences. Only three features were used: position of the sentence in the document (built into the state structure of the HMM), number of terms in the sentence, and likeliness of the sentence terms given the document terms. The HMM was structured as follows: it contained 2s + 1 states, alternating between s summary states and s+1 non-summary states . The maximum-likelihood was obtained and  estimated for each transition probability, forming the transition matrix estimate M , whose element (i, j) is the empirical probability of transitioning from state i to j.  Themade a simplifying assumption that that the features are multivariate normal. The output function for each state was thus estimated by using the training data to compute the maximum likelihood estimate of its mean and covariance matrix. Log Linear Methods Neural Networks Deep Natural Language Analysis Methods : These methods use a set of heuristics to create document extracts.Such methods involve considerable amount of linguistic analysis for performing the task of summarization. This method is the basis of Lexical Chain Algorithm . A Lexical Chain is a sequence of related words in a text, spanning short (adjacent words or sentences) or long distances .This method progressed with the following steps: Segmentation of the text Identification of lexical chains Using strong lexical chains to identify the sentences worthy of extraction.For finding out Lexical Chains , 3 generic steps are proposed in the literature : Selecting a set of candidate words. For each candidate word, finding an appropriate chain relying on a relatedness criterion among members of the chains If it is found, inserting the word in the chain and updating it accordingly.A lexical chain is created by taking a new text word and finding a related chain for it according to the “relatedness criteria”. A sample Lexical chain for the below paragraph  : Mr. Kenny is the person that invented an anesthetic machine which uses micro-computers to control the rate at which an anesthetic is pumped into the blood. Such machines are nothing new. But his device uses two micro-computers to achieve  such closer monitoring of the pump feeding the anesthetic into the patient. Looks like : Then the lexical chains are scored. It is scored by 2 parameters : Length: The number of occurrences of members of the chain. Homogeneity Index: 1 – the number of distinct occurrences divided by the length.The Score function: Score(Chain) = Length * Homogeneity Index The strong chains are those which satisfy the “Strength Criterion”: Score(Chain) > Average(Scores) + 2 * StandardDeviation(Scores) Once strong chains have been selected, the next step in the summarization algorithm is to extract full sentences from the original text based on chain distribution. For each strong chain,  a list of “representative words” is chose . A “representative word” is defined as a word that has a frequency in the chain no less then the average word frequency in the chain. For each strong chain, the sentences are chosen that contains the first appearance of a representative chain member in the text. One of the main challenge of automatic text summarization is how to assign importance given to a particular sentence while forming a summary . This is  because importance is naturally subjective; while a culinary novice might be looking for a basic ingredients list from an article about mushroom risotto, a more experienced chef might be looking to learn new techniques from the same article . A significant work can be done on using user-specific data to work in creating personalized summaries . References : Flipboard's Approach to Automatic Summarization , LexRank: Graph-based Lexical Centrality as Salience in Text Summarization , A Survey on Automatic Text Summarization , Automatic News Summarization and Extraction System 