What are the challenges of automatic text summarization?
TL;DR: Creating summaries that somehow resemble human summaries is challenging. But it is doable. Basically, there are two types of summarization techniques. First one, extractive summarization is focused on finding salient (important) sentences from the text and extracting them to create summary. This field is well researched, however there is an empirical limit intrinsic to pure extraction [1], mainly because summaries lack coherence and cohesion because they are merely excerpts from the text. Paper by Das and Martins does a good job in listing contemporary methods in extractive summarization [2]. There is also abstractive summarization techniques that tries to summarize the text in the same fashion as humans do (understand the text, identify the most salient parts of it and generate sentence that will convey the key messages from the text). And the real problem is that scientific community mostly focused on research that deals with extractive summarization, mainly because it is easier to come up with the algorithm that produces extractive summaries rather than abstractive ones that requires tedious text-to-text generation. I have been doing extensive research on summarization techniques and found that it is possible (on paper) to generate abstractive summarization algorithm by combining existing several techniques and merging them into one big algorithm. Basically there are 3 main steps in generation of abstractive summaries: Identification of topic(s) (this could be the main idea of the text and sub-topics) Selecting the most salient sentences from which you will derive the information Generate sentences based on the information you got from the salient parts of the text (this could be sentence reduction, sentence fusion or parsing the sentences and feed the output of parsing into SimpleNLG[3] that will generate grammatically correct sentences) Well, basically you have to understand how humans summarize the text and build your system based on that. However, because individuals are so different in their thinking and interpretation that it is hard to create "gold-standard" summary against which output summaries will be evaluated. I urge you to read the paper that tries to explain how humans summarize texts in order to understand the whole process [4]. Hope it helps! References: [1] Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi- Monod. 2009b. HexTac: the Creation of a Manual Extractive Run. Link: http://www.nist.gov/tac/publicat... [2] Das, Dipanjan. A Survey on Automatic Text Summarization SingleDocument Summarization. Language, 2007. 4: p. 1-31. Link: http://www.cs.cmu.edu/~nasmith/L... [3] Gatt, Albert, and Ehud Reiter. "SimpleNLG: A realisation engine for practical applications." Link: https://hal.archives-ouvertes.fr... [4] Jing, Hongyan, and Kathleen R. McKeown. "The decomposition of human-written summary sentences." Link: http://academiccommons.columbia.... (*note: this link will download paper to your computer, opposed to opening it in your browser) 