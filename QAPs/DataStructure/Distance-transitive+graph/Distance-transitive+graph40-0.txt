What are the most beautiful but still not so known algorithms in computer science?
After going through a lot of websites and Books, I find that some of the more interesting and powerful algorithms in a problem solving setting find little use in the industry. I'm thus going to present algorithms that carry over to engineering, not just research and TopCoder. Another goal was for these algorithms to be useful and widely applicable hammers in their own right, not just as an idea. The benefits of most "algorithms" will be hard to explain them to someone who has not yet been introduced to them, but I tried my best of giving a hint for their role. Naive Bayes. Classification is a natural first step to machine learning and the Naive Bayes classifier combines that with the trending concept of Bayesian inference. Despite its simplicity, can address key problems like spam classification. Map-Reduce. A simple and visual way of leveraging multiple computers to solve a single task. More on its different uses here: MapReduce Patterns, Algorithms, and Use Cases. Hill Climbing. Introduces optimization over a non-exhaustible domain and leads to a whole array of powerful methods like gradient-descent, simulated annealing. Optimization allows us to do decision making, it's important. Binary search. Introduces logarithmic complexity and how the structure of data can change complexity characteristics of operations. Exponentiation (powers) by halving. Great example of a recursive attack. You would be surprised how many problems can be described as exponentiation (for an associative operation) of something: a number, a matrix and more. Again it's magical that associativity of an operation allows you to turn a linear factor into a logarithmic one. Floyd-Warshall. Graphs as matrices, shortest paths, negative cycles, dynamic programming in five lines of code. Inductive inference. Inductive inference. This idea on how to find programs that do certain things without having a human write them I believe is the key to the future of AI and simulating and understanding the human mind. Markov Chain Monte Carlo. Approximate integral, expected value computation based on a conditional specification of the probability distribution. This gives us a general way of doing data analysis. Shor's algorithm. Non-trvial example of a quantum algorithm that shows a significant complexity improvement. Reveals how quantum computing works. PageRank. How to iteratively solve for tightly related variables to extract metrics on graphs. Along with them, Let me mention some very less known Algo: 1.) An ingenious algorithm to me is Bill Gosper's HASHLIFE. When you think you have your optimized, bitboard simulation of Conway's Game of Life, Gosper shows up with exponential speedup in running the algorithm, so in the blink of an eye, you can witness the Game after billions or trillions of generations. He uses quad-trees and caching in a remarkably novel way. Probably even more important, also due to Gosper, is his procedure forindefinite hypergeometric summation. This kind of problem was formerly difficult and ad hoc—akin to doing integrals as a college freshman. But he made an infallible algorithm that will return the closed-form sum if it exists, or return with certainty that it doesn't. It was a major step in the field of computer algebra, and is the reason why programs like Mathematica,Wolfram|Alpha, Maxima (the algorithm was first implemented in Macsyma!), and the rest can do your sums. It was also the basis for Doron Zeilberger's algorithm for definite summation. 2.) I would also like to put in a plug for Breiman's Random Forest (RF) algorithm, because I think it illustrates (a) how machine learning has evolved over the past decades and (b) why computer science is a fun discipline. RF is really an expansion of an older algorithm, decision tree learning.BORING DIGRESSION ON DECISION TREES FOLLOWS: A decision tree learning algorithm takes a bunch of observations with predictive variables (X) and a variable to be predicted (Y). It generates a tree of if-then rules to model the relationship between X and Y by splitting the data over and over. Typically, you build this by starting with all of the data in one group at the root, then splitting based on the variable that has the strongest impact on Y, then splitting those two halves best on the variable that has the strongest impact on Y within each group, and so on until the tree gets "big enough." There are many subtly-different algorithms to decide when the tree is big enough, to decide the exact split, to "prune" trees so they don't get too complicated, and to try and optimize them for different real-world cases. None of these is rocket science, but there are lots of picky little distinctions to worry about and parameters to choose. People wrote plenty of papers on these topics, and decision trees were a hot topic in data mining from like 1984 until at least the late 90s. In practice, they run into all kinds of issues, and their accuracy is pretty bad compared to modern machine learning algorithms. END OF DIGRESSION. Ok, so, if decision trees are kind of picky and complicated, and random forests are built on top of decision trees, they must be complicated, right? Not at all. Here is the RF algorithm: (1) Choose a random sample from your learning data X and Y (2) Build a decision tree on top of this data, but whenever you want to learn a new node in the tree, you are only allowed to look at a randomly-chosen subset of the variables. So, maybe we randomly select that when splitting node A, we can look at the "height" and "gender" variables but not the "weight" variable.      (2a) Ignore all of the fancy tweaks to the decision tree algorithm and just build the biggest tree that the data will allow (3) Return to step 1, using a new random sample and building a new tree. Do this N times (maybe a couple of hundred times), and save all N trees. Now, when you want to predict the Y for a new observation, you ask each one of the trees to predict the Y, and you average their results together. That's it. No fancy rules and tweak. Oh, and now you'll get excellent results instead of the crappy results from each tree by itself. Wait, that sounds like you gave this problem to a frustrated CS sophomore who just said "Ah, let's sprinkle some calls to rand() here and a couple more rand()s there and do it over and over." There are no fancy statistical tricks or optimization algorithms, but it all works great. Why is this a cool algorithm? Well, expanding on my original two reasons, RF: (a) Uses lots of compute power combined with a simple, but clever algorithm to give great results. This application of compute power to statistical problems has been the crucial factor in the growth of machine learning over the past decade, much more so than the much-hyped growth in data volume. Brute force allows us to get good results from un-tweaked, off-the-shelf machine learning code for a huge variety of problems in a matter of minutes, whereas a past generation of would-be "data scientists" faced a choice between inaccurate-but-fast algorithms and writing heavily customized code to eke out more accuracy at the cost of enormous development time. (b) Is incredibly interesting in practice and not that interesting in theory. Part of the charm of computer science is that methods like that can still thrive in academic literature and jump into common usage almost immediately. Even more amazing is that RF is a significant improvement in one of the most widespread problems in the world (how to do predict Y from X?), and it was only discovered in the early 2000s despite requiring no complex math. Anybody with basic training in statistics could have made the leap from decision trees to random forests, but for 20 years, no one did. Who knows what simple algorithm will come along next to have a huge impact on statistical practice? 3.) And not to forget one of my favorite I think the Floyd–Warshall algorithm for shortest paths between all pairs of vertices in a graph is very neat for two reasons. The first reason is its elegance. It's a classic example of dynamic programming. You can check it out on Wikipedia, but the gist is that you find the shortest path between all pairs up to vertex number k, and slowly increase k while routing through previously discovered paths. The sheer elegance results in an implementation in maybe ten lines of standard imperative code. The second reason is its relevance. The algorithm can be extended to deterministic finite automata in order to prove that all regular languages are accepted by DFAs, and all DFAs accept regular languages. This is done essentially by labeling states sequentially and routing through them incrementally using regular operations. Additionally, re: relevance, the algorithm can be used for matrix inversion, transitive closures, and a host of other useful applications. It's a very neat idea that ends up being extremely powerful in theory and practice. Other than these Paxos (algorithm) Byzantine Generals Delaunay triangulation Voronoi diagram Gaussian Elimination Hindley–Milner Type Inference Dinic’s Algorithm Linguistica (http://acl.ldc.upenn.edu/J/J01/J...) Nevill-Manning algorithm (http://en.wikipedia.org/wiki/Seq...) Berlekamp–Massey algorithm Advanced Encryption Standard (AES) SHA2 SHA3 finalist Skein, BLAKE, Grøstl, JH, Keccak (Keccak team, Daemen et al.) Needleman–Wunsch algorithm Kalman Filter Mean l2 metric Mean Hellinger distance Latent Dirichlet Allocation (LDA) Levenshtein distance Cosine Similarity Smith–Waterman algorithm Pointwise mutual information Jaro–Winkler distance Kullback–Leibler divergence TFIDF 