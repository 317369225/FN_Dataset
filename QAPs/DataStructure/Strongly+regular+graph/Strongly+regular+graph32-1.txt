Why does deep learning require the construction of generative data models?
I agree with Joseph's answer. I just want to add a remark: unsupervised (trained without labeled data) != generative (can sample from trained model) For instance (stacked denoising) autoencoders are a way to train an unsupervised feature extractor but they do not provide any natural way to sample an estimated probability distribution of the input space: in fact they do not even estimate explicitly a probability density function for the input data. For autoencoder, you could hackÂ  something by arbitrarily sampling a pseudo input X from some distribution (pdf(X) = uniform, normal, ...) and then consider X' = decode(encode(X)) a random variable that follows the distribution estimated by your autoencoder. However the training algorithm of the autoencoder does not even try to minimize the KL divergence of this distribution w.r.t. the empirical distribution. The objective function of the autoencoder is just to minimize the quadratic euclidean distance of the input X and it's reconstructed version X'. This is even more the case with the convolutional triangle-kmeans feature extractor described in: An Analysis of Single-Layer Networks in Unsupervised Feature Learning by Adam Coates, Honglak Lee and Andrew Ng. In NIPS*2010 Workshop on Deep Learning and Unsupervised Feature Learning. http://ai.stanford.edu/%7Eang/pa... In this case there is no canonical way to "decode" extracted features to be able to apply the decode(encode(Normal_X) trick. 