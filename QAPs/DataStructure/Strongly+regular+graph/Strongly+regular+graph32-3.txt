Why does deep learning require the construction of generative data models?
I want to extend the answers by connecting some aspects of deep learning to common themes and goals in machine-learning. (1) The question considers the unsupervised learning as a burden, but effective combination of labeled and unlabeled data (Semi-supervised Learning) has been desired in many domains of machine learning. (2) Generalization power of ML solutions increases as the number of training samples increases relative to the dimensionality of available solution space (VC dimension). Using the unlabeled samples greatly increases theÂ  number of samples for most layers of the model. (3) Another way to view it: Limiting the lower layers, such that they act as good generators (or auto-encoders), limits the solution space to a subspace which learns good features, and more importantly has a much lower dimensionality. (4) Some similarity to visual information processing in human/animals. 