What algorithms do data scientists actually use at work?
This stuff depends very heavily on the problems at hand. 1. If you are doing classification/ranking with high dimensional data like text, a linear classifier will likely be your best bet to avoid the curse of dimensionality and modern SVMs will with extremely high likelihood outperform Naive Bayes, Logistic Regression and pretty much anything else (both Naive Bayes and LR are linear classifiers as well, whose job is to finally learn a hyperplane that separates the two classes and in case of n >2 classes, learn n hyperplanes). In the past, Platt's SMO Algorithm and variants were roughly quadratic in the number of train instances leading SVM training to not be very scalable. That is no longer a problem with modern SVM train algorithms that train in linear time and also have linear convergence properties (eg: each iteration causes the "error/residue" to become 1/2 of the previous iteration, leading to very rapid convergence). 2. Linear Regression: This is likely to be applicable to a wide variety of problems and is indeed the first thing you might want to think of when you are looking to predict a score and not a category. The loss function here is the square loss so you might want to consider whether that matches your true loss. If not it would be better to adapt the loss function as I describe in (3) below. 3. Optimizing other regularized loss functions which are as close as possible to your true loss function of interest is a good idea. Very often your true loss function is not convex and you would attempt to approximate it with the closest possible loss function that is convex (eg: in the SVM example above, the 0-1 loss is approximated with a hinge loss). In the popular Netflix challenge the loss function to be optimized was very close to the squared loss, except that the loss was to be truncated with the function f(y) = 1 if y<1, f(y) = 5 if y>5 and f(y) = y otherwise, since it was known that acceptable ratings had to fall in the range of 1 to 5. Understanding loss functions here, would let you tweak the loss function to the best possible convex approximation and tweak the optimization code to deal with the new function. Having a convex loss function to optimize is good because that means that there exist gradient descent like methods which would guarantee reaching the global optimum and there are usually efficient methods to do that. The other alternative is to put up with a non-convex loss function where there is the risk of getting stuck in a local optimum and instead find ways to minimize the effect of getting stuck in a horrible local optimum. 4. Conditional Random Fields, Markov Random Fields and Maximum Entropy Markov Models: These are extremely helpful when dealing with graphical models (models where you have to predict multiple unknowns and there exists some special structure of dependencies between the unknowns). There exist exact inference methods only on trees/forests (graphs with no cycles). Such graphs permit us to efficiently attain global optima provided the loss function is convex. With graphs that involve cyclic dependencies, one needs to resort to some form of approximate inference and you would need to find a method that is fairly efficient and it's best to use domain knowledge of the problem you are solving here, than use an off the shelf approximate inference method.  The simplest applications of graphical models are the ones involving sequential dependencies (a sequence is a special case of a tree) - some problems are building part of speech taggers, sentence parsers, Named Entity Detection and many problems with biological/genomic data. I have also benefited from these in my work on Question Classification. For a lot of these problems Conditional Random Fields (CRFs) usually do better than other models I mentioned but also take longer to train. While I have never used SVMStruct on graphical models myself, based on experience with discriminative models in general and the fact that when the graphical relations are removed from consideration SVM >= Logistic Regression (SVMStruct with no graph is SVM, CRF with no graph is logistic regression), I think SVMStruct might very well match or do better than the CRF. 5. Problems with enough train examples and not too many features (say 50 to 1000 attributes, millions of train instances).     Here all bets are off - neural networks, decision trees, bagging and many other things capable of learning non-linear models might be worth trying, since it is possible to learn enough complex dependencies in this data. There are equivalent complex models such as Regression trees, Multivariate Adaptive Regression Trees, Multivariate Adaptive Regression Splines that be learned in a similar scenario if the aim is to predict a score rather than a category. Note that all these models typically will involve a lot of tweaking and a lot of thinking on your part about bias (ability of the model to learn your pattern) vs variance (your model having endless degrees of freedom and consequently have the ability to overfit in a manner that doesn't generalize well to unseen data). Thus, this scenario is typically going to be far more messy than any of the other scenarios, where you can be reasonably confident what the best algorithm is and work within the framework of that to optimize its capability and optimize your feature set. 6. Feature Selection Algorithms (Recursive feature addition/elimination, chi squared feature selection etc.): Often doesn't give accuracy benefits in discriminative models, but helps in model size reduction which may be important to the application at hand. Gives accuracy benefits in many generative models and models that don't work well with an enormous set of features. 7. Many custom models and algorithms built from first principles to solve the problem at hand. 8. Clustering algorithms like k-means and others: routinely used in applications like google news to cluster similar stories etc. 9. Language Modeling, smoothing etc. : Very useful in machine translation, speech to text etc. since these help you predict the probability of a particular sentence being spoken. Humans use this all the time. We routinely fail to hear specific words in a sentence but yet piece the sentence together based on our knowledge of other words. Language models provide this exact critical information, since sound to text signals will necessarily be noisy and it is important to understand what a certain word is likely to be both based on the audio signal as well as context. Applications like Siri most definitely use this. As do applications like Google translate, since the ordering of words is quite different in different languages. 10. 1-9 above could all be strictly considered machine learning. However one category of important and widely used big data algorithms is A/B testing and data analytics in general. In all machine learning algorithms, you do much better by understanding the domain of the data well and also understanding the underlying models, their strengths and weaknesses etc. It is much the same with A/B testing. You most certainly need a strong understanding of probability and statistics, and consequently the ability to compute statistical significance for your tests. You need to understand the underlying assumptions behind statistical significance and evaluate how valid those assumptions are in your case and to the extent they are not valid, what corrections you can make etc. A/B testing removes some of the issues with confounding variables and allows separation of correlation and causation. A lot of other data analytics would involve looking at data where causation is hard to find - here you would need to understand a good deal about the underlying data, have some sense of what is likely, try to test competing hypothesis separately etc. I would say that 95% of the stuff done in the industry with big data is A/B testing and analytics, involving no machine learning. Updated 154w ago • View Upvotes