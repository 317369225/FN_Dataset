When will be the time when software engineers should know about quantum programming?
You don't need to know anything about quantum computing to program the D-Wave machine. It's pretty straightforward, and I'll explain it briefly. You reformulate your optimization problem in quadratic unconstrained binary form. It looks like, [math]E(\mathbf{z}) = \sum\limits_{i,j}^N z_i Q_{ij} z_j[/math] [math]z_i \in \{0,1\}[/math] (they are binary variables), [math]N[/math] is the number of variables, [math]Q[/math] is a symmetric matrix where the diagonals act like variable weights. You can think about this in terms of a graph with weighted edges given by off-diagonal terms of [math]Q[/math]. The optimization problem is then to minimize [math]E(\mathbf{z})[/math]. So it's just your standard quadratic programming problem, except that there are no constraints allowed (not difficult to handle usually) and variables must be binary (you could make them real and just do a binary expansion though, but this would require a larger machine with less noise). Then you embed that graph into the hardware graph of the machine itself, which is comprised of connected "unit cells", where each unit cell consists of 8 qubits connected in a bipartite fashion. If you have a fully connected problem graph, your embedding will require [math](k-1)^2/2[/math] qubits (vertices in the hardware graph), where [math]k[/math] is the number of vertices in the problem graph. Here's a graphic: Embedding the problem into the hardware optimally is an NP-hard problem, but this is not something programmers will need to worry about. You can think of the problem embedding part being wrapped in a sort of QC compiler provided by D-Wave (and maybe other vendors who think they have better graph theorists). There are various ways to do this. If you're really good, like those guys on StackOverflow good, you might think really hard about how to restructure your problem so that you have a more natural mapping into the hardware graph (e.g. sparse connectivity). There's also the amount of time you want to run this optimizer (called the annealing schedule) for (which will affect the accuracy of your results), but it might eventually be just as simple as running many instances to figure out a sweet spot which could be automated (the calculations are not long, currently they have a max running time in the milliseconds). But you can see that none of this required me saying anything further than just mentioning qubits. All of those details will be what hardware people and "compiler designers" will worry about, though it's likely that "quantum programmers" will be very serious about performance so they might think harder about the mapping than your average Python coder might think about hacking the assembly code. If you're actually worried, just study your graph theory (and in particular, reducing problems and algorithms to more fundamental problems), optimization, and particularly quadratic programming. There will likely be IDEs to help people program these machines more easily. In fact, that was my focus when I first started doing research on adiabatic quantum computation. As far as I know, it is the first IDE for AQC. It is meant to help design new hardware graphs, test problems, design and test new annealing schedules, and eventually it will be able to model noise and underlying physics of devices. It is meant to be helpful to both algorithms developers and the engineers working on building AQC devices. Here is the preprint: [1309.3575] An Integrated Development Environment for Adiabatic Quantum Programming Updated 120w ago â€¢ View Upvotes