What is the fundamental difference between a classical bit and a quantum bit?
Classical bit is a variable that holds value from {0, 1}. Qubit is a variable holding superposition of clear states (clear states [math] |0>, |1>[/math] correspond to 0,1 respectively) qb =  [math]a |0> + b |1>[/math] for complex a,b such that [math]|a|^{2}[/math]  +  [math]|b|^{2} = 1[/math] (If you don't get |x> notation, go look up Dirac/bra-ket notation). But what does this superposition mean? It means that if you measure qb in standard basis ([math]{|0>, |1>}[/math]) you will get [math]|0>[/math], or  [math]|1>[/math] with probability [math]|a|^2[/math] and  [math]|b|^{2}[/math] respectively. So, in a way, qubit holds much more information (in a classical sense) than classical bit - it can be in any of states that correspond to two complex numbers which squared magnitudes sum to one. The problem is, there is no simple way of extracting that information, but why is that so probably belongs to another question (this is what quantum computing is about). Updated 20w ago