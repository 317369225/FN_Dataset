If just Earth, with no space around it, was a computer simulation, how many of the fastest CPU/GPUs available today would it take to simulate everything in the world simultaneously?
Starting with this calculation: http://education.jlab.org/qa/mat... Iron has an atomic number of 26 and an atomic mass of about 56. Oxygen has an atomic number of 8 and an atomic mass of about 16. We can't use the abundance of the elements in the universe, because the early solar system separated the elements out. That's why the above table has iron being so common - almost as common as oxygen - despite the fact that iron is much harder to synthesize. You'll find abundance charts show atomic numbers that are powers of two being much more common than anything either side, because hydrogen is the most abundant element and combining two common things is going to be more frequent than combining two uncommon things. This matters because in order to simulate something, you have to know how many somethings to simulate. Let's simplify and say that, on average, we need to simulate the average of iron and oxygen. In other words, we need to simulate (56+16)/2 objects in the average nucleus (36). Technically, we should have spare capacity for all electrons that can be emitted by the nucleus on absorbing a gamma ray of the correct frequency, but I'll cheat and ignore that. The average atomic number is (26+8)/2, which is 17. I'll say this is the average number of electrons per atom we need to simulate. Thus, the total number of objects per atom is 53. That's going to bring the total number of objects you need to simulate for Earth itself to around 6 x 10^51. That's still not enough. You have to model all inbound cosmic rays, space dust/meteors, extraterrestrial fields (such as gravity), etc. There's no good way to estimate the total amount you have to simulate, simply because (as shown by James Gleik) the gravitational field of an electron near Alpha Centauri can affect a game of pool on Earth due to cascading effects and sensitivity to initial conditions. At some point, you have to approximate and there will be a realistic timeframe in which that approximation is inadequate. You have to simulate the universe to get a perfect answer. I'm going to say that, for the purposes of simulation, doubling the number of entities will be sufficient to get a good enough answer. We need to simulate the moon, anyway. That brings us to 2 x 10^52. Now, we need to know the time intervals to use. Because this is a necessarily crude simulation, I don't think time intervals shorter than 10^-12 will be necessary. This means we need to handle 2 x 10^64 sets of calculations per second. Assuming Maxwell's equations can be extended to all forces (doubling the number of equations as you double the forces combined), you'd need 16 partial differential equations per interaction. Within the atom, everything interacts with everything. So that's 53 x 52 x 16 internal interactions you need to calculate for. About 44,000. You also have about 10^51 external interactions per atom to contend with, giving you 10^101 calculations there. These are additive, so we can ignore the internal value. Multiplying by the calculations per second, you end up with 10^165. There are no CPUs or GPUs capable of a FLOPS rating high enough to make a substantial dent in this. Remember, you need FLOPS, not MIPS. The top-rated supercomputers are in the tens of petaflops, which would be enough to subtract 13 or 14 off the exponent. These have thousands of processors in them, if not tens of thousands. Let's keep it simple and call it a thousand. So that would imply each processor can bump 10 off the exponent. This means you need 10^155 processors to do the actual work. However, the network fabric needs to be fast to keep this in sync. You probably need another 10^155 processors dedicated to packet forwarding in order to provide the hard real time guarantees implied by the question. This is assuming a top-down approach. You could also go with a bottom-up approach, using Planck Units. http://en.m.wikipedia.org/wiki/P... One unit of Planck time is about 5 x 10^-44 seconds. No event can occur in less time. None. All things happen in a natural number of Planck time units of time. One unit of Planck length is about 2 x 10^-35 metres. Nothing can be smaller than this, all things occupy a natural number of Planck length units of space. At this level, there is no distinction between mass and energy. You can also ignore all the fundamental particles, they carry things from one grid position to another but we can do that with simple cellular automata rules. The particles become the algorithm, our 4D grid only needs to concern itself with energy and since light travels one Planck length per Planck time, no interaction can ever involve a non-adjacent hypercube of spacetime. The numbers are still huge, far exceeding a multi-particle model of Earth, but because there is a very small, constant number of interactions, it scales. The entire system is embarrassingly parallel. You have a read buffer and a write buffer, which you switch over at the end of each cycle. There are no locks, synchronization is guaranteed, it's very clean. So whilst particle modelling suffers from combinatorial explosion, this doesn't. There is therefore a size in spacetime at which a cellular automata simulation will require less processing. Since there are more grid hypercubes than particles (unless you're working with quantum foam, then it is always 1:1), you consume vastly more memory, but no processor-to-processor communication takes place, no complex network fabric is needed, a processor can be tightly coupled with a fixed region of memory, you could even eliminate distinct processors and use a processor in memory architecture. Using existing technology alone, you could simulate around 1/(10^30)th of the universe in 1/(10^34)th time with this approach, but it would require turning the entire universe into a computer for running the simulation. 