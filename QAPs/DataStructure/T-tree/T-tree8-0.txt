How can bagging avoid overfitting in Random Forest classification?
Random Forest involves following broad parameters: Number of trees, t Size of Sample, n (Total - N) Number of variable to sample at each node, m (Total - M) So, bagging is done at 2 levels. 1. Data Selection 2. Variable Selection There is no pruning involved. So a single tree highly over fits the data. Low bias and High Variance. The construction of 't' trees ensures that each tree uses a different set of data and a different set of variable. Each individual tree will Low bias and High Variance. (Look at the figure), Taking the average over all trees (ensembling),  each fitted to a subset of the original data set, we arrive at one  bagged predictor (red line). Clearly, the mean is more stable and there  is less over fit. In the original paper on random forests, it was shown that the forest error rate depends on two things:       The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate. The strength of each individual tree in the forest.  A tree with a low error rate is a strong classifier. Increasing the  strength of the individual trees decreases the forest error rate.       Reducing m reduces both the correlation and the strength.  Increasing it increases both. Increasing n also increases the correlation among different trees. High n means the similarity index of data among all tree is very high. The strength of ensemble lies in that all trees are very different, which means low m and low n. (Short answer to your question: Bagging ensures that all trees are different). However, keeping them very low would mean that every tree is missing on some crucial information. This would increase the error rate. There is no thumb of rule to find these parameters. Hit and Trial. References: https://www.stat.berkeley.edu/~b... http://en.wikipedia.org/wiki/Ran... 