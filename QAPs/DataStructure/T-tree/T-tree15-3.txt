Random Forests: How do random forests work in layman's terms?
If you are familiar with decision trees, then Random Forest is a fairly easy extension (at least conceptually).   Decision trees identify the features that are most discerning when it comes to identifying classes.   For example, suppose you are trying to classify individuals as a 'good' credit risk or a 'bad' credit risk.  Suppose further that 10% of individuals in general are 'bad' and 90% are 'good'.  Then, any (large enough) random sample will have both 'good' and 'bad' individuals in these proportions.   Using decision trees you may be able to identify segments of individuals with a higher concentration of a particular class.  We'll try to identify 'bad' individuals in this example.  Suppose that you had additional information about each individual ...  say Income.   A decision tree algorithm may identify that the segment of individuals with less than 50,0000 of income is composed of 30% 'bad' and 70% 'good'.   That is, people in this segment are 3X more  likely to be 'bad' vs. the general population.   Now, suppose that you also have information about each individual's age.  The Decision Tree algorithm might also identify that, within the "less than 50,000 of income" group, that the segment of individuals under the age of 30 is composed of 50% 'bad' and 50% 'good'.   This more-refined segment (income less than 50,000 AND age less than 30) is 5x more likely to be 'bad' when compared to the general population.    And so on  ... decision trees may identify many, many such features that are predictive of whether an individual will be a 'good' credit risk or 'bad'.   The decision tree algorithm can determine both the order of the branching as well as the bins on which to break (income > 50k, age < 30, ...etc).  But generally, tuning a decision tree requires a trained analyst to make sure the breaks are meaningful.  Also the analyst needs to help "prune" the tree to avoid over-fitting and/or nodes with inappropriate coverage. Random Forest uses decision trees, but takes a different approach.   Rather than growing a *single*, very deep tree that is carefully overseen by an analyst, Random Forest relies on aggregating the output from many "shallow" trees (sometimes called "stumps") that are tuned and pruned without much (or any) analyst oversight.   Some of these trees may have been grown from samples that said age was the more important feature (as opposed to income).  Other trees may find that a more relevant bin of income is 100,000 (as opposed to 50,000).  Other trees may find completely different features to be relevant.   The idea is that the errors from many "shallow" trees will wash out when aggregated and lead to a more accurate prediction.  The papers referenced by some of the others above are excellent resources to better understand this. 