Random Forests: How do random forests work in layman's terms?
What I have observed in practice is that Random Decision Forest is very effective in eliminating noise in the model input data.  Given a long list of input variables and a potentially sparse dataset, it is very likely that any predictive model will discover spurious relationships between those inputs and the chosen target variable.  This results in overfitting (Overfitting ) and the model does not generalize well enough to future input it has not seen. Because Random Forest builds many trees using a subset of the available input variables and their values, it inherently contains some underlying decision trees that omit the noise generating variable/feature(s).  In the end, when it is time to generate a prediction a vote among all the underlying trees takes place and the majority prediction value wins.  If for example ice cream sales in NYC was randomly correlated to the stock market index, it is very likely that a good portion of the ensemble of trees in your Random Forest will not even take into account ice cream sales (variable selection happens randomly thus the name) to begin with so they will be able to sidestep that spurious connection and the resulting prediction will do so too. Of course this answer simplifies the concept since there are a number of configuration variables to tune the model and different but related approaches in constructing Random Forests, but I think it gets the main argument through. If you wanted to check out a good and easy to use Random Forest implementation, you can do so by signing up for BigML (BigML is Machine Learning for everyone), which is where I work.  There is a free version that should give you a pretty good idea as to how it works. 