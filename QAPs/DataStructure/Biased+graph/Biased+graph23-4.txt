Why do overfit models have high variance but low bias?
Low bias in supervised learning means that the underlying assumption (parameterized function that is used for modeling) is more likely to overfit without proper regularization. In simpler terms, referring to bias-variance tradeoff, high bias = simpler functions, low bias = complex functions. Complex functions, such as higher-degree regression polynomials, can overfit very easily. I’ll try to give the examples for bias-variance tradeoff that make more sense. Low variance, low bias = high degree polynomial with small coefficients. Its values don’t fluctuate much and the parameters are flexible e... (more)