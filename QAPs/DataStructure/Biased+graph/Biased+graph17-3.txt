What are the concise meaning and interpretation of bias and variance in machine learning and statistics?
Say you want to estimate or predict some fixed-but-unknown value θ  θ using a random measurement T  T . A commonly used performance metric for T  T is the mean (aka expected aka average) squared error: MSE=E(T−θ) 2   MSE=E(T−θ)2 If we write μ=E(T)  μ=E(T) , then we can rewrite the above as: E(T−μ+μ−θ) 2   E(T−μ+μ−θ)2 =E(T−μ) 2 +2E[(T−μ)(μ−θ)]+(μ−θ) 2   =E(T−μ)2+2E[(T−μ)(μ−θ)]+(μ−θ)2 There is no E()  E() on the last term because it's not random, it's just a number. The middle term is 0, so that gives us the identity: MSE=E(T−μ) 2 +(μ−θ) 2   MSE=E(T−μ)2+(μ−θ)2 which decomposes the mean squared error into variance (the first term) and squared bias (the second term).  The existence of this additive decomposition is one of the nice things about using mean squared error.... (more)