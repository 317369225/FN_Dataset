What are the different dimensionality reduction methods in statistics?
There are so many ... first, let's break it down to "feature selection" and "feature extraction" Some examples of feature selection:- L1 regularization (e.g., Logistic regression) and sparsity - variance thresholds - recursive feature elimination based on the weights of linear models - random forests / extra trees and feature importances (calculated as average information gain) - sequential forward/backward selection (an article about those that I've written some time ago: Sequential Feature Selection Algorithms in Python) - genetic algorithms - exhaustive search Some examples of feature extraction: - Principal Component Analysis (PCA), unsupervised, returns axes of maximal variance given the constraint that those axes are orthogonal to each other (also, an article I've written some time ago: PCA in 3 Steps) - Linear Discriminant Analysis (LDA; not to be confused with Latent Dirichlett Allocation), supervised, returns axes that maximizes class separability (same constraint that axes are also orthogonal); and another article: Linear Discriminant Analysis bit by bit - kernel PCA: uses kernel trick to tranform non-linear data to a feature space were samples may be linearly separable (in contrast, LDA and PCA are linear transformation techniques (the last one of my articles that I will link here ;) Kernel PCA) - supervised PCA - and many more non-linear transformation techniques, which you can find nicely summarized here: Nonlinear dimensionality reduction Edit: So, which technique should we use? This also follows the "No Lunch Theorem" principle in some sense: there is no method that is always superior; it depends on your dataset. Intuitively, LDA would make more sense than PCA if you have a linear classification task but empirical studies showed that it is not always the case. Although kernel PCA can separate concentric circles, it fails to unfold the Swiss roll for example: here locally linear embedding (LLE) would be more appropriate.  In a nutshell, you have to try it out on your actual dataset and evaluate and compare the results. Personally, I rarly use dim reduction techniques since computational efficiency is typically not an issue for me. I don't need to make predictions in "real time" and the datasets I am working with are also typically moderately sized. If not, I have access to a supercomputer where I can run those things with "unlimited" ram and number of cores. To tackle "the curse of dimensionality," I prefer regularized models or ensemble techniques Updated 24w ago • View Upvotes • Asked to answer by Chunhui Zhang