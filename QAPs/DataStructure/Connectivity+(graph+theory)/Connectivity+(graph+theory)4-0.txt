What do eigenvectors represent in real life?
Principal component analysis (PCA) is a great application to illustrate the meaning of eigenvectors and eigenvalues, especially the eigenvector corresponding to the largest eigenvalue (this eigenvector is called the principal component). Here are three planar pointsets (black dots), and the eigenvectors of their covariance matrices. ​ ​ ​ The eigenvector corresponding to the largest eigenvalue gives the direction that minimizes the mean-squared displacement (reconstruction error) over the datapoints. In other words, if you had to approximate the pointset by one straight line, the principal component gives the best approximation. This is best seen in the rightmost example, where the pointset is long and skinny. The distances from the points to the direction of the longer red vector are small, whereas the distances to the direction of the shorter vector are large. In the example on the left, the pointset is essentially round, and the two red vectors have the same length. So, it does not matter which one you choose, and you could even take their (norm-one) linear combinations. The middle example is ... somewhere in between the other two :) Things get more complicated with multidimensional data. Once you have the best straight-line approximation (the first principal component), you can look into approximating its errors, i.e., finding a two-vector approximation. In other words, subtract the one-vector approximation from individual points, and repeat the whole thing. This will give you the second principal component, and you can iterate this process. Needless to say, the last vector will be least useful if considered by itself. In other examples, the eigenvalues are taken of other types of matrices. For example, the Perron–Frobenius eigenvector (the one with the largest eigenvalue) of the K  -by-K  Pairwise comparison matrix is a great way to resolve contradictory preferences - if N  people express their preferences for K  sports teams, but each person only compares a small set of pairs. Here, the elements of the eigenvector corresponding to the top eigenvalue help you to rank the K  sports teams. According to Spectral graph theory, eigenvalues and eigenvectors of the Laplacian matrix of a graph reflect various connectivity information. For example, the graph is connected if and only if the smallest eigenvalue (zero) has multiplicity 1 (this is easy to see, as the matrix has block structure for a disconnected graph). For a connected graph, the smallest nonzero eigenvalue is related to the minimum cut of the graph, and the respective eigenvector partitions vertices to achieve this cut. The spectrum of the graph Laplacian, especially eigenvalue multiplicities, is related to graph symmetries. Updated 31 Oct • View Upvotes