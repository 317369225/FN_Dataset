Probabilistic graphical models: what are the relationships between sum-product algorithm, belief propagation and junction tree algorithm?
The asked question is about the Inference problem in probabilistic graphical models (PGM). The inference problem is to ask the query based on the representation of the model according to a specified graph. This task is NP-hard in general and can be classified in two solution categories, Exact solution Approximate solution In exact solution, we’d like to answer the following main probabilistic query in an exact way, P(X=x|E=e) The exact approaches for answering the query are, Variable elimination algorithm (or called Graph elimination Algorithm) Sum-product message passing algorithm (or called Belief propagation Algorithm) Junction tree algorithm In variable elimination, we want to compute the marginal for a node of interest in directed graphical models (DGM) or Undirected graphical models (UGM) based on dynamic programming ideas. For example in the following figure, The aim is to compute the query, P(X_1=x_1|X_6=x_6)  . The variable elimination tries to exploit the conditional independencies in the graph to reduce the computational complexity, such as in this example, we should compute the joint probability via the following , Where            In fact we can repeat this process for all inner sums via the same sum-product operator in above formula. The variable elimination algorithm depends on the order of elimination of the variables, e.g. in the above, the order is [math]x_5, x_4, x_3, x_2[/math]. If we choose the order with induced marginal distribution with fewer parameter the total computational complexity is more less than the original one’s. The other point about variable elimination is that you just notice the wanted query and do not save the sum-product operations (called meassages). The idea of sum-product message passing (belief propagation) is that to save the all of the message information such that we can apply them for every wanted query and we don’t waste our computational resources again. For intuition see the following figure, Briefly speaking, the message passing algorithm only works for tree (poly tree) structures. Indeed the message passing for tree structures yields to a unique solution for every possible path of message spreading in the graphs but for non-tree structures there is no unique solution for inference problem. How we can use the message passing for general PGM’s? The junction tree algorithm tries to solve the problem of message passing algorithm by creating the chordal graphs for a general non-tee PGM and then apply message passing algorithm for inference task. The problem for junction tree algorithm is the computation of Chordal graphs that in general is NP-hard problem! Nowadays the loopy message passing algorithm (Big Picture: the message passing algorithm on the non-tree using one of the paths (approximate solution)) is used rather than difficult junction-tree or similar algorithms. 