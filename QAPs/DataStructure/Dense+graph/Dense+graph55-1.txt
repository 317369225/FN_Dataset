What's the best parallelized sparse SVD code publicly available?
Mahout's SVD (originally from decomposer) implementation runs on Hadoop, and scales linearly with the number of rows of the input matrix (double the number of Hadoop nodes, handle a matrix with double the number of rows, for a fixed rank result you want), and has no memory limitations regarding this row size (if you don't have a big cluster, it'll just take longer to run). As Yura mentions, there is currently an implementation artifact (ie developer laziness) which leads to a restriction on the product of the number of columns * desired rank < RAM available (divided by 8), because the Lanczos basis vectors are kept in memory to be turned into eigenvectors at the end of the day. For text corpora (vocabulary size < 1MM or so, say), this means that on commodity hardware you can still get 300-400 singular vectors just fine, but for doing big big graphs (wikipedia, social network graphs, web), this doesn't cut it. It's not hard to modify the Mahout implementation to store the (dense) basis vectors to disk, and then do eigenvector creation (the last step of Lanczos) piece by piece, with only a couple vectors in memory at a time.  There is an open JIRA ticket for this (https://issues.apache.org/jira/b...), and if someone else doesn't get around to submitting a patch, I'll spend a few hours and do it Real Soon Now. Updated 269w ago • View Upvotes