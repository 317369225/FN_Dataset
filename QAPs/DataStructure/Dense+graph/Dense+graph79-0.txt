Is there a simple explanation of the Louvain Method of community detection?
TL;DR/Short version: Communities are groups of nodes within a network that are more densely connected to one another than to other nodes. Modularity is a metric that quantifies the quality of an assignment of nodes to communities by evaluating how much more densely connected the nodes within a community are compared to how connected they would be, on average, in a suitably defined random network. The Louvain method of community detection is an algorithm for detecting communities in networks that relies upon a heuristic for maximizing the modularity. The method consists of repeated application of two steps. The first step is a "greedy" assignment of nodes to communities, favoring local optimizations of modularity. The second step is the definition of a new coarse-grained network in terms of the communities found in the first step. These two steps are repeated until no further modularity-increasing reassignments of communities are possible. The Louvain method achieves modularities comparable to pre-existing algorithms, typically in less time, so it enables the study of much larger networks. It also generally reveals a hierarchy of communities at different scales, and this hierarchical perspective can be useful for understanding the global functioning of a network. Meanwhile, there are certain pitfalls to interpreting the community structure uncovered by the Louvain Method; these difficulties are actually shared by all modularity optimization algorithms. I strongly recommend reading the original paper on the Louvain method [1]. It's fairly short, easy to read, and will give you a solid understanding of the algorithm. Incidentally, one of the originators of the Louvain Method (Renaud Lambiotte) seems to have a Quora account; it would be awesome to get his perspective on recent developments and applications. Long version: Below, I'm going to give my own description of the Louvain method. To hopefully not be too redundant with the original paper, I'm going to also focus on background, context, applications, and follow-up work. Covering all that ground makes this answer pretty lengthy, so please just skip to the sections that interest you. In particular, if you're familiar with the problem of community detection and the role of modularity in community finding algorithms, you can skip to the Louvain method section below. Also, before I begin, I'll point out that my discussion will exclusively consider the case of undirected, but possibly weighted, links. The Louvain method has been extended to the directed case, so interested readers should seek out the relevant literature. Motivating the Problem of Community Detection A fairly common feature of complex networks is that they consist of sets of nodes that interact more with one another than with nodes outside the set. Social networks, for instance, might consist of tightly knit communities of friends with rarer friendship ties between different communities. In protein interaction networks, certain groups of proteins interact with one another more frequently than they do with proteins in other groups. If you map out a network of projects going on in a large company, certain projects will likely have more conceptual overlap and mutual dependency than others; this network structure will hopefully be mirrored to some degree by the organizational structure within the company. In 1962, H.A. Simon proposed that this type of community structure might be a defining characteristic of complex systems, or at least those like the protein interaction network, in which many interacting constituent elements adaptively organize to achieve some higher-order function (e.g., the functioning of an organism). His reasoning was that individual actors have a much higher chance of collectively achieving a higher-order function if that function can be iteratively achieved by constructing intermediate stable forms (also called communities or modules) that achieve simpler functions. The first-order intermediate forms would be communities in terms of the original nodes, but then interactions between these first-order communities could generate second-order communities that accomplish somewhat more complicated functions, and so on. In this way, a hierarchical structure can emerge in complex adaptive systems [2]. However, even when there isn't necessarily adaptive pressure towards achieving some higher-order function (as in the case of some social networks), community structure is a common observed feature of complex networks. When we're handed a data set describing a network, we typically don't know  what the underlying community structure is. We often don't even know how many first-order communities we should be looking for, let alone how many hierarchical levels of communities we should consider. Meanwhile, for the reasons given above, the community and hierarchical structures are often very useful for understanding the overall functioning of the complex system. This motivates the search for good community detection algorithms. Definition of Modularity One popular class of community detection algorithms seeks to optimize the so-called modularity of the community assignment. Modularity is a metric that was proposed by Newman and Girvan in reference [3]. It quantifies the quality of a community assignment by measuring how much more dense the connections are within communities compared to what they would be in a particular type of random network. There are two mathematical definitions of modularity that you'll frequently encounter. The first is the one used in the original paper on the Louvain method [1]: Q=12m ∑ i,j [A ij −k i k j 2m ]δ(c i ,c j )  Here, A  is the usual adjacency matrix, k i =∑ j A ij   is the total link weight penetrating node i  , and m=12 ∑ i,j A ij   is the total link weight in the network overall. The Kronecker delta δ(c i ,c j )  is 1  when nodes i  and j  are assigned to the same community and 0  otherwise. Consider one of the terms in the sum. Remember that k i   is the total link weight penetrating node i  , and note that k j 2m   is the average fraction of this weight that would be assigned to node j  , if node i  assigned its link weight randomly to other nodes in proportion to their own link weights. Then, A ij −k i k j 2m   measures how strongly nodes i  and j  are in the real network, compared to how strongly connected we would expect them to be in a random network of the type described above. The other definition of modularity that you'll see is the one proposed in the paper by Newman and Girvan [3]. They defined a matrix (let's call it B  , though Newman and Girvan used the symbol e  ), where B c 1 c 2    denotes the fraction of all edge weight in the network that connects community c 1   to community c 2   . They also defined W c 1  =∑ c 2  B c 1 c 2    as the total edge weight penetrating community c 1   . Then, they defined the modularity as: Q=∑ c [B cc −W 2 c ]  It's pretty easy to show that this definition is equivalent to the one from the Louvain method paper, but I won't write out the steps here. Modularity Optimization Algorithms Returning to the first definition of modularity above, the problem of modularity optimization can be thought of as assigning communities such that the elements of the sum that contribute are as positive as possible (remember that the delta function kills terms in the sum corresponding to pairs of nodes that belong to different communities). The problem is that you can have situations where it makes sense to put nodes 1  and 2  in the same community and to put 2  and 3  in the same community, but where A 13 −k 1 k 3 2m   is very negative, indicating that nodes 1  and 3  really shouldn't be in the same community. This is a simple example of a kind of frustration that makes the modularity optimization problem really hard. Indeed, the modularity optimization problem is actually NP-hard, and that has motivated searches for heuristic approaches that typically do a good job at finding high modularity community assignments with more scalable complexity [1]. The modularity maximization algorithms that I'm aware of typically fall into one the following categories [3]:    agglomerative: where you iteratively group nodes into communities divisive: where you progressively remove links from the network, and the depleted network reveals good community assignments simulated annealing: where you introduce an artificial "temperature" and perform Metropolis-like Monte Carlo updates while gradually lowering the temperature. Here, you take −Q  to be the energy function for the system. The proposed moves can consist of both agglomerative and divisive steps [4] spectral: where an eigenvector of the so-called modularity matrix encodes the community structure In their original 2003 modularity paper [3], Newman and Girvan proposed a divisive algorithm for modularity optimization. The intuition behind their algorithm can be motivated by the following diagram: To the naked eye, it's obvious that this network consists of two densely connected communities that are only linked to one another by the single link connecting nodes 5  and 12  . That link has been highlighted in blue to emphasize the special role it plays. If that single link were to be cut, it would completely decouple the two communities. A metric that distinguishes the blue link from the others is the edge betweenness centrality. That link lies on all geodesic paths from the community on the left to the community on the right, so it will have the highest betweenness centrality of any edge in the graph. More generally, edges with high betweenness will tend to be those that link communities rather than those that connect nodes within communities. Therefore, Newman and Girvan proposed iteratively removing the edges with the highest betweenness centralities. After each removal, the betweenness of all the edges needs to be recomputed to figure out which edge to remove next. The procedure can be stopped at any point, with the connected components of the edge-depleted graph giving the communities. You can continue the Newman-Girvan procedure until the network is fully depleted, computing the modularity of the community assignment at each step, and ultimately choosing the intermediate step that maximized the modularity. This process takes time O(N n N 2 ℓ )  , where N n   is the number of nodes and N ℓ   is the number of links in the network. If the networks under consideration are sparse so that N ℓ ∝N n   , this algorithm is O(N 3 n )  Between 2003 (when the Newman-Girvan paper came out) and 2008 (when the Louvain method was proposed), several faster modularity optimizing algorithms were proposed by: Clauset, Newman, & Moore (CNM) [5] Pons and Latapy (PL) [6] Wakita and Tsurami (WT) [7]All of these methods are agglomerative, some of them run in close to linear time (in the number of nodes, or by the assumption of sparsity, in the number of links) on sparse networks, and it was these methods that the Louvain method's originators benchmarked their algorithm against (see the table at the end of the next section). Louvain Method of Community Detection Now, I'm going to illustrate how the Louvain Method works by applying it to a "connected caveman graph." This is a network where you begin with N cl   fully-connected cliques of M  nodes each. Next, you arrange these cliques in a circle. Then, you take one random link from each clique and rewire it so that the clique is connected to its nearest clockwise neighbor. You do this once for each clique, and you end up with something that looks like this: All links in this initial network have unit weight. This is the test network with which we'll explore the Louvain Method below. The "intuitive partition" here consists of the six communities of five nodes each that we've put in by hand. At the beginning of the Louvain Method, we assign each node to its own community, so in the connected caveman network above, there are 30 initial communities, each containing one node. Stage 1: Community Reassignments In the first stage of the Louvain Method, we iterate through each of the nodes in the network. For each node, we consider the change in modularity if we remove the node from its current community and place it in the community of one of its neighbors. We compute the modularity change for each of the node's neighbors. If none of these modularity changes are positive, we keep the node in its current community. If some of the modularity changes are positive, we move the node into the community for which the modularity change is most positive. Ties can be resolved arbitrarily. We repeat this process for each node until one pass through all nodes yields no community assignment changes. For the initial connected caveman graph, we can figure out what will happen in the first few steps of this community reassignment change by hand. In my implementation of the Louvain Method, the first node considered was 29  , which is connected to nodes 25  -28  . If we were to merge nodes 25  and 29  into the same community, that would correspond to activating the terms A 25,29 −k 25 k 29 2m   and A 29,25 −k 25 k 29 2m   in the sum above. Each of these have value 1−25120 =1924   . So performing the community reassignment will boost the modularity by 1912   . Actually, all of the proposed mergers for node 29 will have this same modularity boost, so we can accept any one of them. My implementation assigns node 29  to the same community as node 25  and then moves on to consider another randomly selected node. Actually, in the initial graph, merging any pair of neighboring nodes will increase modularity by the amount that I calculated in the previous paragraph. In particular, this will be true even for pairs like nodes 10  and 8  , which intuitively belong to different communities. This means that, in the initial stages of the community reassignment phase, pairs of nodes like this have a possibility of being assigned to the same community. It's an instructive exercise to think through how mistakes like this are remediated as the community reassignment stage progresses, and I encourage you to do so. Here are the community assignments that my implementation of the Louvain Method finds after the first community identification stage: As you can see, the algorithm has already discovered the "intuitive" communities, which is unsurprising, since this is a rather simple network. Stage 2: Coarse Graining The next stage in the Louvain Method is to use the communities that were discovered in the community reassignment stage to define a new, coarse-grained network. In this network, the newly discovered communities are the nodes. The edge weight between the nodes representing two communities is just the sum of the edge weights between the constituent, lower-level nodes of each community. The links within each community generate self-loops in the new, coarse-grained network. In the simple connected caveman network that we're studying, there's only one, unit-weight link connecting neighboring communities, so the links between the coarse-grained communities also have unit weight. If there were two or more links between communities, the new coarse-grained link would have weight equal to the sum of all the lower-level links. Meanwhile, within each community, there are 5×42 −1=9  unit-weight links, so the self-loops have weight 9  . Here's what the coarse-grained network will look like: In this graph, the black numbers associated with the links indicate edge weights, but the white numbers on the nodes are just labels for the communities. As a side note, I had to draw in the self-loops by hand because NetworkX, while generally an awesome Python package for doing these types of analyses, doesn't appear to draw them. If you're writing your own implementation of the Louvain Method, you'll want to be careful about how you're assigning the diagonal matrix elements of the adjacency matrix (corresponding to the self-loops). To be able to consistently apply the modularity formulae above, you actually want to double the diagonal matrix elements. This means that, for the graph above, the diagonal elements will be 18  . It's worth thinking through why this choice is appropriate for preserving the interpretation of modularity that I mentioned above. A good consistency check for debugging your implementation is that, when you construct your coarse-grained network, m=12 ∑ i,j A ij   shouldn't change. Repeated Iteration of Stages 1 and 2 The rest of the Louvain Method consists of repeated application of stages 1 and 2. By applying stage 1 (the community reassignment phase) to the coarse-grained graph, you find a second tier of communities of communities of nodes. Then, in the next application of stage 2, you define a new coarse-grained graph at this higher-level of the hierarchy. You keep going like this until an application of stage 1 yields no reassignments. At that point, repeated application of stages 1 and 2 will never yield any more modularity-optimizing changes, so the process is complete. For the connected caveman graph, the process terminates on the second community reassignment stage. We can see why if we propose a merger of two communities in the coarse-grained graph above. This would result in a modularity change of 2∗[1−20∗20120 ]=−143   , so no merger will be accepted. As an exercise, I'd encourage you to think about what happens if the original caveman graph had more than 20  cliques. You can work this out analytically, or you can implement the Louvain Method yourself and test it out. My own implementation of the Louvain Method took less than 200 lines of Python code, and I'm not particularly good at keeping my Python code (or my Quora answers) brief. Working through the 21-clique test case, whether you do it analytically or numerically, will give you some intuition for the final section of this answer: "Possible Pitfalls of Modularity Maximization." In the original paper on the Louvain Method, the authors benchmarked their new algorithm against some of the other modularity optimization algorithms that I mentioned above. Here's a table from the paper showing that the Louvain Method achieved similar modularities in typically faster time [1]: Real-World Applications of the Louvain Method One of the applications reported in the original Louvain Method paper was a study of a large Belgian phone call network in which nodes represented customers and weighted links represented the number of phone calls between two customers over a six-month period. The network had 2.6 million nodes and 6.3 million links. The Louvain Method revealed a hierarchy of six levels of communities. At the top level of this hierarchy, the communities representing more than 10,000 customers were strongly segregated by primary language. All except one of these communities had an 85% or greater majority of either French or Dutch speakers. The sole community with a more equitable distribution was positioned at the interface between French and Dutch clusters in the top-level coarse-grained network. Here's what the authors had to say about this community [1]: These groups of people, where language ceases to be a discriminating factor, might possibly play a crucial role for the integration of the country and for the emergence of consensus between the communities. One may indeed wonder what would happen if the community at the interface between the two language clusters... was to be removed. Since 2008, the Louvain Method has found a wide range of applications in analyzing real-world networks. Several of these can be found on the "official website" for the method: analysis of online social networks like Twitter, LinkedIn, Flickr, Youtube, and LiveJournal analysis of collaboration communities in citation networks analysis of a network of retail transactionsOne application that I want to call out independently, just because I found it particularly interesting, is the study of brain networks using the Louvain Method [9]. This is pretty far outside my expertise, so I won't pretend to understand all the details, but the authors of that study found similar community structure across the brains of 18 different people and modules that made sense from a functional perspective. Possible Pitfalls of Modularity Maximization The Louvain Method, and modularity optimization algorithms more generally, have found wide application across many domains. However, fundamental problems with these algorithms have also been identified. To close out this answer, I want to discuss some of these difficulties. Here's a quick introduction to two: The "resolution" limit: If you worked out the 21-clique caveman graph example that I suggested above, you would have noticed that for larger networks, the Louvain Method doesn't stop with the "intuitive" communities. Instead, there's a second pass through the community modification and coarse-graining stages, in which several of the intuitive communities are merged together. This is unfortunately a general problem with modularity optimization algorithms. They have trouble detecting small communities in large networks. It's a virtue of the Louvain Method that something close to the intuitive community structure is available as an intermediate step in the process. The "degeneracy" problem: There are typically an exponentially large (in network size) number of community assignments with modularities close to the maximum. This can be a severe problem because, in the presence of a large number of high modularity solutions, it's (a) hard to find the global maximum and (b) difficult to determine if the global maximum is truly more scientifically important than local maxima that achieve similar modularity. Good et al. showed that the different locally optimal community assignments can have quite different structural properties [10].These problems are discussed and studied at length in reference [10]. The authors of that paper concluded with this cautionary note about modularity maximization: ...modules identified through modularity maximization should be treated with caution in all but the most straightforward cases. That is, if the network is relatively small and contains only a few non-hierarchical and non-overlapping modular structures, the degeneracy problem is less severe and modularity maximization methods are likely to perform well. In other cases, modularity maximization can only provide a rough sketch of some parts of a network's modular organization. References [1] V.D. Blondel et al. Fast unfolding of communities in large networks. J. Stat. Mech. P10008 (2008). [2] H.A. Simon. The Architecture of Complexity. Proceedings of the American Philosophical Society. 106-6: 467 (1962). [3] M.E.J. Newman and M. Girvan. Finding and evaluating community structure in networks. Phys. Rev. E. 69: 026113 (2004). [4] R. Giumerà and L.A.N. Amaral. Functional cartography of complex metabolic networks. Nature. 433: 895 (2005). [5] A. Clauset, M.E.J. Newman, and C. Moore. Finding community structure in very large networks. Phys. Rev. E 70, 066111 (2004). [6] P. Pons and M. Latapy. Computing communities in large networks using random walks. Journal of Graph Algorithms and Applications. 10: 191 (2006). [7] K. Wakita and T. Tsurami. Finding Community Structure in Mega-scale Social Networks. arXiv: 0702048 (2007). [8] V.D. Blondel. Louvain method website. [9] D. Meunier et al. Hierarchical Modularity in Human Brain Functional Networks. Frontiers in Neuroinformatics. 3: 37 (2009). [10] B.H. Good,  Y-A de Montjove, A. Clauset. The performance of modularity maximization in practical contexts. Phys. Rev. E 81, 046106 (2010). Updated 24 Oct • View Upvotes