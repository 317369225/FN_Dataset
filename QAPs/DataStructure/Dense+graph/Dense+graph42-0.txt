Facebook is actively using Apache Giraph in its graph analysis pipeline. What were the factors that affected this architectural decision?Does Facebook consider Apache Spark as the next-generation data processing framework?
I don't know of the leadership decisions that led to the shift, but its actually pretty easy to see why Giraph/Pregel is more effective than Hadoop for large scale graph processing. If you read the Pregel paper, which Giraph is based off, then you'll see that Pregel is essentially a message passing interface. It takes a large graph, splits the nodes into batches, trying to keep densely connected nodes on one machine as much as possible. Then, Giraph runs this process until a stability condition is satisfied: Step 1: Each node computes something Step 2: Each node passes a message to its neighbors. This kind of process is not bound to the the constraint of having to collect keys on a handful of reducers, which is the inherent constraint of MapReduce/Hadoop. Graph processing is just one case that benefits from removing a bottleneck from the MapReduce process. Google internally (Cloud Dataflow is the external facing product) as well the as Apache Spark project are exploring ways to break distributed computation problems into a more flexible set of paradigms than MapReduce allows for. 