How can I do a projection with a 100-dimensional vector to preserve the inner product?
There are (at least) two possible reasons for why you want to do this. 1) You want a compact representation that approximately preserves the geometry of your data (as defined by the inner product).  The first thing I would do here is to check the eigenspectrum of your data.  If there's a heavy tail after the 10th eigenvalue, there's basically no way to achieve what you want.  Otherwise, you can simply take the top 10 eigenvectors as your projection. 2) You want a compact representation that achieves good prediction performance.  Even though the overall dimensionality of the data may be high, the dimensionality of some classification surface could be quite low.  In this case, you might want to try using something like Hash Kernels. However, keep in mind that things like hash kernels really only start working once you hit very high dimensional data (e.g., reducing 1 million dimensions).  100 dimensions isn't really that big in the grand scheme of things, and reducing down to 10 seems overly severe. 