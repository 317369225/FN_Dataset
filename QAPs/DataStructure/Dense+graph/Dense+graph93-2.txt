How can I do a projection with a 100-dimensional vector to preserve the inner product?
I agree with some of the previous recommendations.  However, perhaps, the simplest way to reduce dimensionality while preserving the inner product (as well as the Euclidean distance and the cosine similarity) is to do classic random projections. I recommend double-checking, e.g., these articles to make sure you are doing them correctly: 1. Experiments with Random Projection, Sanjoy Dasgupta. 2. Random projection in dimensionality reduction: Applications to image and text data. lla Bingham and Heikki Mannila Random projections do preserve the inner product with high probability. However, as mentioned by Wei Wang (and I fully agree), random projections aren't very helpful if you carry out k-NN search on top of this (i.e., in the space of projected vectors). Even though popular libraries like Annoy do rely on random projections, the overall accuracy is rather low,  because, IMHO, you can't get more than 60-70% of your nearest neighbors efficiently. In particular, in our recent experiments, we found out that to get 10 nearest neighbors at recall 90%, you may actually need to carry out a 1000-nearest or even 10000-nearest neighbor search in the space of projections (instead of 10-NN). This kind of search is hard to do in a way other than brute-force filtering. This may be an intrinsic problem related to all dimensionality-reduction approaches, i.e., better sketching techniques may not necessarily make things much better (bu who knows exactly). However, the cone trees is not the only way to proceed with a cosine-similarity based search. 1) We know that k-NN graphs are quite good for high-accuracy kNN search. See, e.g., the results of of our recent evaluation: http://boytsov.info/pubs/da2014.pdf 2) If the intrinsic dimensionality is low, you can use class metric space structures such as VP-trees, or k-means clustering (check e.g., the FLANN library). The thing is that cosine similarity appears to be almost metric in many cases. In addition, a simple monotonic transformation of the cosine similarity produces a fully metric distance function (called angular distance). PS: If you have very sparse vectors of high dimensionality, I would also recommend to use a hashing trick to obtain dense vectors of the fixed dimensionality first. Simply speaking, for every element i with the value x[i] in the original vector, you add the value x[i] to the element hash(i) % dense_vector_size in the target dense vector. This makes sense if you build an index and store a lot of pre-processed dense vectors. The hashing trick (sorry I don't remember who proposed it) may decrease the quality slightly. However, with the big enough size of the dense vector, the deterioration is negligible. 