MLconf 2015 Seattle: When do I use zero-mean/unit variance normalization vs unit L1/L2 normalization?
The short answer is that when you have dense data (meaning for every dimension you have a measurement) you use zero mean/unit variance normalization and when you have sparse data (meaning that every observation has very few measurements, most of the data is zero), then you use use unit L1 or L2 normalization. An interesting clarification though is that zero mean / unit variance normalization is performed typically on every column unit L1/L2 is done on every row The first normalization puts the data into a "Gaussian" ball centered around the origin while the second normalization places the data on the surface of the unit sphere. We quoted the "Gaussian" because your data might not actually be Gaussian. The truth is that if your data is continuous values and dense it is fairly reasonable to make this assumption. When your data is sparse (and usually nonnegative) the row normalization makes them behave as a probability distribution (most of the times multinomial). There is also another practical perspective. If you do just zero mean normalization on sparse data then it is not going to be sparse any more, since all the zeros will become nonzero when you subtract the mean. But that is kind an unorthodox way to answer the question. What if you really need to do it? In that case you keep on a separate vector the means divided by variances and you apply them later in your computations. For example if you have to take the dot product between a sparse vector that has been zero mean normalized and the unit vector that has all ones, then you sum your initially non-zero elements and then you add the count of initially zeros multiplied by the -mean/variance. The people who follow me know that I always have an unorthodox way of responding to questions. All of the above was just intuition and rule of thumbs, kind of what  happens in most of the cases. The truth is that normalization depends on what you really want to do and differs from case to case. Let me show you some case studies and more references to read: If you are going to train a linear classifier  with gradient methods, then you get much faster convergence if you do zero-mean unit variance. Look at the excellent talk of Yann Lecun and John Langford at the NYC big data open source http://cilvr.cs.nyu.edu/diglib/l... When you are doing PCA if you don't do zero mean normalization then you are going to get a very high magnitude first eigenvalue that is going to limit the visibility of your principal components. This is also show in the previous slide set. The short story is that it will distort the results of PCA (Principal Component Analysis) and you will not get the full picture If your data is really relational, which means that they are basically a graph then L1 normalization of columns and rows  and then SVD (singular value decomposition) is equivalent to doing spectral clustering. Kernel matrices or in general matrices  that represent the similarity of item at row i with the item at column j benefit a lot from row column unit L1 normalization. See the work by Steffane Lafon here http://en.wikipedia.org/wiki/Dif... When you want to measure similarities between two rows with the dot product then it is a good idea to do row/column unit L1 normalization. For example if every row has the counts of bag of words from text documents, you want to divide each column (word) by the total counts (that is the L1 norm of column) because you want to stress that frequent words have low information. Then you want to do a unit L1 row normalization, because you want to penalize documents with too many words (encyclopedia). If you don't do that you will end up having the encyclopedia as the nearest neighbor to every other document. As a last comment, if you are trying to compute nearest neighbors with dense data mean normalization will not affect the results, but unit variance will distort the geometry a lot and most of the times it will give you very bad results. I hope these case studies were useful, 