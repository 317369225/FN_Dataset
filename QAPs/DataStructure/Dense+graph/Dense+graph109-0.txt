Why are sparse matrices an important consideration in optimization and machine learning?
The answer is simple. In machine learning, you might have a correlation matrix or stochastic matrix whose edges define a relationship between data points. The algorithm used by Google to rank pages for their search engine is a stochastic matrix, meaning that each element denotes some probability for a "transition" from one [webpage] to another, via a click or something similar by the user. It is a graph of the internet, in a sense. But it's pretty clear why this graph should not be densely populated. What's the probability that you'll jump to a CNN article about Ukraine from a math tutoring website? Very small, as it's pretty unlikely. Most things don't have direct connections with each other, a statement that runs contrary to the often-used phrase, "everything's connected, man" (it's not that this is wrong, it's just missing the information about how things are connected, which is to say, very indirectly). The Google stochastic matrix for all Wikipedia articles has a rank of about 3 million. If you store the complete matrix, zeros included, you are storing 9 trillion 64-bit floating-point numbers. For an arbitrary number of pages, the number of numbers you have to store goes as the square, which quickly becomes unmanageable even for the world's fastest supercomputers (which typically also have the largest memories). For Google, this is simply unacceptable. But given the earlier example, we know that the graph encoded by the stochastic matrix is sparse. In practice, you would see the numbers you need to store drop by several orders of magnitude. Usually sparsity is a good assumption, but even if the reality of the problem disagrees with this, which can certainly be the case, it is usually very beneficial to assume that it does not, both for computational tractability and for the interpretability of your model. For example, modeling a physical system like a pond can be done using a statistical model that links many things together, like water temperature, humidity, wind, solar radiation, dissolved oxygen, air pressure, etc. However, if you want to predict the temperature of the water at some depth over time, it's unlikely that the humidity will effect the type of vertical oscillations you'll see in a water column. The more likely explanation is because of variations in solar radiation (night and day, plus seasonal trends) and internal seiches due to wind. This does not mean that humidity is completely independent of water temperature, of course. You will find that this is true using a simple dynamical or regression model. Most vanilla optimization methods start to slow down big time as you add variables to search over. This is obvious--what is the area of a circle, compared to the volume of a sphere with the same radius? Keep extending to a case where you have 100 dimensions and you'll see that the search space grows immensely. Dimensionality reduction methods are, in some way, attempting to enforce a type of sparsity on a problem (it is not usually the same thing in a technical sense however, this is just intuition). For the interested reader, see this very nice answer by Charles Yang Zheng about sampling in high-dimensional spaces and how it relates to optimization. 