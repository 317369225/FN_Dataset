What are some examples of how conditional random fields works?
A conditional random field can be thought of as a set of factors, each representing a relation between a number of variables. Suppose we have a target class variable Y  and a set of features X  , one way of inferring about the class Y would be to find the joint probability distribution P(X,Y)  . But this would require some knowledge about the correlation between the features of X  . Moreover this would lead to a very densely connected graph representation, which may be computationally very expensive. An alternative model would be to use a conditional distribution P(Y|X)  . With the help of a neat normalization trick, we can compute it without any knowledge of the correlation between features, just focusing on the target variable Y  . The unnormalized probability P ~ (X,Y)  is simply the product of the individual factors, ϕ i (X i ,Y)  . To get the conditional probability P(Y|X)  , we need to normalize P ~ (X,Y)  with P ~ (X)  . P ~ (X)=Σ Y  P ~ (X,Y)  and P(Y|X)=P ~ (X,Y)P ~ (X)   Now we have the conditional probability, without the explicit need to specify the correlations between the different X i   s. Logistic regression can be thought of as a simple CRF, with a graph similar to a Naive bayes model, the only difference being that edges being undirected and each edge having a factor defined. As you can see there is no need to capture the correlations between x1, x2 and x3 , since they don't really matter when computing the conditional probability. That is why logistic regression works better than generative models. To learn more about conditional random fields and other probabilistic graphical models, you should check out Daphne Koller's Coursera course. Updated 30 Nov 2012 • View Upvotes