What is the best algorithm to match resumes with jobs?
There isn't really going to be a definitive, unquestionably best algorithm for an ML / data mining problem like this (the Netflix Prize winners employed a multitude of different techniques in their matching system), but here are some ideas. I'm not sure what your setup is for training your model, but I'll assume that you have a machine learning training set consisting of a set R of resumes, a set J of jobs, and <r,j> pairs of known good matches.  In practice, if this were a job website, you would collect those pairs for training and validating candidate algorithms from user actions (jobs viewed, applied for, taken, etc.).  Maybe the <r,j> edges are even weighted. With these inputs, you have a bipartite graph G of resumes and jobs, from which you can easily create a weighted graph G_j of jobs to model job similarity, where "similar" means "likely to attract the same kind of candidates."  For example, if the vertices of G_j are all jobs in G, you can imaging creating (or incrementing the weight of) an edge <j_i, j_k> for all resumes r for which there are edges <r,j_i> and <r, j_k> in G.  You could also consider job connections that are more than one resume apart.  The general idea is to use the proximity and connectedness of jobs via resumes to come up with a graph to model job proximity.  You can analogously come up with a weighted graph G_r of resumes to model resume proximity.  With G, G_j, and G_r, one possible algorithm that takes a job description and finds matching resumes with strength scores would run like this: 1. A job has a bunch of "features", in the simple case just say keywords, so you can run a nearest neighbors algorithm on the jobs in G_j (simple feature-based k-nearest neighbors, i.e., doesn't use the structure of G_j). 2. Now you have a subset of weighted jobs J' that are close in the sense that there's a high degree of candidate overlap. 3. For all jobs in J', use G_j to expand / reweight the set of candidate jobs into a set J''. 4. For all jobs in J'', use the original graph G to collect weighted resume matches R'. 5. For all resumes in R', use G_r to expand / reweight the set of candidates into a final list R'' of weighted candidates. One of the strengths here is that kitchen sink resumes don't get the advantage pointed out in the question because there's not really any direct feature-based analysis done on resumes.  Also, this should do a reasonably good job at not being tricked in the face of semantic equivalence even though it doesn't directly do anything on this front.  E.g., people with database technologies on their resumes should be clustered in G and G_r, and similarly DB jobs should be clustered in G and G_j, so we could reasonably assume that enough data would be present for a job description with keywords like "MySQL" and "Oracle" to match with people listing "PostgreSQL" and "Microsoft SQL Server" in their resumes without the system needing any knowledge of semantics. There are certainly weaknesses though.  The algorithm relies on the input data being reasonably dense.  The algorithm is going to be as good as the connectedness of resumes and jobs from the graphs.  For sparse data, you might want to consider more strongly feature-based approaches.  E.g., instead of ending at step 5, you could imagine building a feature-based scoring model based on the candidates in R'' (in English, "What variables and coefficients best separate these resumes apart from the rest?") and running that model against the broader set of resumes. In addition to looking at the problem at this "pattern matching" level, one could make huge accuracy gains in a preprocessing phase that does more intelligent feature extraction rather that hardcoding features and / or feature relationships.  Rather than pattern matching raw job features against raw resume features, you transform the feature model into something more powerful so that, e.g., a highly correlated but weaker variable might be ignored in the final feature model.  PCA (principal component analysis) can be used to transform an input feature vector in a (smaller) vector whose variable values explain data most effectively.  In the resume / job case, look at MCA (multiple correspondence analysis) since the data will be more categorical. On the preprocessing front, you could look at NLP techniques for extracting semantic relationships and use them ahead of time rather than letting those relationships surface later on during the matching phase. 