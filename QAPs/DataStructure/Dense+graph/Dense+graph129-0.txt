What are examples of one CPU architecture clearly beating another, despite both chips' implementations having similar semiconductor process sizes and die areas?
I have the heart of a physicist too, so I think I understand the spirit in which the question is asked. It's a good one. The challenge is finding a "controlled experiment" in which two architectures target the same workload using the same level of semiconductor technology, yet one beats the snot out of the other one. I do recall exactly such an example. The workload is solving systems of linear equations. The year is 1985. The price range is around $500,000 per computing system. In one corner, the Intel iPSC, one of the early hypercube architectures: This was one of several commercializations of the Caltech Cosmic Cube, a collection of PC-like processors that used message-passing as the parallel programming model. The iPSC used Intel 80286 processors with 80287 coprocessors. The largest version could have 128 processor-coprocessor pairs; I don't have the exact power requirements but when it was demonstrated at a trade show, it required its own diesel generator to operate, prompting one conference attendee to ask how many megaflops per gallon the thing was getting. In the other corner, the Floating Point Systems FPS-164/MAX, an array processor: The parallel programming model for the FPS-164/MAX was VLIW (very long instruction word) scheduling of functional units operating in parallel, augmented by VLSI from Weitek to perform 64-bit floating-point adds and multiplies under the control of a single instruction stream (single-instruction multiple data, or SIMD). The Weitek chips were of a size and process level similar to those from Intel; Weitek was formed as an Intel spinoff. Intel hired Cleve Moler to assist with optimizing the use of the iPSC, especially for solving dense systems of linear equations. Moler is one of the original authors of LINPACK (as well as the creator of Matlab), so his qualifications to do high-performance tuning could hardly be higher. In his paper, "Matrix Computation on a Hypercube Multiprocessor," he was able to crank the efficiency up to about 70% of peak speed, and achieved a speed of just over 1.4 MFLOPS on a 64-processor version of the iPSC. Each 80287 coprocessor had a speed of 31.25 KFLOPS for double-precision (64-bit) arithmetic. From his paper, here is a graph of how performance increased as the number of 80286-80287 processor pairs was increased from 4 to 64: Remember, this was 1985 and the CRAY supercomputers of that era were able to achieve over 100 MFLOPS on a system costing over $10 million. So Intel was pretty excited at achieving 1.4 MFLOPS on a system costing far less and built out of parts intended for the IBM PC. Now let's look at the other contender. The FPS-164/MAX also could scale its hardware, by adding accelerator boards instead of processors. It had a peak speed of 341 MFLOPS when all 15 accelerator boards were configured. The optimization of the same matrix computation is described in the paper "Introducing Replicated VLSI to Supercomputing: the FPS-164/MAX Scientific Computer" written by Alan Charlesworth and myself. Here is how the MFLOPS scaled for that machine: By solving even larger systems than shown here, we were able to achieve 330 (double-precision) MFLOPS. If the Intel iPSC results were plotted on the same vertical scale, they would be indistinguishable from the x-axis since they are lower by over two orders of magnitude. Why such a huge advantage for the FPS-164/MAX? In a nutshell, it was architected for speed with a range of techniques (VLIW, pipeline execution, SIMD, re-use of data in space (broadcast), re-use of data in time (storage in local memories), all under the control of a processor that had high speed at both scalar and vector operations. The iPSC was architected mainly as a showcase for Intel's PC and Ethernet chips, and as an interesting platform for academics to experiment with distributed memory computing; it was a one-trick pony, though, relying entirely on a single architectural trick (message passing) for parallel speed. Both machines used state-of-the-art VLSI, with replication of the same parts to create an economy of scale. The prices of the systems, the power consumption, the amount of floor space they require, and the markets they were aimed at, were extremely similar so I believe this is the kind of example a physicist would view as a controlled experiment. 