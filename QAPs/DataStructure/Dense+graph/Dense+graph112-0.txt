Deep Learning: How do I gather deep learning papers?I want to create a corpus of deep learning papers. So far Ive gathered a couple of thousand using arxiv, but I would like to parse upwards of 10k papers. Are there any ways to do this?
Hmm, so just to be clear: You want a field-specific automated paper gatherer? Assuming the answer is yes: Technically you can get a couple of root ones manually and then go through the bibliography sections for each of them automatically to find parent candidate papers.  For "children" candidate papers you should probably use citation engines on main candidate papers (https://www.bc.edu/libraries/hel...).  After doing that you will get a candidate list, at that point your task is binary classification (relevant/not relevant) given text and graph structure. You will have a graph, and elements that have a lot of connections (two papers are connected if one of them cites another) to each other are supposedly relevant. So you can try to estimate the dense regions of your connection graph, nodes in the dense region are significant deep learning papers.  The field is relatively open so you are not going to hit paywall much 