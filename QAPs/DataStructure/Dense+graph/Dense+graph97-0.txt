Bulk Synchronous Parallel Computing: Does the BSP model deal with submachine locality at massive scale?
In a comment on a related question What are models of computing that are relevant on current architectures: superscalar, multicore, attached accelerators such as GPU or MIC, clusters with distributed memory?, the following points were made. "On current scales and especially on future scales moving data is more important than computing…  Any model that does not deal with affinity will be irrelevant pretty quickly on current and future architectures…  We need something that models hierarchically, to reflect physical reality". This latency issue and the related issue of submachine locality is something that has been discussed at great length over the past 20+ years. We covered it in our paper "Questions and Answers About BSP" back in 1996. The paper is available here http://paloaltodata.com/index.ph... It's also discussed again in great detail in Les Valiant's recent Multi-BSP paper, which is available here http://people.seas.harvard.edu/~... Since the mid 1990s we have pointed out that BSP can be easily extended to model a two-level hierarchy, if the local and global latencies are very different. A very large system today might be, for example, 100 physically and geographically separate systems, each one consisting of 1,000 servers, with fast LAN connectivity locally, and slow WAN connectivity globally. Such a system should of course not be used as a homogeneous 100,000-processor BSP machine if you are planning to run an app that has high global communication requirements. That's simple common sense. The BSP cost model will tell you if that is the case or not. E.g. if you're doing dense matrix multiplication you may be fine, but not if you're running certain types of massive sparse linear system solver, which inherently require large amounts of global communication. I describe examples of such "inherently global" problems in my 1995 "Scalable Computing" paper, which is also available at the above link. I point out there that if the sparse matrix has an "expander graph" structure then u=M.v is provably an inherently global (inherently non-local) problem . In today's world of "big data" it is indeed important to exploit as much data locality as possible, as pointed out in the comment. But that's always been the case. It's been the case in supercomputing since single monolithic systems were replaced with horizontally scaled clusters fifteen years ago. In my opinion, BSP provides the best cost model for addressing this data locality issue and related issues around latency and submachine locality. Updated 10 Dec 2013 • View Upvotes