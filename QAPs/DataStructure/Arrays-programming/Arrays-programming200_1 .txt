Why don't they (CS book writers and programmers) make arrays that are sequential in memory and linked together?I wrote such a library once, dedicated to be able to allocate arrays that could grow in the tens of millions of items. These consisted of blocks containing arrays, and if an array was full, a new block could be linked to the previous. There was also a special "root" node, which defined how the "array blocks" needed to be allocated. The process of developing the code was fun, it was tremendously hard to get it bug-free (and bound-checked) but in the end I managed it. However, it turned out that: Sorting was nearly impossible I had to write access functions for every type I wanted to store in the array (very frustrating when working with C structs) There was a penalty if I wanted to remove items from an array (never got around to implementing a linked list for the free slots) The performance gain was much smaller than I originally anticipated So, the library was used in one single project on Windows 3.11 (there, it actually was useful because of the 64KB boundaries, so I could scale the data buffering well), which later also ran fine on Windows XP, but from that time onward I have always been able to justify using hashes, B+ trees or similar storage structures instead. 191 Views  View Upvotes