How can I store and process large 2d array?I have to store and process the relations (correlations) between a large number of objects. And I'm looking for a right concept for this problem. The array can be dynamically resized by adding new objects. I'm going to present the relation values as a float number (like in the image).Unless you have more details I would throw memory at the problem. If you have 1000 x 1000,000 float values this is 4 GB of memory. Even 32 GB of memory might be worth less than one day of your time. If you have much more data than this, try loading only portions of the data at any one time. if you are correlating everything against everything else i.e. O(N^2) the processing time should be your bottleneck rather than memory. One option for loading data from files efficiently is to use memory mapped files. These can be mapped in and out of virtual memory without copying the data. At this point you are only limited by your disk space. Ideally an SSD would help but if you are CPU bound it won’t matter. You could reduce your memory foot print by using 16-bit values or some simple compression to reduce memory used by lots of zeros or lots of repeated for example. Exactly how best to do this depends on the data you are using. e.g. you could use Run Length Encoding. Run-length encoding e.g. Object_1000 would be 3 x 0.5, 3 x 1.2. Compressing the data intelligently could speed up the processing time. Also when you do the correlations you might change the weight of each sample. e.g you might only count when one of the two values changes or after both values have changed. This can give a very different outcome. You might ignore small changes as noise. 305 Views · View Upvotes