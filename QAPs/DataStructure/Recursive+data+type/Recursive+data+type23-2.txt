What is the main difficulty in parallel computing?
There are different types of parallelism. Kalyana describes one, and in his application I'll go along with his analysis. However, in scientific computing parallelism often arises fairly naturally: you have large data sets, and a computation performed on each data point, or each pair of points, or each pair and some that are connected to it. In other words: tons of parallelism. Most scientists around me have no trouble keeping 10s of thousands of cores spinning, and the parallelism arises fairly naturally from the application. The problem here is mainly one of load balancing, choice of the right algorithm, and making sure the communication is as efficient as possible. Often the problem is mostly one of programming: the most efficient code is also the most tedious. Recently there is a new type of parallel computing, which involves large graphs such as the internet, facebook, netflix, and such. Here again there is tons of data parallelism, but now the data is unstructured and connections are unstructured. Traditional cluster architectures do not do well here at all, and instead people have had success using somewhat exotic machines like the Cray MTA-2 or Cray XMT. The problem here is one of low latency irregular data access. This area is still wide open for research. 