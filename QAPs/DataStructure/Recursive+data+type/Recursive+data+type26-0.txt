How does compiler know whether the recursion is a tail recursion or not and how does it optimize tail recursion?
Tail call elimination can turn certain function calls into jumps which don't use the stack, making them more efficient and preventing stack overflows. Tail Position A compiler can eliminate a function call if it's in tail position (regardless of whether the call is recursive or not—tail call elimination can hit normal function calls as easily as recursive ones.) Tail position is a syntactic property, meaning that we can get it solely by looking at the code without doing additional processing or abstract execution. Intuitively, a function call is in tail position if it is the last thing evaluated in the function. As long as we do literally nothing else after calling the function, the call can be eliminated. I'm going to give you a couple of examples using JavaScript-ish syntax which is familiar to people and makes things painfully explicit. First, here is an example of a tail call: function foo(a, b, c) {  let x = a + b * c;  return bar(x, x + 1);} Note that we can do computation in the arguments to the function (ie x + 1) as long as we don't need to do anything with the return value. Here are a couple of examples not in tail position: function foo(a, b, c) {  let x = a + b * c;  let y = baz(a, b);  return y + bar(x, x + 1);} Intuitively, the reason we can't elide the call to bar is that we need to keep the value of y around after we get the result of bar to calculate the result of foo. We do this by keeping track of y in a stack frame, which is exactly what tail call elimination avoids. The other piece of information managed by the stack is where the function was called from. Even if we don't need to use any variables from the stack, if the function call isn't in tail position we still need to keep track of it. The following function won't be optimized either: function foo(a, b, c) {  let x = a + b * c;  return 1 + bar(x, x + 1);} Some of these cases could still be optimized in theory but would require substantially more complicated analysis. In order to remain predictable to the programmer, tail call elimination as promised by many functional languages (like Scheme and ML) only hits calls that are strictly in tail position. Writing Functions with Tail Calls in Mind There are a few general tricks that can help you write functions that are more efficient with tail calls. Recursive functions can often use an accumulating parameter that contains the result calculated so far and is passed from call to call. Here's a sum function whose recursive call is not in tail position: function sum(list) {  if (empty(list)) return 0;  return head(list) + sum(tail(list));} The problem is that we do our addition after the recursive call to sum which forces us to use the stack. We can rewrite this by keeping track of the sum so far as an extra parameter, doing the addition before the recursive call: function sum(list, acc) {  if (empty(list)) return acc;  return sum(tail(list), head(list) + acc);} Often, we would factor this logic out into a helper function so that the original function still takes a single argument. Here's that version, using the Haskell convention of calling helper loop functions go: function sum(list) {  return go(list, 0);  function go(list, acc) {    if (empty(list)) return acc;    return go(tail(list), head(list) + acc);  }} The call to go is also in tail position. Compiled, using a helper function won't have meaningful performance overhead. In some compilers (like GHC) this style of function is actually better for optimization because it exposes sum to be inlined since it isn't recursive, which in turn enables other optimizations. This trick works whenever the recursive function is "linear" in shape—there's only one recursive call per iteration. You wouldn't easily be able to rewrite functions with more complex recursion like the classic Fibonacci function or most tree traversals in this style, so don't try. This makes sense because more complex recursive functions use the stack to implicitly keep track of their state and control flow. If you want to avoid the stack, you'll have to do that bookkeeping explicitly instead. Continuation-Passing Style Luckily, there is a systematic way to turn every function call into a tail call. You need to rewrite all your functions such that instead of returning directly, they accept a callback and call that instead. This style of code is called continuation passing style (CPS) because the callbacks you pass around specify how the code continues after an expression and are called continuations. If you've done asynchronous programming in JavaScript (like Node.js) you are probably already familiar with continuation passing style: that's what all the callbacks in non-blocking JavaScript code are. (Promises are a bit more complex but use continuation passing style under the hood, just giving you a better interface over it.) I covered CPS in more detail in two other Quora questions: What is continuation-passing style in functional programming? What are some interesting applications of tail-recursion? Formal Definition Earlier, I mentioned that whether a call is in tail position is a syntactic property. This means that we can formally define it by induction over the syntax of a language, as is done in the Revised^6 Report on the Algorithmic Language Scheme (which formalizes the Scheme programming language). This is useful if you want to understand exactly when a call is in tail position or if you want to write a function that finds tail calls in a program's abstract syntax tree (AST). An inductive definition like this is particularly easy to implement in a language with algebraic data types like Haskell because it boils down to pattern-matching on the AST and apply each case. To do this you have to consider every part of the syntax (ie every production in the grammar), which is tedious even for a small language like Scheme. The definition I linked above, for example, is straightforward but also over a page in length, so I won't reproduce it here. Read the report if you're curious or need to implement tail call elimination yourself. The important insight is that we can detect calls in tail position by recursing down an AST. Particularly, this is a local property: we only need a single pass through the tree to find tail calls and we don't need to keep track of any additional information. This means that, as long as we have a good way to turn a function call into a jump (which depends on the language's calling convention—which is why languages like Rust and Scala don't implement full tail call elimination), implementing tail call elimination is straightforward. It takes no complicated analysis and is easy for programmers to understand, which is why languages like Scheme and ML promise tail call elimination. For those languages, eliminating tail calls is not just an optimization—it's part of their semantics. 