Why doesn't Haskell have mutable array without IO monad?
You really have two questions here: one in the title and one in the details. From the title, if all you want is a mutable array not in IO, you can use an STArray. The main difference is that you can allocate a mutable array, do stuff to it and then safely turn it back into an immutable array. Crucially, this means you can have mutable array code inside a function that looks perfectly immutable to the outside. So if you didn't mind doing the optimization yourself, you could write your fill function as follows: import Control.Monad (forM_)import qualified Data.Array.MArray as MArrayimport Data.Array.ST (runSTArray)fill n a = runSTArray $ do    arr <- MArray.newArray (0, n) 0 -- fill with 0 to start    forM_ [0..n] $ \ i -> MArray.writeArray arr i i    return arr (I didn't actually compile the code so there are probably some mistakes, but you get the idea.) It's verbose, to be sure, but that's partly a matter of API design: since mutating an array is an uncommon thing to do in Haskell, it's made painfully explicit. If you were doing a lot of array manipulation like this, you could come up with your own function names and operators instead. What you're asking for in the details is a bit different—you want immutable array operations to be turned into a bunch of mutable operations implicitly, as an optimization. This is actually a bit harder than it seems because detecting whether it would be safe to do this optimization is decidedly nontrivial. Happily, this not only exists but is actually supplied as a library in the form of Data.Vector. Data.Vector achieves this sort of optimizations with a set of rewrite rules—directives that tell GHC to replace certain patterns in code with other patterns. This is a mechanism for specifying domain-specific compiler optimizations in libraries, which is pretty incredible. (I don't know of other languages that do this.) Combined with GHC's aggressive inlining, these rules capture a lot of common patterns where an intermediate array is created and consumed immediately and fuse them together, not allocating the array. This whole approach is known as stream fusion and is described in detail in "Stream Fusion: From Lists to Streams to Nothing at All". I'll give a brief sketch of the idea here—it's surprisingly simple—but if you really want to understand it read the paper. The paper talks about lists which are simpler and easier to think about, but the idea translates naturally to other structures like vectors. The optimization comes in, essentially, two parts. The first transforms normal list operations into non-recursive ones that consume a stream data type and the second one fuses these operations together. Splitting this into two parts keeps the number of rules linear to the number of fusable operations—we have a single rule per fusable function rather than one for every possible pair of functions. The Stream type represents a series "steps" in a function like map or filter: you take in a single element and do something to it before going on to the next element in the stream. It looks like this : data Stream a = ∃s. Stream (s → Step a s) sdata Step a s = Done               | Yield a s               | Skip s The existential type ( ∃s) is used to encapsulate the internal state used in the stream. This isn't quite legal Haskell; you'd have to use the ExistentialQuantification extension which is more verbose. Now we can write the functions we care about ( map, filter and so on) over Stream a instead of [a]. Crucially, the functions themselves don't have to be recursive—the recursion is part of the Stream type. Here is a version of map, for example: mapS :: (a -> b) -> Stream a -> Stream bmapS f (Stream next_0 step_0) = Steam next step_0  where next step = case next_0 step of      Done -> Done      Skip step' -> Skip step'      Yield x step' -> Yield (f x) step' Each step can tell you one of three things: you're done, you should skip this step or here's an element. For map, if you're done, you're done. If you're skipping, you're skipping. If you have an element of the input type, you apply the function to it an have an element of the output type. Skip is there for things like filter: filterS :: (a -> Bool) -> Stream a -> Stream afilterS f (Steam next_0 step_0) = Stream next step_0  where next step = case next_0 step of      Done -> Done      Skip step' -> Skip step'      Yield x step' -> if f x then Yield x step' else Skip step' It's the same as mapS except, if you have an element, you check whether it satisfies your predicate. If it does, you pass it on; if it doesn't, you skip. Now we can combine our stream operations with toStream :: [a] -> Stream a and fromStream :: Stream a -> [a] functions to give us operations over lists. Thus, map f xs is equivalent to (fromStream . mapS f . toStream) xs. This would be awkward to write by hand, of course, but we don't have to: since the two expressions are equivalent, we can have rewrite them automatically with a rewrite rule. The second part of the system is fusing these operations. Here are two operations on a list which create and immediately consume an intermediate list: myList = filter (> 5) (map (+ 2) xs) After we run the rewrite rule we have above, it would look something like this: myList = (fromStream . filterS (> 5) . toStream . fromStream . mapS (+ 2) . toStream) xs Now we see what we can eliminate: toStream . fromStream has to be the identity function because going from a list to a stream and back should not change anything. So we can have another rewrite rule that just eliminates toStream . fromStream giving us a streamlined (heh) expression: myList =  (fromStream . filterS (> 5) . mapS (+ 2) . toStream) xs So now we've completely removed the intermediate list that was created in favor of a stream of steps. The cool thing is that, since filterS and mapS are not recursive, existing GHC optimizations can get rid of the intermeidate Step values too using existing general-purpose optimizations. A big part of the reason this approach works is that it plays well with GHC's aggressive inlining. The rewrite rule optimizations can fire after functions you use are inlined, allowing us to fuse away operations that are used in the definitions of different functions, including functions from different modules. Inlining is a surprisingly powerful optimization because it turns local optimizations (like our rewrite rules) into global ones that can even be applied across modules which results in a form of whole-program optimization that's way less complex than it usually is. Now, this isn't strictly making a structure immutable under the hood, but the effect is the same: instead of creating a bunch of extra intermediate copies, your operation can be done in a single pass. The Data.Vector module uses this for general optimizations and the repa package uses a similar framework to automatically efficiently parallelize array operations. All this said, this still won't work for your example, unless perhaps GHC is smart enough to unroll your recursive loop, which it probably isn't. The problem is that general recursion is quite hard to analyze in general and recursive functions aren't inlined. All these fancy rules and optimizations don't get a chance to fire for your fill function because all the recursive calls stay as function calls (ie jumps). While it's clear in this specific case that we can optimize your function, it's extremely hard or even impossible to determine this sort of information about more complex recursive functions. In general, if you want to take advantage of high-level optimizations, it's best to use library functions rather than writing your own recursive operations. The recursion itself can be compiled efficiently as a loop in assembly¹, but it will be opaque to other optimizations performed by the compiler. If you want to be really sure, you can rewrite the function yourself using ST like my first approach; for Data.Vector, this is possible with the modify function. Crucially, the type system guarantees that this is safe: nobody can see the mutation you're doing outside of your scope. footnotes ¹ For, umm, reasons, I recently implemented a really inefficient solution to Project Euler Problem 5 in x86, which took ~2.5 seconds to run. The exact same logic written as a recursive function in Haskell took ~3.5 seconds, so there's relatively little overhead. (I wouldn't be surprised if it's mostly in startup time and the Haskell runtime or something—I didn't exactly run a rigorous benchmark.) 