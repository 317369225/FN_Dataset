Why is file access so slow in Matlab?
[Edit] Multiple instances of Java threads can be opened by Matlab to speed up file i/o. Yair Altman of undocumented matlab describes this here: Explicit multi-threading in Matlab - part 1 This question really talks about accessing data stored across volatile + non-volatile memory (as the example illustrates) as opposed to reading multiple files stored directly on non-volatile memory (HDDs). In case of the first script, you fetch the 'next line' in every step of the while-loop, move the 'cursor' to the beginning of the line after that 'next line' while keeping track of that, and repeat till fgetl returns a numeric (-1) value. So you fetch lines 450K times in sequence, while incrementing a counter. In the second case, you read the whole file only(/at) once, that results in a Mx1 character array, then compare the array with the LF character. All values that are not LF characters are 0, those that are, are 1. So you end up with a large, but sparse array, then finally you add up all the elements of the array to get the total count. In the first case, you kept track of a pointer across a variable-sized block of memory that contains the HDD addresses corresponding to the requested path, fetching variable-sized values (making calls to the filesystem driver, opening and closing streams and allocating caches in RAM) stored in the addresses indicated by the pointer (implemented in fgets inside fgetl), carrying out logical operations on those values (internal to fgets, that generates a -1), then storing the result of operations in another location (count), freeing up allocated spaces and then updating addresses to indicate the next line. Then you repeat this till you get the errorval/-1. In the second case, you call the filesystem only once, a chunk of known memory gets assigned at one go (fread), the logical comparison operation is a low-complexity/memory footprint algorithm ('==', worst O(n)) and results in an object with the same size as the input. The sum function (worst O(n)) works similarly, but with known sizes. So the use of fgetl is the primary reason for the performance hit in the first case. This is also a functionally wrong usage of the fgetl command, it wasn't designed to enable LF+CR counting (note fgetl just wraps fgets). So going back to the question of slow-file-access-in-matlab, it is not. It works fine for an interpreted language. It only appears slow because the number of accesses was large in the script. Matlab generally croaks when numbers (iterations, variables, array sizes) start reaching in excess of 100K (because it has to make requests to and expand its memory from the default allocation). Instead of implementing explicit loops, significant benefits can be obtained by paying attention to the encoding and data-types used in data files and using 'find functions' (strfind/find on LF/CR commands) or logical array-evaluations -- then working with the resulting arrays. Even though matlab gives us freedom from memory management, it is rather a good idea to understand how it works (optimal data types based on application, rather than blind reliance on double/float -- lesser the memory we need or need runtime access to, zippier things are) A lot of functions in matlab already implement loops in binary and will always be faster than any interpreted loop we write. So that's another strategy, to offload things to specific, compiled functions. Updated 101w ago â€¢ View Upvotes