How can I prove the following graph theory problem?
Expanders are graphs that have good "expansion" properties. Graph expansion is a graph invariant that is related to the "connectedness" of a graph. For regular graphs, the expansion of a set of vertices [math]S \in V[/math] is defined as [math]h(S) = \frac{E(S,S\V)}{d|S|}[/math], where [math]E(S,S \V)[/math] is an indicator function for the number of edges that cross the boundary from the set [math]S[/math] to [math]S\V[/math] and [math]|S|[/math] is the cardinality of the set [math]S[/math]. The expansion of a graph [math]G[/math] is defined as [math]h(G) = \min_{1\leq S \leq \frac{n}{2}} h(S)[/math]. Expanders are basically sparse (usually regular) graphs that have a small diameter and high edge connectivity. They are used in cryptography, communications theory, and theoretical computer science. It is very easy to encounter expanders in the world, as I am told. However, they are notoriously difficult to analyze. You will most likely have to devise a way to show that for a family of d-regular graphs, [math]G_d[/math], the expansion [math]h(G_d)[/math] is uniformly bounded away from some constant, i.e. [math]h(G_d) > C[/math]. 