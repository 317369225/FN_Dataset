What is an intuitive explanation of a decision tree?A decision tree is a set of rules used to classify data into categories. It looks at the variables in a data set, determines which are most important, and then comes up with a, well, tree of decisions which best partitions the data. The tree is created by splitting data up by variables and then counting to see how many are in each bucket after each split. The key idea is that the procedure to create decision trees is recursive. For a set (S) of observations, the following algorithm is applied: If every observation in S is the same class or if S is very small, the tree becomes an endpoint, labeled with the most frequent class. If S is too large and it contains more than one class, find the best* rule based on one feature (e.g., "is weight > 150?") to split it into subsets, one for each class. If you had to go to step 2, apply step 1 to each new subset. If your subsets need to go to step 2, apply step 1 to the sub-subsets, etc. When everything is split up appropriately (into buckets that are very small or entirely one class), you have a set of rules that look like a tree.** Here's a minimum example. Given only the gender and weight of a person, can we predict whether they are Japanese or American? If this is the training set: Name/Weight (lbs.)/Sex/Nationality Larry 195 M American Jerry 190 M American Carrie 160 F American Cheri 165 F American Yoshi 165 M Japanese Yasuo 160 M Japanese Michiko 130 F Japanese Noriko 140 F Japanese ... then a decision tree would look like this: First, the model checks if a person is under 150 pounds. If so, they're classified as Japanese. If not, it sees if their weight is over 177 pounds. If so, they're American. If not, the last question asks whether it's a man or woman. With these three questions, you can use a person's sex and weight to predict what their nationality is. E.g., if you know a man is 200 pounds, the model predicts that he's American. This example is convenient because the tree can perfectly explain the data--in the real world, there is overlap; there are fat Japanese people and skinny Americans. The model would be optimized to make the largest possible number of correct predictions, even though it can't be perfect. (Note: decision trees are used for predicting continuous variables also, as opposed to categorical variables. It's easier to explain if I stick to categories.) * The "best" branching rule is the one that results in the most information gain in most tree-generating algorithms. ** Trees are usually "pruned" to avoid overfitting. The pruning algorithm removes final nodes, based on misclassification rates, so that the model is a little more general. 2.3k Views  View Upvotes  Answer requested by William Chen