How does boosting works with decision tree classification algorithms?Boosting in classification usually works by applying a higher weight to an error. In decision trees there in no loss function, there is the information gain (or is there?). So, what is the boosting procedure here?There are two ways that this can be done with decision trees, one that requires modifications to the decision tree algorithm, and one that doesn’t. Both of them are based on the fact that one of the outputs of boosting is a new weight for each instance, with the training instances that were classified incorrectly increased in weight and all others decreased in weight. So for example, you might end up with one instance having a weight of 1.8 and another with a weight of 0.7. Reweighting In this approach, you modify your in your information gain criteria (or gain ratio or Gini coefficient or whatever your favorite split criteria is) to include the weights of the instance in the computation. This is usually relatively, since reweighting for a weight w say, is essentially the same as having w copies of the same instance (although you can’t have 1.8 copies, the maths doesn’t care about partial copies). Resampling If your algorithm is a black box, then you randomly sample with replacement based on the weights. You then feed this into the decision tree classifier. So, for example, the instance with a weight of 1.8 is 1.8/0.7 = 2.6 times more likely to be included in the training set you give to the classifier for building the next level. In general, reweighting is preferred, because it has lower variance. 587 Views · View Upvotes · Answer requested by Avicohen