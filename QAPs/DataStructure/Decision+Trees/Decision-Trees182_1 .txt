Decision Trees: How do you prune a CART?There are generally two methods for pruning trees: pre-pruning and post-pruning. Pre-pruning is going to involve techniques that perform early stopping (e.g. we stop the building of our tree before it is fully grown). Post-pruning will involve fully growing the tree in its entirety, and then trimming the nodes of the tree in a bottom-up fashion. With pre-pruning, we'll typically make some decisions before we start growing the tree. I like to break these decisions down into two categories - size related decisions and condition related decisions. (1) For size related decisions, we'll decide that we don't want to grow it past a certain depth, we don't want more than a given number of total leaves, or we don't want fewer than a given number of observations per leaf (there are others, but these are common). (2) For condition related decisions, we'll decide that we will stop growing the tree if all observations in a leaf belong to the same class (if it's a classification tree), if all attribute values for the observations in a leaf are the same, or if expanding the current node does not improve the purity measure/cost metric we're using (again, there are more, but these are common). With post-pruning, there are also a number of available techniques, many of which are listed on the wiki for pruning. Probably the three most common are reduced error pruning, cost-complexity pruning, and pessimistic pruning (the first two of which are described on the wiki). (1) Reduced error pruning works by basically considering the replacement of the subtree at each node within the tree with a leaf, assigning all observations in that newly assigned leaf to the majority class (if a classification problem) or assigning them the mean (if a regression problem). If the replacement of this subtree with a leaf leaves our overall error/cost no worse, then we keep it, and otherwise we don't. We continue iterating over all nodes until the pruning is no longer helpful. (2) Cost-complexity pruning works by successively collapsing the node that produces the smallest per-node increase in our error/cost, while at the same time weighing the overall complexity (e.g. size) of our tree to decide upon the best pruned tree that minimizes our cost-complexity function. (3) Pessimistic pruning works by effectively adding in a penalty term to the error at each node. This penalty term is often referred to as an "error correction," with the motivation that we want to try to conservatively estimate the true error at each node. So, which one of these do we choose? If you're building the tree algorithm yourself and don't mind some extra computation, I believe that it's common (and most effective) to do post-pruning. This might be especially true given that the pre-pruning methods are going to allow you to achieve the same results (just in a different order). In terms of the different post-pruning methods, (1) and (2) above typically require that you use a training and validation set, which means more computation. (3) does not have this stipulation, and so involves less computation. This paper discusses these three techniques (along with one other), and basically comes to the conclusion that (1) typically performs the best, but not significantly better than (3) to make the extra computation worth it. As you might guess, the pre-pruning methods are going to save you the worry of the extra computation that some of the post-pruning methods bring, and probably not do that much worse (if at all) than the post-pruning methods. This is probably especially true if you optimize over the pre-pruning decisions you make. In practice, if you're using a tree algorithm that somebody else put together, then you'll be restricted to the decisions they give you. I am most familiar with the implementation that sklearn offers, which only gives you the option to pre-prune by specifying parameters like max depth, min. samples per leaf, etc. 224 Views