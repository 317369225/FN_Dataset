Why do people use gradient boosted decision trees to do feature transform?In Facebook's ADKDD 2014 paper, they use gbdt to do feature transformation before feeding features into the linear regression classifier. In the paper, they say this has something to do with the nonlinearity of the original feature space, but no details. Can someone elaborate on this ?Non Linearity in the Input Space: Consider the following set of points where we colored the points red to show negative labels and blue to show positive labels. Looking at the picture, one would realize that there is no way to separate the positive and negative labeled points using a single line. When we have a set of features with labels where we are unable to separate them using a single line, then we say that there is non linearity in the input space. Why is linearity/non-linearity important? Most of our classification models try to find a single line that separates the two sets of point. I say that they find a line (but a line makes sense for 2 dimensional space), but in higher dimensional space, "a line" is referred to as a Hyperplane. But, for the moment, we will work with 2 dimensional spaces since they are easy to visualize and hence we can simply think that the classifiers try to find lines in this 2D space which will separate the set of points. Now, since the above set of points cannot be separated by a single line, which means most of our classifiers will not work on the above dataset. How to solve? There are two ways to solve: One way is to explicitly use non-linear classifiers. Non linear classifiers are classifiers that do not try to separate the set of points with a single line but uses either a non-linear separator or a set of linear separators (to make a piece-wise non-linear separator). Another way is to transform the input space in such a way that the non-linearity is eliminated and we get a linear feature space.Second Method Let us try to find a transformation for the given set of points such that the non linearity is removed. If we carefully see the set of points given to us, we would note that the the label is negative exactly when one of the dimension is negative. If both dimensions are positive or negative, the label given is positive. Therefore, let x=(x1,x2) x=(x1,x2) be a 2D point and let f f be a function that transforms this 2D point to another 2D point as follows: f(x)=(x1,x1x2) f(x)=(x1,x1x2) . Let us see what happens to the set of points given to us, when we pass it through the above transformation: Oh look at that! Now the two sets of points can be separated by a line! This tells us that now, all our linear classification models will work on this transformed space. Is it easy to find such transformations? No, while we were lucky to find a transformation for the above case, in general, it is not so easy to find such a transformation. However, in the above case, I mapped the points from 2D to 2D, but one can map these 2D points to some higher dimensional space as well. There is a theorem called Cover's theorem which states that if you map the points to sufficiently large and high dimensional space, then with high probability, the non-linearity would be erased and that the points will be separated by a line in that high dimensional space. What have they done in ADKDD paper? There is a classifier called the gradient boosted decision tree. It is a non-linear classifier, that is, it is one of those classifiers that does not separate the labeled points using a line, but instead makes use of a set of separators. However, the problem with non-linear classifiers is that there can be different ways to separate labeled points using different set of separators. Usually to get over this, people do multiple things: 1] You learn several non-linear classifiers (different different decision trees for example) and then average their outputs. 2] Alternatively, one can take the outputs of these multiple decision trees and give it to another classifier to decide on the final label. The second method above is a form of stacking [1]. Stacking essentially means that you stack several classifier one on top of the other and then feed output of one classifier to another. In the ADKDD paper, the authors stack Gradient Boosted Decision Trees (GBDT) and Logistic Regression (LR). Now, LR is a linear classifier, that is, the labeled set of points are separated by a line in LR model. However, the authors claim that their features show non-linearity (with respect to the labels). But they observe better results by stacking a GBDT with LR. Which is probably why they claim that GBDT serves to eliminate the non-linearity in the features thus enabling the LR to give better results. At the end of the day however, it is stacking [1]. The authors may interpret it any number of ways so as to gain an understanding of why it might perform better than conventional models. Stacking is known to do better than single model and even model averaging methods in some cases. Your take away from their work is that stacking is useful and can be used sometimes. Their interpretation of it as a feature transform that eliminates non-linearity in the input space may hold true only for their application. There are no guarantees that the transformations using the decision trees will yield a separable feature space to learn from. [1] Ensemble learning 6.2k Views  View Upvotes