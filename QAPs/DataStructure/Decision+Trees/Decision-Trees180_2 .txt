What are the advantages of logistic regression over decision trees?Are there any cases where it's better to use logistic regression instead of decision trees?Let me add to Jack's points. Logistic Regression is better than decision trees particularly when you are dealing with very high dimension data. Text classification is a classic problem. You might have 100,000 train documents and also see around 500,000 distinct words (features). In such a case, a simply rule like learning of a linear hyperplane is strictly better due to the Curse of dimensionality, since Decision Trees have far too many degrees of freedom and you will almost necessarily overfit. One could still try to use a decision tree on text data by doing Feature selection. However you will lose on a lot of valuable information for text classification, by merely picking a small reduced subset of features. When learning models with high dimensional data, it is very easy for variance based errors to get out of hand and simple models with higher bias are a better bet.  Decision Trees are likely to be a better fit for problems where you have a small number of features (say < 100 features) and plenty (say > 100,000) train examples. In such a case, your data permits you to learn more complex decision rules. Here variance is a smaller concern and one would likely be better off opting for Decision Trees with their high expressive power and low bias. I have written in more detail about what learning algorithms would be suitable for different kinds of data here: Vijay Krishnan's answer to Big Data: What algorithms do data scientists actually use at work? 16.2k Views  View Upvotes