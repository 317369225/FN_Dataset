What does the weight of a data point change about the way a decision tree is fit?For example, AdaBoost (see p.13) creates a new learner/decision tree every iteration i with a weight α. Each data point is originally equally weighted where ω = 1/N. The weight of each observation is updated every iteration. What does ω actually mean for how each new tree is built or fit?adaboost, like all boosted or ensemble methods is a basis function expansion where typically the basis functions are estimated in parallel, e.g., random forest, sequentially, greedy, or are known a priori…the problem then becomes estimating the coefficients corresponding to each basis function. Now a tree is typically constructed using a gini coefficient to guide which node and which split to determine as “best”. There are a variety of approaches to attribute variable important across ensembles of trees but a leave one out method, if computationally feasible is one way. The elements of statistical learning cover this in great depth. good luck 188 Views · View Upvotes