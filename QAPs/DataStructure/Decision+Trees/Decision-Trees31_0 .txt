What does the weight of a data point change about the way a decision tree is fit?For example, AdaBoost (see p.13) creates a new learner/decision tree every iteration i with a weight α. Each data point is originally equally weighted where ω = 1/N. The weight of each observation is updated every iteration. What does ω actually mean for how each new tree is built or fit?When building a decision tree, you usually use the information gain or a similar criteria for making splits. From that article: Let T  T denote a set of training examples, each of the form (x,y)=(x 1 ,x 2 ,x 3 ,...,x k ,y)  (x,y)=(x1,x2,x3,...,xk,y) where x a ∈vals(a)  xa∈vals(a) is the value of the a  a th attribute of example x  x and y  y is the corresponding class label. The information gain for an attribute a  a is defined in terms of entropy H()  H() as follows: IG(T,a)=H(T)−∑ v∈vals(a) |{x∈T|x a =v}||T| ⋅H({x∈T|x a =v})  IG(T,a)=H(T)−∑v∈vals(a)|{x∈T|xa=v}||T|⋅H({x∈T|xa=v}) What we do is modify the above equation used for deciding which way to split a decision tree to read (note this isn’t quite right, but gives the gist): IG(T,a)=H(T)−∑ v∈vals(a) ∑ α {x∈T|x a =v}∑α ⋅H({x∈T|x a =v})  IG(T,a)=H(T)−∑v∈vals(a)∑α{x∈T|xa=v}∑α⋅H({x∈T|xa=v}) In other words, we multiply each instance by the weight and instead of dividing by the number of instances, we divide by the sum of the weights. 330 Views · View Upvotes · Answer requested by 1 person