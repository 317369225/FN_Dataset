How does a decision tree automatically pick up the effect from the product of two independent variables?Such that one does not need to specify X1*X2 explicitly. Just feeding X1 and X2 solely to the model is enough.Interesting question. Lets see if we can guess what the decision tree learns. Pertinent facts about decision trees: A tree tries to model any outcome as nested if-else statements. In the case of a regression tree, where the outcome is a continuous value, the goal of the tree is to have leaves where the variance of the training responses within a leaf is as low as possible. Consider 1. Your decision tree would learn a bunch of statements like the following: if 9≤x 1 ≤9.5  9≤x1≤9.5 and 5≤x 2 ≤5.5  5≤x2≤5.5 return 46 if 9≤x 1 ≤9.5  9≤x1≤9.5 and 10≤x 2 ≤10.5  10≤x2≤10.5 return 92 if 10≤x 1 ≤10.5  10≤x1≤10.5 and 5≤x 2 ≤5.5  5≤x2≤5.5 return 51 if 10≤x 1 ≤10.5  10≤x1≤10.5 and 10≤x 2 ≤10.5  10≤x2≤10.5 return 102 Each statement represents a rectangular region in the coordinate space (for ex the first statement defines a region defined by 9, 9.5, 5, 5.5). The multiplicity of these statements would best try to mimic, using discrete regions, the product x 1 ⋅x 2   x1⋅x2 for different values of x 1   x1 and x 2   x2 . Yes, its messy, but it gets the job done (on most real world data anyway). When you have a significant term like x 1 ⋅x 2   x1⋅x2 , note that as the values of x 1   x1 or x 2   x2 increase, the value of the response increases fast. If you want to look at the math, consider ∂x 1 x 2 ∂x 1  =x 2   ∂x1x2∂x1=x2 ; hence, greater slope at higher values for x 2   x2 . Similarly for ∂x 1 x 2 ∂x 2  =x 1   ∂x1x2∂x2=x1 .  Considering point 2 above, this would mean for high values of  x 1   x1 or x 2   x2   you would have more leaves. So that the variance in each of regions a leaf defines is low. Looks like what we get is lot of regions trying to mimic x 1 ⋅x 2   x1⋅x2 , with a lot more of these regions at high values of either variable. This is easily seen using scatter plots. Lets assume we have a 2-dimensional dataset with features as x 1   x1 and x 2   x2 , and response as y=x 1 ⋅x 2 +ϵ  y=x1⋅x2+ϵ , where ϵ  ϵ represents noise. For this example the noise follows a normal distribution with mean 0  0 and standard deviation 1  1 . When the features vary in the range 1-10, this is what the responses look like; a darker shade denotes a higher response value ("black" is close to 100 in this example): ​ ​ ​ If you fit a regression tree over this dataset, ensuring some kind of pruning occurs, and then let the tree predict the responses for each of the points in the original dataset, you get this: ​ ​ ​ I had limited the tree to have 20 leaves. Like we guessed, more leaves/regions where the value of the response increases fast. EDIT 1: Adding a slightly different perspective. Here is what the scatter plot looks like for 200 leaves: If you step back a bit and look at the plot, it almost looks like the plot of the original dataset. It actually looks like a pixelated version of the original plot. Except that decision trees are smart about how many pixels to use. In regions where the shade of gray doesn't vary much, it uses larger pixels. For regions where the shade varies a lot, it uses smaller and more pixels to better approximate the original coloring. This is true for all approximations done by decision trees (not just the specific case of x 1 ⋅x 2   x1⋅x2 ). (Quick data munging and plotting, courtesy scikit, matplotlib, numpy) 844 Views · View Upvotes