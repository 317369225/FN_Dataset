What method XGBoost uses for pruning and regularization?Hi and thanks for the question. Pruning and regularisation two methods that share the same purpose and principle. The purpose is to control model complexity and the principle is simple models tend to generalise better than complex models. Similarities apart, pruning is can only be used for decision trees while regularisation can be applied to any learning algorithm. There are many methods for pruning, since the depth of the tree, to the number of nodes, to the use of statistical distributions such as the Chi-square, etc (check Kuncheva’s book “Combining pattern classifiers” on section 2.4.). Pruning can be performed while the tree is growing or after the tree has grown, and a tree are typically developed using a greedy algorithm (i.e. picking the best split at every node, and hoping that the accumulation of locally best results deliver the global best partition of the space). Now going specifically to your question: in slide 34 (top equation), the first 3 terms compose the training loss term, which for decision trees are the splitting metrics (e.g. entropy, classification error, etc - again check Kuncheva books on splitting rules, section 2.4.2). The last term is the regularisation term: the magical trade-off number between training error and complexity. If you pick a path from root to a leaf node adding the gain in each one, your equation will resemble the regularisation general formula. What you have there is combination of pruning and regularisation in one step. Check again Kuncheva 2.4.3 point 4. I want to keep the answer short, so have a quick look here: Introduction to Boosted Trees in the section “the structure of score”. There is explained in detail what I mean by “adding the gain in each one, your equation will resemble the regularisation general formula”. Hope it helps. 319 Views · View Upvotes