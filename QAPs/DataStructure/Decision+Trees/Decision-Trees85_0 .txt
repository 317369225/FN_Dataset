How can I apply decision tree for variable selection?People say "use decision tree for variable selection". I am confused. My understanding about what people say is that I construct decision trees with cross-validation applied and choose best decision tree. That means the decision tree itself has variables. Am I right?predictorImportance is a Matlab function which computes the varaible importance score from a decision tree. Below is its documentation which nicely explains how it works. Predictor Importance predictorImportance computes estimates of predictor importance for tree by summing changes in the risk due to splits on every predictor and dividing the sum by the number of branch nodes. The risk for each node is the measure of impurity (Gini index or deviance) for this node weighted by the node probability. If tree is grown without surrogate splits, this sum is taken over best splits found at each branch node. If tree is grown with surrogate splits, this sum is taken over all splits at each branch node including surrogate splits. Predictor importance associated with this split is computed as the difference between the risk for the parent node and the total risk for the two children. Estimates of predictor importance do not depend on the order of predictors if you use surrogate splits, but do depend on the order if you do not use surrogate splits. If you use surrogate splits, predictorImportance computes estimates before the tree is reduced by pruning or merging leaves. If you do not use surrogate splits, predictorImportancecomputes estimates after the tree is reduced by pruning or merging leaves. Therefore, reducing the tree by pruning affects the predictor importance for a tree grown without surrogate splits, and does not affect the predictor importance for a tree grown with surrogate splits. Impurity and Node Error ClassificationTree splits nodes based on either impurity or node error. Impurity means one of several things, depending on your choice of the SplitCriterion name-value pair argument: Gini's Diversity Index (gdi) — The Gini index of a node is shown below, where the sum is over the classes i at the node, and p(i) is the observed fraction of classes with class i that reach the node. A node with just one class (a pure node) has Gini index 0; otherwise the Gini index is positive. So the Gini index is a measure of node impurity.               Deviance ('deviance') — With p(i) defined the same as for the Gini index, the deviance of a node is shown below. A pure node has deviance 0; otherwise, the deviance is positive. Node error — The node error is the fraction of misclassified classes at a node. If j is the class with the largest number of training samples at a node, the node error is 1 – p(j). Yes whatever you have heard is right. In a cross-validation framework, you get different trees for the 10 folds. You may select one of the models. But i suggest you could take the average of variable importances from the 10 trees. This is more appropriate. For more info: Estimates of predictor importance Binary decision tree for classification Example program in Matlab Now in the example, 'trees'  variable contains the 10 trees created. For instance, trees{1} is trees{2} is Similarly, there are trees{3} to trees{10}. Each tree gives a variable importance score and I save it in a variable 'pred_imps'. So this variable is 10X4 matrix, as there are 4 variables. Now I take the mean to get a 1X4 array which is the average predictor importance which is in 'avg_pred_imp'. Now to explain mathematically, Take trees{1}. It has 9 nodes, 4 branch nodes and 5 pure nodes. A pure node is one which does not have any branches. Now you have to compute the node risk for each of the 9 nodes. Usual used measurement for this is the Gini diversity index. Now it is computed as where p(i) is the probability of each class i at a node. This gdi is then multiplied with the node probability. Node probability is the number of samples reaching the node divided by total number of samples. Node risk is the product of gdi and node probability. This does not stop here. Now you have to compute variable importance which is obtained by summing changes in the risk due to splits on every predictor and dividing the sum by the number of branch nodes (which is 4).  Now the node risks for the 9 nodes are among these 1st and 4 th correspond to varaible x3. Now the change in node risk/num_branch_nodes is {(0.6667 -0 - 0.3333) + (0.0665 - 0.0145 - 0.0198) }/4 which is 0.0914 which is what we get from the program. 535 Views · View Upvotes