How do decision trees for regression work?Classification makes sense--you get the nodes for each category. But wouldn't you have a ton of nodes if you want a real-valued answer?The most common method for constructing regression tree is CART (Classification and Regression Tree) methodology, which is also known as recursive partitioning. Take basic regression tree as example: The method starts by searching for every distinct values of all its predictors, and splitting the value of a predictor that minimizes the following statistic (other regression tree models have different optimization criteria): SSE=∑ i∈S 1  (y i −y 1  ¯ ¯ ¯ ¯ ¯  )+∑ i∈S 2  (y i −y 2  ¯ ¯ ¯ ¯ ¯  )  SSE=∑i∈S1(yi−y1¯)+∑i∈S2(yi−y2¯) where y 1  ¯ ¯ ¯ ¯ ¯    y1¯ and y 2  ¯ ¯ ¯ ¯ ¯    y2¯ are the average values of the dependent variable in groups S 1   S1 and S 2   S2 . For groups S 1   S1 and S 2   S2 , the method will recursively split the predictor values within groups. In practice, the method stops when the sample size of the split group falls below certain threshold, e.g., 50. To prevent over-fitting, the constructed tree can be pruned by penalizing the SSE with tree size: SSE c p  =SSE+c p ×S t   SSEcp=SSE+cp×St where S t   St is the size of the tree (number of terminal nodes), and c p   cp is complexity parameter. Smaller c p   cp will lead to larger trees, and vice versa. Of course, this parameter can also be tuned by cross-validation. Unlike linear regression models that calculate the coefficients of predictors, tree regression models calculate the relative importance of predictors. The relative importance of predictors can be computed by summing up the overall reduction of optimization criteria like SSE. 1.7k Views · View Upvotes