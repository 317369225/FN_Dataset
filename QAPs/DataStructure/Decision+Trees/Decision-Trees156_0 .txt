What are the disadvantages of using a decision tree for classification?Related: What are the advantages of using a decision tree for classification?3 Problems with Decision Trees I illustrate by fitting a decision tree model in R to the "iris" dataset, which collects measurement data on 3 species of flowers. I focus on two of those measurements: sepal length and sepal width. library(rpart) library(rpart.plot) model1 <- rpart(Species ~ Sepal.Length + Sepal.Width, iris) prp(model1, digits = 3) Now, I will perturb the data by adding 0.1 to each datapoint with probability 0.25, and subtracting 0.1 to each datapoint with probability 0.25. set.seed(1) tmp <- function() rbinom(nrow(iris), size = 1, prob = 0.5) perturb <- function() (tmp() - tmp()) / 10 iris$Sepal.Length.Perturbed <- iris$Sepal.Length + perturb() iris$Sepal.Width.Perturbed <- iris$Sepal.Width + perturb() model2 <- rpart(Species ~ Sepal.Length.Perturbed + Sepal.Width.Perturbed, iris) prp(model2, digits = 3) Key observation - Notice how just by perturbing the data a little bit, I made a different-looking decision tree? To get a better look at what's happening, I plot the decision tree boundaries and the actual data points on a scatter plot. I color each region by the plurality class. Some problem we see here when we apply our decision tree on continuous data: Instability - The decision tree changes when I perturb the dataset a bit. This is not desirable as we want our classification algorithm to be pretty robust to noise and be able to generalize well to future observed data. This can undercut confidence in the tree and hurt the ability to learn from it. One solution - Is to switch to a tree-ensemble method that combines many decision trees on slightly different versions of the dataset. Classification Plateaus - There's a very big difference between being on the left side of a boundary instead of a right side. We could see two different flowers with similar characteristics classified very differently. Some sort of rolling hill type of classification could work better than a plateau classification scheme. One solution - (like above), is to switch to a tree-ensemble method that combines many decision trees on slightly different versions of the dataset. Decision Boundaries are parallel to the axis - We could imagine diagonal decision boundaries that would perform better, e.g. separating the setosa flowers and the versicolor flowers. One very good method to reduce the instability is to rely on an ensemble of decision trees, by trying some sort of random forest or boosted decision tree algorithm. This also helps smooth out a classification plateau. An ensemble of slightly different trees will almost always outperform a single decision tree. If you prefer classification boundaries that aren't as rigid, you would also be interested in tree ensembles or something like K-Nearest-Neighbors. If you're looking for decision boundaries that are NOT parallel to the axis, you would want to try an SVM or Logistic Regression. See What are the advantages of logistic regression over decision trees? For the other side of decision trees, see What are the advantages of using a decision tree for classification? 13.7k Views  View Upvotes