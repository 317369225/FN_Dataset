What is the TDIDT algorithm?TDIDT is short for "top down induction of decision trees". It's really a family of algorithms that covers CART, ID3, C4.5 and similar algorithms (see Decision tree learning). Decision trees look like this: Each node in the decision tree tests an attribute (e.g. male). and each node is a class (in this case "died" or "survived"). Most decision tree algorithms have the same basic "divide and conquer" top-down recursive pattern: 1. Take the entire data set as input. 2. Find a split based on an attribute that maximizes some purity measure 3. Take the outcome of apply the split to the input data (the "divide" step). 4. Re-apply steps 1 and 2 to each split (the recursive "conquer" step). 5. Then prune back to reduce overfitting. It's "top down" because you look at the entire data in each step. It's "induction" because you're building the tree. So along the way, there are many different decisions for each of the components: Finding a split: Methods here vary from exhaustive search (e.g. C4.5) to randomly selecting attributes and split points (e.g. Random forests). Purity measure: Measures here include: information gain, gain ratio, Gini coefficient, minimum description length and Chi-squared values. Stopping criteria: Methods here vary from a minimum size, to a particular confidence in prediction, to certain purity criteria. Pruning method: Methods here include no pruning, reduced-error pruning, and in ensemble cases such as bagging, out-of-bag error pruning. These four different components can mixed and matched pretty much, so there are many many methods for induction of decision trees. 1.2k Views  View Upvotes