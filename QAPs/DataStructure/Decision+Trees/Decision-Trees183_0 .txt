Can someone compare different decision tree algorithms?The most famous decision trees are ID3, C4.5 and CART. Can someone compare these different algorithms?If by comparison you mean how they work and which are the differences, then the following are some important facts. ID3 can be seen as an early draft of C45. So it can be saw as a C45 which does not handle missing values, does not handle numeric input variables, uses only entropy and infoGain as impurity functions, does not perform pruning. So the comparison will be only between CART and C45. Node splitting CART does only binary splits, C45 does binary splits for numeric variables, and can produce more than 2 child nodes for nominal variables. As a consequence C45 use nominal variables only once as a split test from a leaf to the root, since it exhaust the discriminative power of that variable. Also as a consequence C45 can be used to generate rules from the tree, while CART is not able to do this. Another consequence is that C45 tends to build smaller trees. Another significant consequence is that CART by not exhausting nominal variables once used as a test, is able to delay the split decision, capturing usually more detail. Missing values CART use surrogate variables to distribute instances to a single child node, while C45 distributes all instances with missing value for test attribute to all child nodes, but with smaller weights (proportional with the total weights of the instances from children). Pruning C45 uses binomial confidence interval upper values as criteria for pruning a node, while CART uses cross validation for tree pruning. CART has a stronger statistical reasoning for pruning, while C45 procedure breaks a lot of statistical assumptions, but it happens, as the author himself stated, to work at least in teh data sets that he used to work with Impurity function CART uses Gini gain criteria, while C45 uses Shannon Entropy, Info Gain, Gain Ratio. However, this impurity functions usually has similar results since most of them have strong roots in the more general Renyi entropy. CART only CART has variable importance feature which is extended in the later works for Random Forests. CART has also the option called twoing (which splits a nominal variable in two partition labels). For some cases this might be very helpful. C45 only C45 has an option to build more trees from an incremental data set called windowing. It starts to build a tree from a small subset of the data and later from larger subsets with incremented size and uses some performance criteria. This follows the idea of Occam's Razor, build as simple as possible without loosing the relevant things. However it is not usually needed since, as Breiman said, Occam's Razor is not really a problem in machine learning (I do not remember the paper exactly, but it is a paper about two cultures in statistics ad machine learning). General notes C45 produces usually smaller and more interpretable trees, while CART produce longer trees with slightly greater accuracy. However as a generic problem with most trees, interpretability has not a solid statistical ground, mostly due to improper scoring measures which trees proposes, so this is why trees are mostly used in meta algorithms like RF or boosting families, where the interpretability is impossible. 2.8k Views  View Upvotes