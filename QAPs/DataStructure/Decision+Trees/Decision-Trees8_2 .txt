When is the random forest better than decision tree?Decision Trees: The output (decision tree) is easy to understand which is very important in many applications (especially if you are trying to understand your inferences) [Structurally] Unstable/(not Robust) — small changes in training data can lead to significant changes in the structure of the trees and different predictions for the same validation examples. NOTE: There are some ways to improve the stability of traditional decision trees by using bootstrap The method (as opposed to the output) is easy to understand and explain (people don’t like using stuff they don’t understand — it’s always a challenge to explain to a customer why they should spend their 10 million dollar advertising budget in a certain way because a random forest says so) Computational less expensive than a random forest Random Forsts: Opaque — it’s really difficult to understand what a thousand random trees are saying Robust — does’t suffer the instability problems of decision trees The method is harder to explain and understand for a layperson Computationally can be very expensive 97 Views