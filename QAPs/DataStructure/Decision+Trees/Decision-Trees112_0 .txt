What is the Bonsai Boosted Decision trees technical from LHCb?Why and how was it introduced? What are its strengths and weaknesses? What would be other usages for it?As a member of the LHCb collaboration, I thought I should tackle this, but I should warn you that this topic isn't my expertise. The point of the Bonsai BDT is laid out pretty clearly in the abstract for the paper (preprint available at [1210.6861] Efficient, reliable and fast high-level triggering using a bonsai boosted decision tree): ABSTRACT: High-level triggering is a vital component in many modern particle physics experiments. This paper describes a modification to the standard boosted decision tree (BDT) classifier, the so-called bonsai BDT, that has the following important properties: it is more efficient than tra- ditional cut-based approaches; it is robust against detector instabilities, and it is very fast. Thus, it is fit-for-purpose for the online running conditions faced by any large-scale data acquisition system. The two authors and creators of the algorithm, Vladimir Grigorov and Mark Williams, work on LHCb's trigger team. Naturally I can't come close to explaining the technical details of the algorithm, or summarizing it non-technically, as well as the authors have in their paper, and if I tried I'd just be reading their paper and paraphrasing it. So, this is where you should look for details. Perhaps I can talk about the context that motivated the development of the Bonsai BDT. LHCb is one of the experiments located at the LHC (a little sibling of the big two, CMS and ATLAS). At each experiment's interaction point, the LHC collides bunches of protons every 25ns (more info on the proton collisions here: LHC collisions). Calculating 1/25ns, you'll see that this means that the initial data taking has to take rate at 40MHz. Moreover, the sheer amount of information that's produced at each of these collisions every 25ns would add up to whole lot of data (too much data) to acquire and store. So, that data has to be filtered, and doing it randomly wouldn't be too many steps above the worst possible solution. This necessitates hardware and software triggers to determine whether to save an event that operate extremely quickly. If you're designing a trigger, you are aiming for the highest efficiency you can get, where the exact meaning of "efficient" depends on context but is related to the #interesting events kept/#uninteresting events tossed out vs. #interesting events tossed out/#uninteresting events kept. If the signature of an interesting event was very simple and easily identifiable, this problem wouldn't be such a bottleneck. However, as experiments have probed deeper and deeper into the intricacies of particle physics, what is "interesting" has become much more subtle and harder to pick out. Enter machine learning algorithms such as BDTs--voila! These algorithms are great at being taught to distinguish the interesting from the pass (certainly there's no one definition of interesting, so many work in parallel), and can do a better job of finding needles in the haystack than older methods (what the paper calls cut-based selections, so-called because these algorithm look like: 1) cut out all events with some variable X < 1500[unit], 2) cut out all events with Y > 250[unit], etc.). On of the potential problems with these algorithms is speed, looking back to the 40MHz problem. In particular, boosted decision trees (BDTs) are quite popular, but as the name says they involve a logical tree structure, where at each node a decision is made based on the values of some input variables. The idea of the Bonsai BDT is that if the input variables are discrete (with a finite number of bins), then the number of configurations of input variables is finite, and you can simply store in memory the result of the algorithm for each of those finite cases. So, the problem has been reduced to one-dimensional array look-up, which is much faster than a tree of logical decisions (i.e. huge tree nested if/else-s). The ability to discretize the input variables into a finite number of bins is crucial, since with variables that can take on an infinite number of values, there are in principle an infinite number of possible ways for the algorithm to behave. In theory, such a parameter space could usually be broken up into chunks like a map, where in each region/country the BDT has a certain value, but the boundaries of the regions/countries would be arbitrary surfaces. Nothing would have been simplified, since the algorithm to determine which side of the boundary you are on would just be the original BDT. And as it turns out, the requirement to discretize the input variables isn't a great loss. No continuous quantity measured by an experimental apparatus can be perfectly measured -- there is always experimental uncertainty in the quantities' values. The precision to which the quantities can be measured is the detector resolution. It doesn't make sense too much sense to train the BDTs to treat differently two events with quantity X separated by less than the detector resolution. 413 Views  Answer requested by 1 person