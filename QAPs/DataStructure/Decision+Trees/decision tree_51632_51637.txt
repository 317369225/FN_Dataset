classifier optimization if you need the optimal answer , the best solution i know be exhaustive search : try all $ -lcb- p \ choose 10 -rcb- $ different subset , and see which be best . the run time of this will be $ o -lrb- p ^ -lcb- 10 -rcb- -rrb- $ , though , which be probably too high to be feasible . give this , you will probably need to accept solution that be heuristic or not guarantee to give a absolutely optimal answer . one standard approach be to use a greedy algorithm : you iteratively build up a set of property , one by one . at each step , if you set be currently $ s $ , you choose the property $ p $ that make $ s \ cup \ -lcb- p \ -rcb- $ as accurate as possible , and then add $ p $ to $ s $ . to turn this into a full algorithm , you need to decide how you want to measure\/evaluate each candidate $ s \ cup \ -lcb- p \ -rcb- $ . for comparison , you can look also at <a href="https://en.wikipedia.org/wiki/id3_algorithm" rel="nofollow"> the id3 </a> algorithm . rather than try to pick a set of size 10 , it try to pick a decision tree of depth 10 , so it s not solve exactly the same problem : but it be similar . the metric use at each step to evaluate the candidate be the <a href="https://en.wikipedia.org/wiki/information_gain_in_decision_trees" rel="nofollow"> information gain </a> ; you could do the same , but for a set rather than a tree . in the machine learn literature , there be a lot of work on <a href="https://en.wikipedia.org/wiki/feature_selection#subset_selection" rel="nofollow"> feature selection </a> : give a large number of possible feature , the goal be to pick a subset of the feature that make the classifier as accurate as possible . you could explore that literature and see if any of those method be effective in you domain . 