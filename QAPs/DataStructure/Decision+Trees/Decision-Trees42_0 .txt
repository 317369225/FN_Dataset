What is decision tree?When you construct a decision tree as a model, then basically we are constructing a binary tree, like: Here entire input space is divided in two regions at first node depending on whether x1 is greater than theta1 or not, these regions can then be independently divided further. Likewise at each node we split the previous regions of input space into regions (cuboidal) depending on the thetas. These thetas are the parameters of the model which we learn from training set. Thus, for any new instance of input vector, as I traverse down the tree, I end up in a specific region of input space depending on what path I take (which is determined by the decision criterion at each node, hence model isn’t probabilistic). Now, for classification, all the leaves of the tree (which are a region in input space) correspond to the target classes and for regression, the leaves can simply be constants. This concludes the classification or regression part. Now moving on to learning the tree: By learning, we mean what should be the structure of the tree? This means we have to choose which input variable to look at when splitting at a node and the decision threshold at that place, this needs to be done at each node. Also we need to see how many nodes do we require and the value of predictive variable within each region. Taking one issue at a time: Structure: Even if I know the number of nodes, still to get the best possible structure is computationally consuming task as I would need to look at all possible values of input variable at each node and the threshold values to determine optimal splitting criteria. So, what we do is that we start with a root node which splits entire input space in two and then subsequently add nodes further down the tree. In this greedy approach, one way to go about splitting is to go as follows: Assuming our input vector x is D dimensional, at root we have D choices of input variables to split and need to see corresponding threshold values. Then the choices of split for the next node are D-1 and so on the keep on reducing. So at any node, I can exhaustively search from the remaining variables and their thresholds to see which one gives me optimal choice. This optimality can be in terms of minimizing a cost function when the split occurs and the leaf nodes (sub regions) generated after split. For regression: We minimize the sum of squared error between predictive variable of the target values of the points that fall in that region and the one we assign to it (here the optimal choice of the target variable is the average of target values of all the data points which fall into the region). For classification: We minimize a more intuitive measures like cross entropy and Gini index. By minimizing cross entropy, basically we are trying to reduce the entropy of each region after the split. Now maximum entropy occurs when we get equal proportion of inputs of all classes in our split region, this is what we don’t want. Here, minimizing cross-entropy tries to put all the variables belonging to one class in one region thus providing us with better classification. Then after getting the entropy, we can maximize the information gain. IG = Entropy(parent) - sum(Entropy(children)) The sum is a weighted sum. So we choose the variable which maximizes the information gain in each split and the proportion of the number of instances in each of the children gives us the threshold. Gini impurity is a measure of how often a randomly chosen instance from the children would be incorrectly labelled if we employ the splitting scheme. We wish to minimize Gini impurity when we split, so we choose the splitting criterion which minimizes this quantity. G = sum over all classes(Proportion of instances in class i * (1 - Proportion of instances in class i ) Now the question of how long do we grow the tree? (because we don’t want overfitting) One of the approaches is to keep on growing the tree until the residual error fall below a certain threshold. Another method is to grow a large tree using number of data points in leaf nodes as a stopping criterion and then pruning it. Now how do we define such a pruning criterion? We try to strike a balance between tree complexity and residual error. Tree complexity can be given by the number of leaf nodes |T|. The residual error is basically sum over all the regions of the squared difference between actual target value and the average of target values of variables in that region (each leaf node): Q(t). Hence pruning criterion is a linear combination of these two C = sum over all regions(Q(t)) + k*|T| here k determines the tradeoff between complexity of tree and error. ————————- This sort of sums up the idea of decision trees. For further details use your friend Google and learn more (or message me) 804 Views · View Upvotes