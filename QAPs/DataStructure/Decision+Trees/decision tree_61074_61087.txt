algorithm for guess a word in a dictionary with weighted operator here be a greedy algorithm . it might not be optimal , but i suspect will yield a solution that be good enough . it be basically a instantiation of you thought , but take into account a bit of information theory . the idea will be to greedily choose which operator to query at each iteration ; to tell which operator be best , we ll simulate all possibility for the choice of operator and see which give we the best gain . basically , we re build a decision tree on the fly , via greedy choice at each step . keep a set $ t $ of all possibility for the unknown word . -lrb- initially this be $ s $ , but you ll narrow down the possibility as you go . -rrb- in each iteration : for each possible operator $ f $ you could possibly try , compute the `` information gain '' of use that operator , $ $ i -lrb- f -rrb- = h -lrb- x -rrb- - h -lrb- x f -lrb- x -rrb- -rrb- , $ $ where $ x $ be a random variable that be uniformly distribute on the current set $ t $ . this tell you the expect number of bit of reduction of the entropy -- roughly speak , the reduction in uncertainty about the word . for each possible operator $ f $ , compute the cost of that operator , $ \ text -lcb- cost -rcb- -lrb- f -rrb- $ . also compute the gain-per-unit-cost , $ i -lrb- f -rrb- \/ \ text -lcb- cost -rcb- -lrb- f -rrb- $ . pick the operator $ f $ with the largest value of $ i -lrb- f -rrb- \/ \ text -lcb- cost -rcb- -lrb- f -rrb- $ , and query that operator . use the response to reduce the set of possible word , i.e. , remove from $ t $ each word that be inconsistent with the response you get . repeat until you be leave with only one possibility . notice that there be only $ 26 +50 = 76 $ different possible operator , so it be easy to enumerate over they all in each iteration . this algorithm will be pretty fast . how do you compute the information gain of a operator $ f $ ? well , you know the set $ t $ . also , give any word $ w \ in t $ , you can compute the result $ f -lrb- w -rrb- $ of apply $ f $ to word $ w $ . it s easy to compute the entropy $ h -lrb- x -rrb- $ : that be just $ \ lg t $ . the entropy of a uniform distribution on some set be the base-2 logarithm of the size of that set . to compute $ h -lrb- x f -lrb- x -rrb- -rrb- $ , we partition $ t $ into subset $ t_1 , t_2 , \ dot , t_k $ accord to what the operator return -lrb- all word that return the same value in response to this operator be group into the same subset -rrb- . now $ h -lrb- x f -lrb- x -rrb- = y -rrb- $ be just $ \ lg t_i $ , where $ t_i $ be the subset correspond to word that yield response $ y $ . the conditional entropy $ h -lrb- x f -lrb- x -rrb- -rrb- $ be the average of $ h -lrb- x f -lrb- x -rrb- = y -rrb- $ over all possible value of $ y $ , weight by the probability of get $ y $ . thus , $ $ h -lrb- x f -lrb- x -rrb- -rrb- = \ sum_i -lcb- t_i \ over t -rcb- \ cdot \ lg t_i . $ $ it follow that $ $ i -lrb- f -rrb- = \ lg t - \ sum_i -lcb- t_i \ over t -rcb- \ cdot \ lg t_i . $ $ the algorithm above doesn t guarantee to output the optimal solution . what if we want the optimal sequence of query ? well , basically this correspond to build a decision tree , where each node of the tree apply one operator and then branch upon the result . we can evaluate the `` cost '' of a decision tree accord to the expect cost spend by the decision tree to narrow down the word to a single possibility , with the expectation take over the random choice of the unknown word . give a decision tree , it be easy to compute its expect cost . for each of the 100 word , you can simulate what path it will take through the decision tree and the total cost before it hit a leaf ; average those 100 cost yield the expect cost . you could now enumerate over all decision tree , and take the one whose expect cost be minimal . of course , this will be highly inefficient , as there be exponentially many tree , but it do find the optimal solution . it should be possible to make this more efficient use branch-and-bound . explore the space of decision tree , by build they up in a top-down fashion -lrb- from the root downward -rrb- . at any point , you have a partial decision tree -lrb- the top of some decision tree , with bottom part yet to be fill in -rrb- . you can get a upper bind for the cost of the best way to complete a partial decision tree to a full decision tree , by apply the greedy heuristic above to complete it to a tree and see what the result cost . also , you can get a lower bind as follow . suppose we have any set $ u $ of word . find the letterx such that the number $ b $ of different possible result of getoccurrence -lrb- x -rrb- -lsb- take over all word of $ u $ -rsb- be as large as possible . then the cost of any decision tree for $ u $ will necessarily be at least $ \ lceil \ log_b u \ rceil $ . for each bottom node in a partial decision tree , find the set $ u $ of word that reach that node , compute a lower bind on the cost of the best decision tree to put as a subtree of that node , and average these up over all the bottom node of the partial decision tree -lrb- together with the cost of reach that node -rrb- . now use these lower and upper bound to prune you exploration , follow standard branch-and-bound technique : if the lower bind for a partial decision tree be at least as large as the expect cost of the best complete decision tree see so far , then you can prune that partial decision tree -lrb- there be no need to explore any of the candidate way to complete it to a complete decision tree -rrb- . whether this algorithm will be efficient enough to use , i don t know . you could implement it and give it a try . 