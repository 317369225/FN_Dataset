What are the advantages of logistic regression over decision trees?Are there any cases where it's better to use logistic regression instead of decision trees?With logistic regression it's also important to consider regularization. With very high dimensional (and possibly sparse) features, L1 or L2 regularization is critical to avoid over fitting. L1 regularization leads to a sparse model which has been shown to perform as well as an SVM on some text processing tasks. Straying off topic here, but as far as equivalence goes, kernel methods like the SVM can't always be realized with a traditional logistic regression. Some kernel functions like the radial basis kernels lead to an infinite expansion of the original feature space. Polynomial kernel functions are finite but will lead to practical problems very easily. Having said all that, logistic regression really shine when you want to analyze things like the relative importance of each feature. The "glmnet" package in R demonstrates this very well. For anyone who's interested, it's really worth while looking at Hastie and Tibshirani work on regularization paths. Lastly, logistic regressions can optimize the multi-class (multinomial) problems directly. With some techniques like an SVM you have to use something like "one-vs-rest" training to make a multi-class classifier which may be less optimal. 9.6k Views  View Upvotes