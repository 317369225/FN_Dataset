Is taking most common features selected by decision trees while using bagged or random forests a good feature selection method?One of the hidden job of decision trees is to choose relevant features for the task of classification. Basically a C4.5 decision tree is constructed based on information gain (difference in entropy) [1] of attributes; therefore, you are indirectly performing feature selection. Similarly, for RF you can use the selected features for each ensemble. However, this is one of the many techniques available for feature selection and may or may not work well for different datasets. For more details on different feature selection techniques read this famous JMLR paper - http://www.jmlr.org/papers/volum... Footnotes [1] C4.5 algorithm 254 Views