How many methods can be followed to built decision tree?Most decision tree algorithms have the same basic structure: take the data, find a split based on an attribute that maximizes some purity measure, take the two halves of the data created by the split, and recurse until some stopping criteria is reached. Then prune back to reduce overfitting. So along the way, there are many different decisions for each of the components: Finding a split: Methods here vary from exhaustive search (e.g. C4.5) to randomly selecting attributes and split points (e.g. Random forests). Purity measure: Measures here include: information gain, gain ratio, Gini coefficient, minimum description length and Chi-squared values. Stopping criteria: Methods here vary from a minimum size, to a particular confidence in prediction, to certain purity criteria. Pruning method: Methods here include no pruning, reduced-error pruning, and in ensemble cases such as bagging, out-of-bag error pruning. These four different components can mixed and matched pretty much, so there are many many methods for building a decision tree. It does not include certain "fancy" options that have been considered but don't seem to really help, e.g. lookahead in the split selection and/or backtracking as the tree is being built. 768 Views  View Upvotes