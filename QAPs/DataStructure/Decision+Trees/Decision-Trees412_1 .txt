Why do people use gradient boosted decision trees to do feature transform?In Facebook's ADKDD 2014 paper, they use gbdt to do feature transformation before feeding features into the linear regression classifier. In the paper, they say this has something to do with the nonlinearity of the original feature space, but no details. Can someone elaborate on this ?Facebook's paper gives empirical results which show that stacking a logistic regression (LR) on top of gradient boosted decision trees (GBDT) beats just directly using the GBDT on their dataset. Let me try to provide some intuition on why that might be happening. First, let's assume that GBDT had learnt the weight of the tree T  T to be W T   WT . Let's also assume that the output of a single tree's leaf l  l (say determined by averaging all training observations in that leaf) is P l   Pl . Also, let's assume l(x)  l(x) to be an indicator variable which is 1  1 iff observation x  x falls into region identified by leaf l  l . We can then write: GBDT(x)=  GBDT(x)= ∑ T W T ∗(∑ l∈ T  P l ∗l(x))  ∑TWT∗(∑l∈TPl∗l(x)) If we denote by T(l)  T(l) the tree to which leaf l  l belongs to, we can rewrite it as: GBDT(x)=∑ l W T(l) ∗P l ∗l(x)  GBDT(x)=∑lWT(l)∗Pl∗l(x) If we use W l   Wl to denote W T(l) ∗P l   WT(l)∗Pl , we can rewrite the previous equation as: GBDT(x)=∑ l W l ∗l(x)  GBDT(x)=∑lWl∗l(x) Also, by construction, a LR stacked on GBDT has following hypothesis: Stacked(x)=∑ l C l ∗l(x)  Stacked(x)=∑lCl∗l(x) So both of these are doing a linear weighted sum on binary features derived based on inputs belonging to various leaves. So why is LR + GBDT better than just using GBDT alone? LR is a convex optimization problem so it can actually choose the weights C l   Cl of each feature optimally. In GBDT on the other hand, weights W l   Wl aren't guaranteed to be optimal. In fact, since W l =W T(l) ∗P l   Wl=WT(l)∗Pl and W T(l)   WT(l) are chosen greedily (e.g using line search) and P l   Pl are chosen based only on the local neighborhood, it's quite conceivable that W l   Wl aren't globally optimal after all.  So overall, stacked LR + GBDT will never perform worse than directly using GBDT and can actually outperform it for certain datasets. But there is another really important advantage of using the stacked model. Facebook's paper also shows that data freshness is a very important factor for them and that prediction accuracy degrades as the delay between training and test set increases. It's unrealistic/costly to do online training of GBDT whereas online LR is much easier to do. By stacking an online LR over periodically computed batch GBDT, they are able to retain much of the benefits of online training in a pretty cheap way. Overall, it's a pretty clever idea that works really well in real-world. 4.3k Views · View Upvotes