How can we say there is "an interaction" between variables in decision tree?In contrast to the typical linear models, you don't make any assumptions about the independence of features. Let's say we have variables that are highly correlated or "explain the same thing." If you have high collinearity among e.g,. 2 variables, the decision tree would automatically pick the "better" one for the split. Depending on what you are after, this can either be a good or a bad thing. For instance, this is something you have to keep in mind when you are using tree-based methods to assess feature importance. As far as I know, typical (CART) decision tree implementation don't have any backtracking. At each split, the algorithm evaluates each feature individually to select the one that maximizes information gain (/ reduces impurity in the child nodes). This could be a problem if you have a scenario where splitting on feature A works great, splitting on feature B is slightly worse, but splitting on B and then on C would be better than splitting on A. Btw. I achieved quite good results in a similar scenario using sequential floating feature selection algorithms to select a feature subset of features that works well in their particular combination. Sequential feature selection is basically just a way to reduce the combinatorial search space of an exhaustive subset search. I have an implementation here if you are interested http://rasbt.github.io/mlxtend/docs/feature_selection/sequential_floating_backward_selection/ (The documentation is quite rudimentary, but I am planning to update it some time soon) 663 Views  View Upvotes