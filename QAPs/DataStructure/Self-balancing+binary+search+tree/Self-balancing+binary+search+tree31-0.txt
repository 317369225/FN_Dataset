What are some ways to manage keys efficiently in attribute-based encryption with the increase in the number of users and the subsequent increase in the attribute size?
Best conjunct yo'self fo you wreckity wreck your logical truth Get ready to bust out your predicate calculus because this logic bomb's about to get real. Also, much like you should "grab your glocks when you see Tupac", you should probably also get ready to grab your Big-O notation and familiarity with computational complexity. First let's do a quick refresher on what's Attribute-Based Encryption (or ABE). ABE is a very recent form of public key encryption where the ability to encrypt/decrypt a file is based on a universe of attributes that a set of users may or may not have. ABE is extremely valuable for segmenting different sets of information such that not every user within an organization has access to all information simply because they're part of a group. It's a great way of implementing Multi-Level Security (or MLS), with applications in military organizations (not allowing everyone with Top Secret or SCI clearance to know how to build a nuclear weapon), finance firms (installing a Chinese wall between different investment groups within a large financial services org such as an investment bank to stop insider trading), and healthcare (making sure only doctors treating certain patients have access only to their information). For example: let's say you have a universe of the below attributes: {A, B, C, D} ABE encrypts a file such that one requires a certain set of logical operations to unlock a file. A file may be encrypted with the following key: k = (A^B) V C This means that only a user who holds A and B attributes OR holds a C attribute can decrypt that file. What does this really mean? Let's pretend we're talking about an investment bank who's currently working on IPO'ing a big startup. For all sensitive information related to this project, you want to make sure that only two groups of people have access: Someone who is of analyst level or higher (A) who is on the investment banking team (B) A compliance analyst from the SEC who is overseeing this process (C). ABE has great applications in multi-level security situations where you need to segment information within the same strata of employee level. A great application in the private sector is in using encryption to enforce a Chinese Wall within a financial services firm to stop insider trading. The key above is such that you need to be a minimum analyst level AND are on the investment banking team to view the IPO documents (A ^ B), OR you're from the SEC (C) and reviewing the process for compliance with various regulations. You can't be an analyst+ in another group and see the documents, but the documents are still transparent to this outside and necessary observer. Solid. So now we understand ABE, let's talk about how we can design a key management system for it scaleably. First of all the cardinal rule of key management is assume you're going to have a lot of people trying to hack into you. Nobody really does cryptanalysis and tries to directly break encryption today unless they're working at a military/national intel organization. People are going to be slamming your key management server and protocol instead, and try to get around the encryption (i.e.: a side-channel attack). Whatever you do, don't sacrifice security by cheating on performance. Binary in Navajo is still binary. This means that whatever we do, we have to make sure it's secure. In crypto the gold standard of security is Kerckhoff's Principle (see: https://en.wikipedia.org/wiki/Ke...), which basically equates to the phrase "there is no security in obscurity." Your system cannot rely on hiding aspects of its process to exchange keys and store them to be secure. Expect your attacker to know everything. So now that we remember not to hide any flaws to cheat on performance, let's focus on a few things regarding efficiency at scale. The real world performance of your key will rely on how you scale the speed to search and serve an increasingly-large set of attributes as well as the speed to decrypt/encrypt with a progressively larger set of attributes. If we're talking about key management, we honestly don't need to worry about the latter. That's something that we can push to the algorithm itself. But when we increase the number of attributes, we need to accept the reality that the speed to search and serve more attributes from a universe gets harder. For the purposes of simplicity, let's assume that the protocol we have to send these attributes to the user is scaleable for the size of data. If this is the case, this leaves us with one big area to focus on: The data structure holding the universe of keys. This is very important because as we increase the size of the universe, we need to ensure that the time to find a specific attribute remains as close to constant as possible. For most data structures this is not the case. In the case of something like a singular-direction Linked List, searching for a newly added element to that data structure requires you to search the entire set to the end. So how do we deal with the situation where I'm constantly adding new attributes but I need to scale very quickly? Well let's go put on our computer scientist hats and remember our favorite multicolored best friend: the Red-Black Tree (or simply RB Tree). All hail the Red Black Tree, the data structure which both giveth and taketh away for most undergrad computer science students (mostly taketh if you're trying to implement one during a pset at midnight) The RB Tree is a data structure where the distribution of elements within the structure is ordered such that searching for any element in the tree has a O(log N) running time. The tree is dynamic, and you can add or subtract elements in the tree with a pretty high algorithmic performance of O(log N) as well. Using a RB Tree (or really any well designed self-balancing binary search tree) helps you dramatically improve the worst case performance of searching for and assembling newly added attributes to a universe. But if you play with some math and statistics, you can also use how you instrument that RB Tree's balancing mechanisms to improve the potential upper bound of search - the Î˜-Complexity of the algorithm as well. If you integrate frequency of use into how you distribute attributes across your ordered data structure, your KMS can go from Fast and Furious 1 to Tokyo Drift speeds. For example: let's say you know that a certain attribute is used more frequently than any other attribute, by a factor M-number of standard deviations. If we factor frequency into deciding whether an attribute should be stored to the "left" or to the "right" on the tree, we can dramatically increase the best case running time of searching for + assembling a set of attributes for a key. There's still a lot you need to worry about regarding security and efficiency of the protocol for acquiring keys and performing cryptological operations. But with some time spent focusing on the data structure you're going to hold the universe of attributes within, you can design a very efficient means of storing keys in a KMS. 