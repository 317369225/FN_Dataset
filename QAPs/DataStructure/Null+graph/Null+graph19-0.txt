Is my teacher correct in saying that a significance test for correlation measures the number of points in the graph that are close to the line of best fit?
I think that your teacher misinterpreted the t-statistic. The equation for the t-statistic is in the same format as for any other t-statistic: t = (observed - expected)/standard error. In this case, observed is r, expected is [math]\rho = 0[/math] (in this case, it looks like the hypothesis is that there is no correlation, meaning [math]\rho = 0[/math]), and the standard error is [math]\sqrt{\frac{1-r^2}{n-2}}[/math]. Let's say you want a level of significance of [math]\alpha = 0.05[/math]. The degrees of freedom are [math]df = n-2[/math]. Now just look up the significance level and [math]df = n-2[/math] into a table of t values to get the critical value. So let's say you had df=10 and a significance of 0.05. Look this up in the table and get t=1.812. So if your t that you solved for was greater than 1.812, then you would reject the null hypothesis, and otherwise you fail to reject. 