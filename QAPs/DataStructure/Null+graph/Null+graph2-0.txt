How does 'grep' preprocess a text file for efficient regular expression matching?
I don't think grep does any preprocessing of the input file; it certainly doesn't have to, regular expression matching is pretty efficient as it is. Instead of sifting through all the world's different grep implementations looking for evidence of preprocessing that probably isn't there, though, I will argue this with a small experiment for illustration, because I think it's more fun than just saying that expression matching works in linear time. We can try matching a regular expression without grep, so just grab a copy of the collected works of Shakespeare: # I'm going to do everything in a somewhat UNIX-y way throughout% wget http://www.gutenberg.org/ebooks/100.txt.utf-8 According to Words Shakespeare Invented, The Bard came up with the words "lonely" and "lower", so let us match the regular expression 'lo(nely|wer)', and find out how often they occur in there. Regular expressions have a correspondence with deterministic finite automata, so we can draw the expression as a graph of its automaton: The idea is that you start in a numbered state (1), read a character, move to the next state if an arc is labelled with what you read, and repeat the procedure. Implicitly, there should be a bunch of arcs from states 2-8 going back to the start state (labelled with "anything else"), but we can leave those out of the figure for the sake of clarity, and remember them. Similarly, let's quietly add a state 0 with an arc from 0 to 1 that is taken when there's a blank space, so that we actually make it the regular expression ' lo(nely|wer)'. The reason to do so is that 'lower' appears in 'flower', 'follower', and other places, so putting a space in is to narrow the search, and not drawing it in the figure is because I have a hard time making an obvious drawing of a space character. It's a slightly broken way to get space-separated words, but it's simple, and you can surely work out the changes in the graph to fix it up how you like anyway. A nice thing about graphs is that they can be written as tables, so if we take all the 256 values a character can have as columns, and make a row for each of our 10 states, the arcs can be written as the destination state in table entry (state, character). The algorithm for matching text, then, becomes a matter of storing which state we're in, reading a character, and changing state according to the table. Here's our regular expression in C, I've hard-coded the table for the expression, since this is just an illustration: #include <stdio.h>#include <string.h>char table[10][256];voidsetup_table ( void ){    /* Start with everything has arcs to state 0 */    memset ( table, 0, 10*256 );    /* Add the arcs from our automaton */    table[0][' '] = 1;    table[1]['l'] = 2;    table[2]['o'] = 3;    table[3]['n'] = 4;    table[3]['w'] = 7;    table[4]['e'] = 5;    table[5]['l'] = 6;    table[6]['y'] = 9;    table[7]['e'] = 8;    table[8]['r'] = 9;}intmain ( ){    int state = 0, character;    setup_table();    while ( !feof(stdin) )    {        /* Match text */        character = getchar();        state = table[state][character];        /* Recognize if we've reached a complete match */        if ( state == 9 )        {            printf ( "Hooray, we found one!\n" );            state = 0;        }    }} Now, we run Shakespeare through that and count how many lines of hoorays we get: % make mygrepcc     mygrep.c   -o mygrep% cat 100.txt.utf-8 | time ./mygrep | wc -l0.54user 0.01system 0:00.61elapsed 90%CPU (0avgtext+0avgdata 372maxresident)k0inputs+0outputs (0major+117minor)pagefaults 0swaps24 So, that's 24 hits in around half a second for a 5.4MB file, at least on the puny netbook I'm writing this on. You can verify that 24 is correct using an ordinary grep with an expression in the appropriate syntax of choice. Next, we can break this half-second down into how much trouble it is to read characters, and how much trouble it is to match expressions, by loading the entire text into memory in one sweep, and matching the expression in another: #include <stdio.h>#include <string.h>/* We'll need clocks */#include <sys/time.h>#define WALLTIME(t) ((double)(t).tv_sec + 1e-6 * (double)(t).tv_usec)/* The file is less than 6MB, so that will be more than enough for us */int shakespeare[6*1024*1024];char table[10][256];/* The definition of setup_table is the same as before, so it's omitted, * copy it from the previous source if you want to run this. */intmain ( ){    int state = 0, character;    setup_table();    size_t i = 0;    struct timeval t0, t1, t2;    /* Read the whole thing */    gettimeofday ( &t0, NULL );    while ( !feof(stdin) )    {        shakespeare[i] = getchar();        i = i + 1;    }    shakespeare[i] = EOF;    gettimeofday ( &t1, NULL );    i = 0;    while ( shakespeare[i] != EOF )    {        /* Match text against the loaded array */        state = table[state][shakespeare[i]];        i = i + 1;        /* Recognize if we've reached a complete match */        if ( state == 9 )        {            printf ( "Hooray, we found one!\n" );            state = 0;        }    }    gettimeofday ( &t2, NULL );    fprintf ( stderr, "%lf seconds of reading, %lf seconds of matching\n",        WALLTIME(t1)-WALLTIME(t0), WALLTIME(t2)-WALLTIME(t1)    );} Running this on the same machine gets us % make mygrep2cc     mygrep2.c   -o mygrep2% cat 100.txt.utf-8 | ./mygrep2 | wc -l0.597790 seconds of reading, 0.073611 seconds of matching24 As you can see, even this completely unsophisticated little proto-grep could do a full-text match of more than 8 expressions in the time it takes just to get the characters in the text from disk. It has nothing to win from preprocessing, because any preprocessing would take as much time as doing the matching right away, the preprocessor would also have to read the file. All that really differentiates this tiny experiment from a full implementation, is that a proper grep constructs its table from the provided regular expression, and should probably use a more compressed format for the table, since this one grows as the number of states times the size of the possible input alphabet. The procedure for finding matches is straightforward, and its performance is dominated by the cost of fetching all the text it searches through. There would be advantages to using preprocessing and a more elaborate data structure for a search tool that's expected to do a lot of searching in a large, slowly changing data set, but I don't think grep would be useful in the way it is if it relied on a periodically updated index database, or something. Updated 112w ago • View Upvotes • Asked to answer by Satya Nand and Vaibhav Tulsyan