How are regular expressions implemented?
This is a tricky question because there are actually two different things called "regular expressions". They both match text but have different amounts of power and can be implemented in different ways. There are "regular expressions" in the more academic sense that can match regular languages and "regexps" which can match significantly more complex languages. If you've studied the subject in school, you're probably familiar with the former. They're also implemented in a handful of tools like Google's re2 library and lexical analyzers like Lex. On the other hand, if you're familiar with regexps from languages like Perl, PHP, Python, JavaScript, Java or pretty much anything else mainstream, you've seen the more powerful version. A lot of these rely on a particular style called PCRE - Perl Compatible Regular Expressions. For simplicity, I'm going to refer to the academic ones as "regular expressions" and the more powerful Perl-style¹ ones as "regexps". Regular ExpressionsActual regular expressions are actually pretty limited. However, their efficient implementation is actually much more interesting than the implementation of the more powerful version! Regular expressions match strings over some alphabet [math]\Sigma[/math]. A string is just a finite, possibly empty ordered list of elements from [math]\Sigma[/math]; the set of these strings is denoted [math]\Sigma^*[/math]. Each expression [math]r[/math] defines a subset of [math]\Sigma^*[/math]: in a slight abuse of notation, [math]r \subseteq \Sigma^*[/math]. In general, a set of strings is known as a language, and the sets which can be defined using a regular expression in particular are known as regular languages. The possible regular expressions for [math]\Sigma[/math] are defined inductively. There is always the null expression [math]\emptyset[/math] which doesn't match anything at all and [math]\epsilon[/math] which only match the empty string. Thinking of them as sets, [math]\emptyset \equiv \{\}[/math] and [math]\epsilon \equiv \{\langle\rangle\}[/math]. For each symbol [math]\sigma \in \Sigma[/math], we also have the expression [math]\sigma[/math] which matches the string containing just a single [math]\sigma[/math]: [math]\sigma \equiv \{\langle\sigma\rangle\}[/math]. On top of these base cases, we can also combine existing regular expressions to form more complex ones. The easiest method is concatentation: given regular expressions [math]r[/math] and [math]s[/math], [math]rs[/math] matches [math]r[/math] followed by [math]s[/math]. So if [math]r \equiv \{\langle ab \rangle, \langle cd \rangle\}[/math] and [math]s \equiv \{\langle ef \rangle\}[/math] then [math]rs \equiv \{\langle abef \rangle, \langle cdef \rangle\}[/math]. Another operation is alternation which is just set union. Given [math]r[/math] and [math]s[/math], [math]r|s[/math] matches if either [math]r[/math] or [math]s[/math] match. Given the same [math]r[/math] and [math]s[/math] as above, [math]r|s \equiv \{\langle ab \rangle, \langle cd \rangle\} \cup \{\langle ef \rangle\} = \{\langle ab \rangle, \langle cd \rangle, \langle ef \rangle\}[/math]. The last operation is the Kleene star which is repetition. Given an expression [math]r[/math], [math]r*[/math] matches the empty string ([math]\epsilon[/math]), [math]r[/math], [math]rr[/math] and so on. This is also where the [math]\Sigma^*[/math] notation comes from for the set of strings! Given the [math]r[/math] above, [math]r* \equiv \{\langle \rangle, \langle ab \rangle, \langle cd \rangle, \langle abab\rangle, \langle abcd\rangle, \langle cdcd \rangle, \ldots \}[/math]. Note how concatentation and alternation have a nice algebraic structure. [math]\epsilon[/math] is the identity for concatenation: [math]\forall r. \epsilon r \equiv r\epsilon \equiv r[/math]. Similarly, [math]\emptyset[/math] is the identity for alternation: [math]\forall r. \emptyset|r \equiv r|\emptyset \equiv r[/math]. And [math]\emptyset[/math] serves as a "zero" for concatenation: [math]\forall r. r\emptyset \equiv \emptyset r \equiv \emptyset[/math]. Since there aren't any inverses, this forms a semiring with alternation as [math]+[/math], concatenation as [math]\cdot[/math], [math]\emptyset[/math] as [math]0[/math] and [math]\epsilon[/math] as [math]1[/math]. If we throw in the Kleene star (also called a Kleene closure), we get a "closed semiring" which turns out to be a surprisingly versatile structure. (See "Fun with Semirings" for more details.) Often, actual regular expressions have some additional operations like [math]+[/math] for matching at least one instance of an expression. But these can always be desugared into the operations above; for example, [math]r+[/math] can always be rewritten as [math]rr*[/math]. Compiling to an NFA We can implement a regular expression efficiently by compiling it into a finite state machine. I find it easier to think about nondeterministic finite automata (NFAs) for this. An NFA has a set of states and a set of transition functions between them; however, each transition function can lead to any number of states (ie the transition function is nondeterministic). For simplicity, we'll say an NFA always has a start state (labeled 0) and can have any number of accept states (drawn as double circles). The NFA "accepts" a string if it is in an accept state after going through the whole string. The transition function in an NFA takes an input symbol from [math]\Sigma[/math] and the current state to figure out the new states. Since there can be multiple possible current states, we apply the transition function to all of the to figure out the new set of states. If there are no new states—the transition function returned no new states for every current one—the NFA rejects the symbol. We can translate a regular expression to an NFA inductively, just following the same rules as I defined for regular expressions above. [math]\emptyset[/math] has a start state but no accept state. [math]\epsilon[/math] just has a start which is an accept state, with no transitions. The expression [math]\sigma[/math] (one letter from the alphabet) has a transition labeled with [math]\sigma[/math] from the start state to an accept state: The interesting logic comes in the combination rules. Concatenation works by linking up accept states. To make this easier, we can introduce "ε" edges, which are followed without consuming an input character: think of them like symbolic links in the filesystem :). So to get [math]rs[/math], we get the automata for [math]r[/math] and [math]s[/math], turn all of [math]r[/math]'s accept states into normal states and add an ε edge from those states to [math]s[/math]'s start state. Here's what the concatenation of "ab(c|d)" and "e(f|g)" looks like: We could actually have 3 and 4 as well as 7 and 8 as single states, but I figure this illustrates the idea a bit better. This also gives us a hint on how to do alternation: just have two edges from one state! So if we want to do [math]r|s[/math] for the same [math]r[/math] and [math]s[/math] as above, we could create a new start state and have an ε edge from that to the start states of [math]r[/math] and [math]s[/math]: I was too lazy to renumber the nodes, so S is the start state rather than 0 As a side note, if you're familiar with the pumping lemma, this is where the idea comes from: to match a string longer than the number of states, we have to have a loop in the graph of our NFA. Finally, the Kleene star can be accomplished by adding an ε edge between each accept state and the start state of [math]r[/math]. This one came out a bit wonky, but I don't feel like wrestling with GraphViz too much :). Implementing NFAs Now that we have an NFA, how do we implement it? An NFA is made up of a bunch of states [math]S[/math] and the transitions between them [math]f : S \times \Sigma \to \mathcal{P}(S)[/math]. We could just do this directly by maintaining a set of states. Each time we transition, we map our transition function over every state in the set and then flatten the set to get rid of duplicates. This is exactly what Scala's flatMap method on sets does, coincidentally. For more efficiency, we could represent the set of states as a bitvector. Since we only ever care about whether we are currently at a given state or not, we only really need one bit per state. In fact, if you sat down and worked it out on paper, I'm sure you could also implement the transition function largely through bit-twiddling. The important thing to note is that, once we have our NFA, we only need to look at each character of our input once. This means we can make matching a streaming operation, and it also means that it's *very* fast! RegexpsRegular languages have a lot of structure, which is what lets us build up an NFA and implement it efficiently. However, the regexps you are used to in most languages are actually more powerful than this, so we can't use this method. As an extreme example, we can actually write a Perl style regexp that only matches non-prime numbers (written in unary): 1|(11+?)\1+ This exhibits one of the features that make regexps more powerful: backreferences. You can group part of the expression and then match only on the same thing as the group. That is, (a*)\1 matches any number of as followed by the same number of a's. This allows you to match languages that are not regular, like (a*)(b*)\1\2. (By the pumping lemma—I won't explain it here because many find it difficult and it distracts from the main topic.) This means that you can't implement these more general regexps as an NFA. Instead, you have to rely on backtracking. That is, you try to match as much of the string as you can but if you get stuck you have to go back and try a different way to match it. This can be illustrated by some pathological cases that cause common regexp engines to take a long time—exponential time, in fact! Try this in JavaScript, for example: /(a+a+)+b/.test("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"). And that's just 30 "a"s; imagine this on a large document! Of course, a bit of cleverness can overcome particular corner-cases like this one. In fact, on my system, Chrome takes a while to execute the above code but FireFox returns immediately. But, fundamentally, you can't excise all the corner cases—with a bit more thought, I could come up with an example that makes FF freeze up too. Regular-Expressions.info, which is a great source about regexps in general, has a good page detailing problems like this, where I got this particular example. So Perl-style regexps are more powerful and allow you to match more complex patterns but are also slower with some pathological corner cases that take exponential time. This is why Google designed its re2 library using the automata approach I wrote about above: it prevented Code Search from being spammed by slow regexps. footnotes ¹ There are actually other styles of more powerful regexps, like the ones used by GNU tools like grep or Emacs. In particular, there are POSIX-style regexps in two versions: BRE and ERE. While they are syntactically different from Perl-style regexps, they are similar in power and implementation, so I've just ignored them. At least in grep, you can enable full Perl style regexps using the -P flag. And if you're an Emacs user, well, you just learn to live with it :P. Updated 47w ago • View Upvotes