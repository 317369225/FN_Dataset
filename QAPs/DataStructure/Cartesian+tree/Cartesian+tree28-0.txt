What is the most efficient way to store large amount of GIS data?
Managing a geospatial working set that grows by 26M records per day is not trivial, particularly if you plan on doing much beyond simple point search and retrieval. For that many geospatial records, you will want a distributed index. If you are only ever using or generating point-like geometries then a distributed quad-tree works great. If you are using more complex geometries (like paths or polygons) then a distributed grid file is a decent index at your scale. These are simple indexes to build on top of something like MySQL as a storage engine and they can be used to ask complex spatial relationship questions. This is how most people with a lot of GIS data actually do it. Just to put it out there, none of the three databases/storage engines you mention are good for GIS indexing using their built-in functionality, certainly not at anything but small scales. MongoDB's GIS implementation is weak (and in our testing quite buggy). As far as I know, Redis does not have a GIS implementation though you could implement the aforementioned index structures quite easily if you are only doing Cartesian geometry. The only competent GIS implementation in the open source database world I am aware of is PostgreSQL/PostGIS but that implementation is not scalable; you would still have to implement a simple distributed index on top of it. PostGIS may be overkill if you are not trying to do complex analysis or anything like that, in which case MySQL is an adequate fallback. Your data set is large enough that architecting a GIS database starts to become a significant chore with few good off-the-shelf solutions that do not cost a lot of money. If you keep the use cases simple enough it is definitely manageable with a little bit of distributed indexing glue on top of conventional database engines. 