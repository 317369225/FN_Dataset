Is this a valid way of proving that [math]\displaystyle\int^b_{a}xdx=\frac{b^2-a^2}{2}[/math]?
Of course this is easy to do using various non-trivial properties of the integral. In your case, you're assuming that it's already been proven that you can split the integral up like you did and also that an indefinite integral on an interval of the form [math](0,a)[/math] is an antiderivative. The latter is a special case the fundamental theorem of calculus and is stronger than the result you're trying to prove. That's perfectly fine for basic exercises on how to compute integrals but if you're being asked to prove this from first principles then it's very circular. Doing this directly is not very hard. Say that [math]\delta x = (b-a)/n[/math] for some integer [math]n[/math]. For [math]0 \leq i < n[/math] set [math]x_i = a + i(b-a)/n[/math]. Consider the limit [math]\lim_{n\rightarrow \infty} \sum f(x_i) \delta x = \lim_{n\rightarrow \infty} \sum (a + i(b-a)/n) (b-a)/n[/math]. It's fairly easy to see that the sum is simply [math](-na^2-a^2+2 a b+b^2 n-b^2)/(2 n)[/math]. And the limit of this quantity as [math]n[/math] tends to infinity is [math](b^2-a^2)/2[/math]. 