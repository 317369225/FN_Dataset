Is data compression algorithms succeeded in working with large data?I do not think, a file size of 1 terabyte compressed to the size of 500 gigabytes same large size. Compress large size and the resulting large size as well, So keep data compression algorithms in the challengedLossless compression is affected by the joint entropy of the source data, so it all depends on the nature of the “large data.” If it is “noise-like” it is not going to compress very well. If it is “regular” it will compress better. Arguing that if a 1 TB file compresses to 500 GB is “bad” (because it is still “big”) is meaningless. A 2:1 compression is actually very typical in lossless coding. 30 Views · Answer requested by Omran Frhat