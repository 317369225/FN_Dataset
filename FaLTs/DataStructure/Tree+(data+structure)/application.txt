Representing hierarchical data Storing data in a way that makes it easily searchable (see binary search tree and tree traversal) Representing sorted lists of data As a workflow for compositing digital images for visual effects Routing algorithms
Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in physical, biological[1] and social systems. Many problems of practical interest can be represented by graphs.
In computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. One practical example: The link structure of a website could be represented by a directed graph. The vertices are the web pages available at the website and a directed edge from page A to page B exists if and only if A contains a link to B. A similar approach can be taken to problems in travel, biology, computer chip design, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. There, the transformation of graphs is often formalized and represented by graph rewrite systems. They are either directly used or properties of the rewrite systems (e.g. confluence) are studied. Complementary to graph transformation systems focussing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.
Graph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the Principle of Compositionality, modeled in a hierarchical graph. More contemporary approaches such as Head-driven phrase structure grammar (HPSG) model syntactic constructions via the unification of typed feature structures, which are directed acyclic graphs. Within lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still other methods in phonology (e.g. Optimality Theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs, as well as various 'Net' projects, such as WordNet, VerbNet, and others.
Graph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. For example, Franzblau's shortest-path (SP) rings. In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such systems.
Graph theory is also widely used in sociology as a way, for example, to measure actors' prestige or to explore diffusion mechanisms, notably through the use of social network analysis software.
Likewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or habitats) and the edges represent migration paths, or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.
In mathematics, graphs are useful in geometry and certain parts of topology, e.g. Knot Theory. Algebraic graph theory has close links with group theory.
A graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example if a graph represents a road network, the weights could represent the length of each road.
A digraph with weighted edges in the context of graph theory is called a network. Network analysis have many practical applications, for example, to model and analyze traffic networks. Applications of network analysis split broadly into three categories:
First, analysis to determine structural properties of a network, such as the distribution of vertex degrees and the diameter of the graph. A vast number of graph measures exist, and the production of useful ones for various domains remains an active area of research. Second, analysis to find a measurable quantity within the network, for example, for a transportation network, the level of vehicular flow within any portion of it. Third, analysis of dynamical properties of networks.
The B-tree uses all of the ideas described above. In particular, a B-tree:
keeps keys in sorted order for sequential traversing uses a hierarchical index to minimize the number of disk reads uses partially full blocks to speed insertions and deletions keeps the index balanced with an elegant recursive algorithm
In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.
Discrete event simulation Priority queues
Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. Currently the main use of hash trees is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Suggestions have been made to use hash trees in trusted computing systems. Sun Microsystems has used Hash Trees in the ZFS filesystem.[1] Hash Trees are used in Google Wave protocol,[2] Git distributed revision control system, the Tahoe-LAFS backup system, the Bitcoin peer-to-peer network, and a number of NoSQL systems like Apache Cassandra & Riak.[3]
Hash trees were invented in 1979 by Ralph Merkle.[4] The original purpose was to make it possible to efficiently handle many Lamport one-time signatures. Lamport signatures are believed to still be secure in the event that quantum computers become reality. Unfortunately each Lamport key can only be used to sign a single message. But combined with hash trees they can be used for many messages and then become a fairly efficient digital signature scheme.
The heap data structure has many applications.
Heapsort: One of the best sorting methods being in-place and with no quadratic worst-case scenarios. Selection algorithms: Finding the min, max, both the min and max, median, or even the k-th largest element can be done in linear time (often constant time) using heaps.[7] Graph algorithms: By using heaps as internal traversal data structures, run time will be reduced by polynomial order. Examples of such problems are Prim's minimal spanning tree algorithm and Dijkstra's shortest path problem.
Full and almost full binary heaps may be represented in a very space-efficient way using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position n would be at positions 2n and 2n+1 in a one-based array, or 2n+1 and 2n+2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by swapping elements which are out of order. As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.
ext2 Htree indexes were originally implemented for ext2 but the patch never made it to the official branch. The dir_index feature can be enabled when creating an ext2 filesystem, but the ext2 code won't act on it. ext3 Htree indexes are available in ext3 when the dir_index feature is enabled. ext4 Htree indexes are turned on by default in ext4. This feature is implemented in Linux kernel 2.6.23. The Htree is also used to index the files extents when the flow over 4 extents in the inode.
The octree color quantization algorithm, invented by Gervautz and Purgathofer in 1988, encodes image color data as an octree up to nine levels deep. Octrees are used because and there are three color components in the RGB system. The node index to branch out from at the top level is determined by a formula that uses the most significant bits of the red, green, and blue color components, e.g. 4r + 2g + b. The next lower level uses the next bit significance, and so on. Less significant bits are sometimes ignored to reduce the tree size.
The algorithm is highly memory efficient because the tree's size can be limited. The bottom level of the octree consists of leaf nodes that accrue color data not represented in the tree; these nodes initially contain single bits. If much more than the desired number of palette colors are entered into the octree, its size can be continually reduced by seeking out a bottom-level node and averaging its bit data up into a leaf node, pruning part of the tree. Once sampling is complete, exploring all routes in the tree down to the leaf nodes, taking note of the bits along the way, will yield approximately the required number of colors.
3D computer graphics Spatial indexing Nearest neighbor search Efficient collision detection in three dimensions View frustum culling Fast Multipole Method Unstructured grid Finite element analysis Sparse voxel octree State estimation[3]
Image representation Spatial indexing Efficient collision detection in two dimensions View frustum culling of terrain data Storing sparse data, such as a formatting information for a spreadsheet or for some matrix calculations Solution of multidimensional fields (computational fluid dynamics, electromagnetism) Conway's Game of Life simulation program.[2] State estimation[3] Quadtrees are also used in the area of fractal image analysis
Quadtrees are the two-dimensional analog of octrees.
Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in computational geometry can be based on red–black trees, and the Completely Fair Scheduler used in current Linux kernels uses red–black trees.
The AVL tree is another structure supporting O(log n) search, insertion, and removal. It is more rigidly balanced than red–black trees, leading to slower insertion and removal but faster retrieval. This makes it attractive for data structures that may be built once and loaded without reconstruction, such as language dictionaries (or program dictionaries, such as the opcodes of an assembler or interpreter).
Red–black trees are also particularly valuable in functional programming, where they are one of the most common persistent data structures, used to construct associative arrays and sets which can retain previous versions after mutations. The persistent version of red–black trees requires O(log n) space for each insertion or deletion, in addition to time.
For every 2-4 tree, there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2-4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2-4 trees just before red–black trees, even though 2-4 trees are not often used in practice.
In 2008, Sedgewick introduced a simpler version of the red–black tree called the left-leaning red–black tree[5] by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Red–black trees can be made isometric to either 2-3 trees,[6] or 2-4 trees,[5] for any sequence of operations. The 2-4 tree isometry was described in 1978 by Sedgewick. With 2-4 trees, the isometry is resolved by a "color flip," corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node. The tango tree, a type of tree optimized for fast searches, usually uses red–black trees as part of its data structure.
Suffix trees can be used to solve a large number of string problems that occur in text-editing, free-text search, computational biology and other application areas.[19] Primary applications include:[19]
String search, in O(m) complexity, where m is the length of the sub-string (but with initial O(n) time required to build the suffix tree for the string) Finding the longest repeated substring Finding the longest common substring Finding the longest palindrome in a string
Suffix trees are often used in bioinformatics applications, searching for patterns in DNA or protein sequences (which can be viewed as long strings of characters). The ability to search efficiently with mismatches might be considered their greatest strength. Suffix trees are also used in data compression; they can be used to find repeated data, and can be used for the sorting stage of the Burrows–Wheeler transform. Variants of the LZW compression schemes use suffix trees (LZSS). A suffix tree is also used in suffix tree clustering, a data clustering algorithm used in some search engines.[20]
A number of interesting applications have been derived for these Top Trees some of them include
([SLEATOR AND TARJAN 1983]). We can maintain a dynamic collection of weighted trees in time per link and cut, supporting queries about the maximum edge weight between any two vertices in O (log n) time. Proof outline: It involves maintaining at each node the maximum weight (max_wt) on its cluster path, if it is a point cluster then max_wt() is initialsed as . When a cluster is a union of two clusters then it is the maximum value of the two merged clusters. If we have to find the max wt between and then we do Expose, and report max_wt.
([SLEATOR AND TARJAN 1983]). In the scenario of the above application we can also add a common weight to all edges on a given path · · · in time. Proof outline: We introduce a weight called extra() to be added to all the edges in . Which is maintained appropriately ; split() requires that, for each path child of , we set max_wt(A) := max_wt() + extra() and extra() := extra() + extra(). For  := join(, ), we set max_wt() := max {max_wt(), max_wt()} and extra() := 0. Finally, to find the maximum weight on the path · · ·, we set  := Expose and return max_wt().
([GOLDBERG ET AL. 1991]). We can ask for the maximum weight in the underlying tree containing a given vertex in time. Proof outline: This requires maintaining additional information about the maximum weight non cluster path edge in a cluster under the Merge and Split operations.
The distance between two vertices and can be found in time as length(Expose). Proof outline:We will maintain the length length() of the cluster path. The length is maintained as the maximum weight except that, if is created by a join(Merge), length() is the sum of lengths stored with its path children.
Queries regarding diameter of a tree and its subsequent maintenance takes time.
The Center and Median can me maintained under Link(Merge) and Cut(Split) operations in time.
Arrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.
Arrays are used to implement other data structures, such as heaps, hash tables, deques, queues, stacks, strings, and VLists.
One or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate "dynamic memory" portably.
Arrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive), multiple IF statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) - that direct the path of the execution.
The B-tree uses all of the ideas described above. In particular, a B-tree:
keeps keys in sorted order for sequential traversing uses a hierarchical index to minimize the number of disk reads uses partially full blocks to speed insertions and deletions keeps the index balanced with an elegant recursive algorithm
In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.
BDDs are extensively used in CAD software to synthesize circuits (logic synthesis) and in formal verification. There are several lesser known applications of BDD, including Fault tree analysis, Bayesian Reasoning, Product Configuration, and Private information retrieval [8] [9][citation needed].
Every arbitrary BDD (even if it is not reduced or ordered) can be directly implemented by replacing each node with a 2 to 1 multiplexer; each multiplexer can be directly implemented by a 4-LUT in a FPGA. It is not so simple to convert from an arbitrary network of logic gates to a BDD[citation needed] (unlike the and-inverter graph).
Bipartite graphs are extensively used in modern coding theory, especially to decode codewords received from the channel. Factor graphs and Tanner graphs are examples of this. A Tanner graph is a bipartite graph in which the vertices on one side of the bipartition represent digits of a codeword, and the vertices on the other side represent combinations of digits that are expected to sum to zero in a codeword without errors.[35] A factor graph is a closely related belief network used for probabilistic decoding of LDPC and turbo codes.[36]
In computer science, a Petri net is a mathematical modeling tool used in analysis and simulations of concurrent systems. A system is modeled as a bipartite directed graph with two sets of nodes: A set of "place" nodes that contain resources, and a set of "event" nodes which generate and/or consume resources. There are additional constraints on the nodes and edges that constrain the behavior of the system. Petri nets utilize the properties of bipartite directed graphs and other properties to allow mathematical proofs of the behavior of systems while also allowing easy implementation of simulations of the system.[37]
In projective geometry, Levi graphs are a form of bipartite graph used to model the incidences between points and lines in a configuration. Corresponding to the geometric property of points and lines that every two lines meet in at most one point and every two points be connected with a single line, Levi graphs necessarily do not contain any cycles of length four, so their girth must be six or more.[38]
Pairs of consecutive sequence values (shown as the thick red edges) that bracket a sequence value x (the darker blue point). The cost of including x in the sorted order produced by the Levcopoulos–Petersson algorithm is proportional to the logarithm of this number of bracketing pairs.
Levcopoulos & Petersson (1989) describe a sorting algorithm based on Cartesian trees. They describe the algorithm as based on a tree with the maximum at the root, but it may be modified straightforwardly to support a Cartesian tree with the convention that the minimum value is at the root. For consistency, it is this modified version of the algorithm that is described below.
The Levcopoulos–Petersson algorithm can be viewed as a version of selection sort or heap sort that maintains a priority queue of candidate minima, and that at each step finds and removes the minimum value in this queue, moving this value to the end of an output sequence. In their algorithm, the priority queue consists only of elements whose parent in the Cartesian tree has already been found and removed. Thus, the algorithm consists of the following steps:
Construct a Cartesian tree for the input sequence Initialize a priority queue, initially containing only the tree root While the priority queue is non-empty: Find and remove the minimum value x in the priority queue Add x to the output sequence Add the Cartesian tree children of x to the priority queue
As Levcopoulos and Petersson show, for input sequences that are already nearly sorted, the size of the priority queue will remain small, allowing this method to take advantage of the nearly-sorted input and run more quickly. Specifically, the worst-case running time of this algorithm is O(n log k), where k is the average, over all values x in the sequence, of the number of consecutive pairs of sequence values that bracket x. They also prove a lower bound stating that, for any n and k = ω(1), any comparison-based sorting algorithm must use Ω(n log k) comparisons for some inputs.
Caterpillar trees have been used in chemical graph theory to represent the structure of benzenoid hydrocarbon molecules. In this representation, one forms a caterpillar in which each edge corresponds to a 6-carbon ring in the molecular structure, and two edges are incident at a vertex whenever the corresponding rings belong to a sequence of rings connected end-to-end in the structure. El-Basil (1987) writes, "It is amazing that nearly all graphs that played an important role in what is now called "chemical graph theory" may be related to caterpillar trees." In this context, caterpillar trees are also known as benzenoid trees and Gutman trees, after the work of Ivan Gutman in this area.[2][10][11]
The useful property of a circular buffer is that it does not need to have its elements shuffled around when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer.
Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a Linked list approach may be preferred instead.
In some situations, overwriting circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card) is unable to momentarily keep up.
The useful property of a circular buffer is that it does not need to have its elements shuffled around when one is consumed. (If a non-circular buffer were used then it would be necessary to shift all elements when one is consumed.) In other words, the circular buffer is well suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer.
Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Should a maximum size be adopted for a queue, then a circular buffer is a completely ideal implementation; all queue operations are constant time. However, expanding a circular buffer requires shifting memory, which is comparatively costly. For arbitrarily expanding queues, a Linked list approach may be preferred instead.
In some situations, overwriting circular buffer can be used, e.g. in multimedia. If the buffer is used as the bounded buffer in the producer-consumer problem then it is probably desired for the producer (e.g., an audio generator) to overwrite old data if the consumer (e.g., the sound card) is unable to momentarily keep up.
The word "clique", in its graph-theoretic usage, arose from the work of Luce & Perry (1949), who used complete subgraphs to model cliques (groups of people who all know each other) in social networks. For continued efforts to model social cliques graph-theoretically, see e.g. Alba (1973), Peay (1974), and Doreian & Woodard (1994).
Many different problems from bioinformatics have been modeled using cliques. For instance, Ben-Dor, Shamir & Yakhini (1999) model the problem of clustering gene expression data as one of finding the minimum number of changes needed to transform a graph describing the data into a graph formed as the disjoint union of cliques; Tanay & Sharan (Shamir) discuss a similar biclustering problem for expression data in which the clusters are required to be cliques. Sugihara (1984) uses cliques to model ecological niches in food webs. Day & Sankoff (1986) describe the problem of inferring evolutionary trees as one of finding maximum cliques in a graph that has as its vertices characteristics of the species, where two vertices share an edge if there exists a perfect phylogeny combining those two characters. Samudrala & Moult (1998) model protein structure prediction as a problem of finding cliques in a graph whose vertices represent positions of subunits of the protein. And by searching for cliques in a protein-protein interaction network, Spirin & Mirny (2003) found clusters of proteins that interact closely with each other and have few interactions with proteins outside the cluster. Power graph analysis is a method for simplifying complex biological networks by finding cliques and related structures in these networks.
In electrical engineering, Prihar (1956) uses cliques to analyze communications networks, and Paull & Unger (1959) use them to design efficient circuits for computing partially specified Boolean functions. Cliques have also been used in automatic test pattern generation: a large clique in an incompatibility graph of possible faults provides a lower bound on the size of a test set.[3] Cong & Smith (1993) describe an application of cliques in finding a hierarchical partition of an electronic circuit into smaller subunits.
In chemistry, Rhodes et al. (2003) use cliques to describe chemicals in a chemical database that have a high degree of similarity with a target structure. Kuhl, Crippen & Friesen (1983) use cliques to model the positions in which two chemicals will bind to each other.
Several graph-theoretic concepts are related to each other via complement graphs:
The complement of an edgeless graph is a complete graph and vice versa. An independent set in a graph is a clique in the complement graph and vice versa. The complement of any triangle-free graph is a claw-free graph. A self-complementary graph is a graph that is isomorphic to its own complement. Cographs are defined as the graphs that can be built up from disjoint union and complementation operations, and form a self-complementary family of graphs: the complement of any cograph is another (possibly different) cograph.
Dijkstra's algorithm for shortest paths in graphs and Prim's algorithm for minimum spanning trees both use a min-heap in which there are n delete-min operations and as many as m decrease-priority operations, where n is the number of vertices in the graph and m is the number of edges. By using a d-ary heap with d = m/n, the total times for these two types of operations may be balanced against each other, leading to a total time of O(m logm/n n) for the algorithm, an improvement over the O(m log n) running time of binary heap versions of these algorithms whenever the number of edges is significantly larger than the number of vertices.[1][5] An alternative priority queue data structure, the Fibonacci heap, gives an even better theoretical running time of O(m + n log n), but in practice d-ary heaps are generally at least as fast, and often faster, than Fibonacci heaps for this application.[10]
4-heaps may perform better than binary heaps in practice, even for delete-min operations.[2][3] Additionally, a d-ary heap typically runs much faster than a binary heap for heap sizes that exceed the size of the computer's cache memory: A binary heap typically requires more cache misses and virtual memory page faults than a d-ary heap, each one taking far more time than the extra work incurred by the additional comparisons a d-ary heap makes compared to a binary heap.[6][7]
Applications of GADTs include generic programming, modelling programming languages (higher-order abstract syntax), maintaining invariants[disambiguation needed] in data structures, expressing constraints in embedded domain-specific languages, and modelling objects.[9]
Hash trees can be used to verify any kind of data stored, handled and transferred in and between computers. Currently the main use of hash trees is to make sure that data blocks received from other peers in a peer-to-peer network are received undamaged and unaltered, and even to check that the other peers do not lie and send fake blocks. Suggestions have been made to use hash trees in trusted computing systems. Sun Microsystems has used Hash Trees in the ZFS filesystem.[1] Hash Trees are used in Google Wave protocol,[2] Git distributed revision control system, the Tahoe-LAFS backup system, the Bitcoin peer-to-peer network, and a number of NoSQL systems like Apache Cassandra & Riak.[3]
Hash trees were invented in 1979 by Ralph Merkle.[4] The original purpose was to make it possible to efficiently handle many Lamport one-time signatures. Lamport signatures are believed to still be secure in the event that quantum computers become reality. Unfortunately each Lamport key can only be used to sign a single message. But combined with hash trees they can be used for many messages and then become a fairly efficient digital signature scheme.
The heap data structure has many applications.
Heapsort: One of the best sorting methods being in-place and with no quadratic worst-case scenarios. Selection algorithms: Finding the min, max, both the min and max, median, or even the k-th largest element can be done in linear time (often constant time) using heaps.[7] Graph algorithms: By using heaps as internal traversal data structures, run time will be reduced by polynomial order. Examples of such problems are Prim's minimal spanning tree algorithm and Dijkstra's shortest path problem.
Full and almost full binary heaps may be represented in a very space-efficient way using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position n would be at positions 2n and 2n+1 in a one-based array, or 2n+1 and 2n+2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by swapping elements which are out of order. As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.
The mathematical theory of interval graphs was developed with a view towards applications by researchers at the RAND Corporation's mathematics department, which included young researchers—such as Peter C. Fishburn and students like Alan C. Tucker and Joel E. Cohen—besides leaders—such as Delbert Fulkerson and (recurring visitor) Victor Klee.[7] Cohen applied interval graphs to mathematical models of population biology, specifically food webs.[8]
Other applications include genetics, bioinformatics, and computer science. Finding a set of intervals that represent an interval graph can also be used as a way of assembling contiguous subsequences in DNA mapping.[9] Interval graphs are used to represent resource allocation problems in operations research and scheduling theory. Each interval represents a request for a resource for a specific period of time; the maximum weight independent set problem for the graph represents the problem of finding the best subset of requests that can be satisfied without conflicts.[10] Interval graphs also play an important role in temporal reasoning.[11]
As the name implies, lists can be used to store a list of records. The items in a list can be sorted for the purpose of fast search (binary search).
Because in computing, lists are easier to realize than sets, a finite set in mathematical sense can be realized as a list with additional restrictions, that is, duplicate elements are disallowed and such that order is irrelevant. If the list is sorted, it speeds up determining if a given item is already in the set but in order to ensure the order, it requires more time to add new entry to the list. In efficient implementations, however, sets are implemented using self-balancing binary search trees or hash tables, rather than a list.
Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for, as mentioned above).[15] They are invoked as subroutines in algorithms for other problems, including the Christofides algorithm for approximating the traveling salesman problem,[16] approximating the multi-terminal minimum cut problem (which is equivalent in the single-terminal case to the maximum flow problem),[17] and approximating the minimum-cost weighted perfect matching.[18]
Other practical applications based on minimal spanning trees include:
Taxonomy, one of the earliest motivating applications.[19] Cluster analysis: clustering points in the plane,[20] single-linkage clustering (a method of hierarchical clustering),[21] graph-theoretic clustering,[22] and clustering gene expression data.[23] Constructing trees for broadcasting in computer networks.[24] Image registration[25] and segmentation[26] �?see minimum spanning tree-based segmentation. Curvilinear feature extraction in computer vision.[27] Handwriting recognition of mathematical expressions.[28] Circuit design: implementing efficient multiple constant multiplications, as used in finite impulse response filters.[29] Regionalisation of socio-geographic areas, the grouping of areas into homogeneous, contiguous regions.[30] Comparing ecotoxicology data.[31] Topological observability in power systems.[32] Measuring homogenity of two-dimensional materials.[33] Minimax process control.[34]
In pedagogical contexts, minimum spanning tree algorithms serve as a common introductory example of both graph algorithms and greedy algorithms due to their simplicity.
Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for, as mentioned above).[15] They are invoked as subroutines in algorithms for other problems, including the Christofides algorithm for approximating the traveling salesman problem,[16] approximating the multi-terminal minimum cut problem (which is equivalent in the single-terminal case to the maximum flow problem),[17] and approximating the minimum-cost weighted perfect matching.[18]
Other practical applications based on minimal spanning trees include:
Taxonomy, one of the earliest motivating applications.[19] Cluster analysis: clustering points in the plane,[20] single-linkage clustering (a method of hierarchical clustering),[21] graph-theoretic clustering,[22] and clustering gene expression data.[23] Constructing trees for broadcasting in computer networks.[24] Image registration[25] and segmentation[26] �?see minimum spanning tree-based segmentation. Curvilinear feature extraction in computer vision.[27] Handwriting recognition of mathematical expressions.[28] Circuit design: implementing efficient multiple constant multiplications, as used in finite impulse response filters.[29] Regionalisation of socio-geographic areas, the grouping of areas into homogeneous, contiguous regions.[30] Comparing ecotoxicology data.[31] Topological observability in power systems.[32] Measuring homogenity of two-dimensional materials.[33] Minimax process control.[34]
In pedagogical contexts, minimum spanning tree algorithms serve as a common introductory example of both graph algorithms and greedy algorithms due to their simplicity.
Some languages, such as C, allow the declaration of opaque records (structs), whose size and fields are hidden from the client. The only thing that the client can do with an object of such a type is to take its memory address, to produce an opaque pointer.
If the information provided by the interface is sufficient to determine the type's size, then clients can declare variables, fields, and arrays of that type, assign their values, and possibly compare them for equality. This is usually the case for opaque pointers.
In some languages, such as Java, the only kind of opaque type provided is the opaque pointer. Indeed, in Java (and several other languages) records are always handled through pointers.
Some languages allow partially opaque types, e.g. a record which has some public fields, known and accessible to all clients, and some hidden fields which are not revealed in the interface. Such types play a fundamental role in object-oriented programming.
The information which is missing in the interface may be declared in its implementation, or in another "friends-only" interface. This second option allows the hidden information to be shared by two or more modules.
Polytrees have been used as a graphical model for probabilistic reasoning. If a Bayesian network has the structure of a polytree, then belief propagation may be used to perform inference efficiently on it.[1][2]
The contour tree of a real-valued function on a vector space is a polytree that describes the level sets of the function. The nodes of the contour tree are the level sets that pass through a critical point of the function and the edges describe contiguous sets of level sets without a critical point. The orientation of an edge is determined by the comparison between the function values on the corresponding two level sets.[7]
Naphthalene molecule
Quantum graphs were first employed in the 1930s to model the spectrum of free electrons in organic molecules like Naphthalene, see figure. As a first approximation the atoms are taken to be vertices while the σ-electrons form bonds that fix a frame in the shape of the molecule on which the free electrons are confined.
A similar problem appears when considering quantum waveguides. These are mesoscopic systems - systems built with a width on the scale of nanometers. A quantum waveguide can be thought of as a fattened graph where the edges are thin tubes. The spectrum of the Laplace operator on this domain converges to the spectrum of the Laplace operator on the graph under certain conditions. Understanding mesoscopic systems plays an important role in the field of nanotechnology.
In 1997 Kottos and Smilansky proposed quantum graphs as a model to study quantum chaos, the quantum mechanics of systems that are classically chaotic. Classical motion on the graph can be defined as a probabilistic Markov chain where the probability of scattering from edge to edge is given by the absolute value of the quantum transition amplitude squared, . For almost all finite connected quantum graphs the probabilistic dynamics is ergodic and mixing, in other words chaotic.
Quantum graphs embedded in two or three dimensions appear in the study of photonic crystals. In two dimensions a simple model of a photonic crystal consists of polygonal cells of a dense dielectric with narrow interfaces between the cells filled with air. Studying dielectric modes that stay mostly in the dielectric gives rise to a pseudo-differential operator on the graph that follows the narrow interfaces.
Periodic quantum graphs like the lattice in are common models of periodic systems and quantum graphs have been applied to the study the phenomena of Anderson localization where localized states occur at the edge of spectral bands in the presence of disorder.
As mentioned, radix trees are useful for constructing associative arrays with keys that can be expressed as strings. They find particular application in the area of IP routing, where the ability to contain large ranges of values with a few exceptions is particularly suited to the hierarchical organization of IP addresses.[2] They are also used for inverted indexes of text documents in information retrieval.
Recursive trees can be generated using a simple stochastic process. Such random recursive trees are used as simple models for epidemics.
Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in computational geometry can be based on red–black trees, and the Completely Fair Scheduler used in current Linux kernels uses red–black trees.
The AVL tree is another structure supporting O(log n) search, insertion, and removal. It is more rigidly balanced than red–black trees, leading to slower insertion and removal but faster retrieval. This makes it attractive for data structures that may be built once and loaded without reconstruction, such as language dictionaries (or program dictionaries, such as the opcodes of an assembler or interpreter).
Red–black trees are also particularly valuable in functional programming, where they are one of the most common persistent data structures, used to construct associative arrays and sets which can retain previous versions after mutations. The persistent version of red–black trees requires O(log n) space for each insertion or deletion, in addition to time.
For every 2-4 tree, there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2-4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2-4 trees just before red–black trees, even though 2-4 trees are not often used in practice.
In 2008, Sedgewick introduced a simpler version of the red–black tree called the left-leaning red–black tree[5] by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Red–black trees can be made isometric to either 2-3 trees,[6] or 2-4 trees,[5] for any sequence of operations. The 2-4 tree isometry was described in 1978 by Sedgewick. With 2-4 trees, the isometry is resolved by a "color flip," corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node. The tango tree, a type of tree optimized for fast searches, usually uses red–black trees as part of its data structure.
Surprisingly, soft heaps are useful in the design of deterministic algorithms, despite their unpredictable nature. They were used to achieve the best complexity to date for finding a minimum spanning tree. They can also be used to easily build an optimal selection algorithm, as well as near-sorting algorithms, which are algorithms that place every element near its final position, a situation in which insertion sort is fast.
One of the simplest examples is the selection algorithm. Say we want to find the kth largest of a group of n numbers. First, we choose an error rate of 1/3; that is, at most 33% of the keys we insert will be corrupted. Now, we insert all n elements into the heap �?at this point, at most n/3 keys are corrupted. Next, we delete the minimum element from the heap about n/3 times. Because this is decreasing the size of the heap, it cannot increase the number of corrupted elements. Thus there are still at most n/3 keys that are corrupted.
Now at least 2n/3 �?n/3 = n/3 of the remaining keys are not corrupted, so each must be larger than every element we removed. Let L be the element that we have removed with the largest (actual) value, which is not necessarily the last element that we removed (because the last element we removed could have had its key corrupted, or increased, to a value larger than another element that we have already removed). L is larger than all the other n/3 elements that we removed and smaller than the remaining n/3 uncorrupted elements in the soft heap. Therefore, L divides the elements somewhere between 33%/66% and 66%/33%. We then partition the set about L using the partition algorithm from quicksort and apply the same algorithm again to either the set of numbers less than L or the set of numbers greater than L, neither of which can exceed 2n/3 elements. Since each insertion and deletion requires O(1) amortized time, the total deterministic time is T(n) = T(2n/3) + O(n). Using case 3 of the master theorem (with ε=1 and c=2/3), we know that T(n) = Θ(n).
The final algorithm looks like this:
function softHeapSelect(a[1..n], k)
     if k = 1 then return minimum(a[1..n])
     create(S)
     for i from 1 to n
         insert(S, a[i])
     for i from 1 to n/3
         x := findmin(S)
         delete(S, x)
     xIndex := partition(a, x)  // Returns new index of pivot x
     if k < xIndex
         softHeapSelect(a[1..xIndex-1], k)
     else
         softHeapSelect(a[xIndex..n], k-xIndex+1)
1) Commercial Computing:[2]
Government organisations, private companies and many web based applications has to deal with huge amount of data. This data is stored by sorting it using different sorting algorithms. This can be used in quick and easy recovery of data.
2) In discrete mathematics: Sorted arrays can be used to implement Dijkstra's algorithm or Prim's algorithm. Also, algorithms like Kruskal's Algorithm for finding minimal spanning trees.
3) In priority scheduling: At the operating system level many processes are pending at a time, but CPU can handle only one process at single instance of time. Therefore, priorities are associated to each process.Then the processes are sent to CPU according to the highest priority by using sorted array of process ID's. Here, processes got sorted depending upon their priorities and then CPU is allocated to them. The process having the highest priority takes first position in sorted array. Hence priority-wise system processes scheduling is done.[3]
4) In Shortest-Job-First Scheduling: This is the special case of priority scheduling. Here, Processes get sorted according to burst time of the processes. The process requiring the shortest time will be allocated CPU first. Hence, Processes are being sent to CPU according to their burst time.

Process Burst time P1 3 P2 4 P3 1 P4 8 P5 6
Stacks have numerous applications. We see stacks in everyday life, from the books in our library, to the sheaf of papers that we keep in our printer tray. All of them follow the Last In First Out (LIFO) logic, that is when we add a book to a pile of books, we add it to the top of the pile, whereas when we remove a book from the pile, we generally remove it from the top of the pile.
Given below are a few applications of stacks in the world of computers:
The set of distances between the vertices of a claw provides an example of a finite metric space that cannot be embedded isometrically into a Euclidean space of any dimension.[7]
The star network, a computer network modeled after the star graph, is important in distributed computing.
A number of interesting applications have been derived for these Top Trees some of them include
([SLEATOR AND TARJAN 1983]). We can maintain a dynamic collection of weighted trees in time per link and cut, supporting queries about the maximum edge weight between any two vertices in O (log n) time. Proof outline: It involves maintaining at each node the maximum weight (max_wt) on its cluster path, if it is a point cluster then max_wt() is initialsed as . When a cluster is a union of two clusters then it is the maximum value of the two merged clusters. If we have to find the max wt between and then we do Expose, and report max_wt.
([SLEATOR AND TARJAN 1983]). In the scenario of the above application we can also add a common weight to all edges on a given path · · · in time. Proof outline: We introduce a weight called extra() to be added to all the edges in . Which is maintained appropriately ; split() requires that, for each path child of , we set max_wt(A) := max_wt() + extra() and extra() := extra() + extra(). For  := join(, ), we set max_wt() := max {max_wt(), max_wt()} and extra() := 0. Finally, to find the maximum weight on the path · · ·, we set  := Expose and return max_wt().
([GOLDBERG ET AL. 1991]). We can ask for the maximum weight in the underlying tree containing a given vertex in time. Proof outline: This requires maintaining additional information about the maximum weight non cluster path edge in a cluster under the Merge and Split operations.
The distance between two vertices and can be found in time as length(Expose). Proof outline:We will maintain the length length() of the cluster path. The length is maintained as the maximum weight except that, if is created by a join(Merge), length() is the sum of lengths stored with its path children.
Queries regarding diameter of a tree and its subsequent maintenance takes time.
The Center and Median can me maintained under Link(Merge) and Cut(Split) operations in time.
Although there is little difference mathematically between a graph and its transpose, the difference may be larger in computer science, depending on how a given graph is represented. For instance, for the web graph, it is easy to determine the outgoing links of a vertex, but hard to determine the incoming links, while in the reversal of this graph the opposite is true. In graph algorithms, therefore, it may sometimes be useful to construct the reversal of a graph, in order to put the graph into a form which is more suitable for the operations being performed on it. An example of this is Kosaraju's algorithm for strongly connected components, which applies depth first search twice, once to the given graph and a second time to its reversal.
Representing hierarchical data Storing data in a way that makes it easily searchable (see binary search tree and tree traversal) Representing sorted lists of data As a workflow for compositing digital images for visual effects Routing algorithms
