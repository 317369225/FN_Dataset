See also: Glossary of graph theory and Graph property
Two edges of a graph are called adjacent (sometimes coincident) if they share a common vertex. Two arrows of a directed graph are called consecutive if the head of the first one is at the nock (notch end) of the second one. Similarly, two vertices are called adjacent if they share a common edge (consecutive if they are at the notch and at the head of an arrow), in which case the common edge is said to join the two vertices. An edge and a vertex on that edge are called incident.
The graph with only one vertex and no edges is called the trivial graph. A graph with only vertices and no edges is known as an edgeless graph. The graph with no vertices and no edges is sometimes called the null graph or empty graph, but the terminology is not consistent and not all mathematicians allow this object.
In a weighted graph or digraph, each edge is associated with some value, variously called its cost, weight, length or other term depending on the application; such graphs arise in many contexts, for example in optimal routing problems such as the traveling salesman problem.
Normally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called vertex-labeled. However, for many questions it is better to treat vertices as indistinguishable; then the graph may be called unlabeled. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges). The same remarks apply to edges, so graphs with labeled edges are called edge-labeled graphs. Graphs with labels attached to edges or vertices are more generally designated as labeled. Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called unlabeled. (Note that in the literature the term labeled may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)
Every finite tree structure has a member that has no superior. This member is called the "root" or root node. The root is the starting node. But the converse is not true: infinite tree structures may or may not have a root node.
The lines connecting elements are called "branches", the elements themselves are called "nodes". Nodes without children are called leaf nodes, "end-nodes", or "leaves".
The names of relationships between nodes are modeled after family relations. The gender-neutral names "parent" and "child" have largely displaced the older "father" and "son" terminology, although the term "uncle" is still used for other nodes at the same level as the parent.
A node's "parent" is a node one step higher in the hierarchy (i.e. closer to the root node) and lying on the same branch. "Sibling" ("brother" or "sister") nodes share the same parent node. A node's "uncles" are siblings of that node's parent. A node that is connected to all lower-level nodes is called an "ancestor". The connected lower-level nodes are "descendants" of the ancestor node.
In the example, "encyclopedia" is the parent of "science" and "culture", its children. "Art" and "craft" are siblings, and children of "culture", which is their parent and thus one of their ancestors. Also, "encyclopedia", being the root of the tree, is the ancestor of "science", "culture", "art" and "craft". Finally, "science", "art" and "craft", being leaves, are ancestors of no other node.
Tree structures are used to depict all kinds of taxonomic knowledge, such as family trees, the biological evolutionary tree, the evolutionary tree of a language family, the grammatical structure of a language (a key example being S �?NP VP, meaning a sentence is a noun phrase and a verb phrase, with each in turn having other components which have other components), the way web pages are logically ordered in a web site, mathematical trees of integer sets, et cetera.
In a tree structure there is one and only one path from any point to any other point.
Tree structures are used extensively in computer science (see Tree (data structure) and telecommunications.)
For a formal definition see set theory.
Every non-leaf is a 2-node, 3-node or a 4-node. A 2-node contains one data item and has two children. A 3-node contains two data items and has 3 children. A 4-node contains 3 data items and has 4 children. All leaves are at the same level (the bottom level) All data are kept in sorted order Every non-leaf node will contain 1, 2 or 3 fields.
Every non-leaf is a 2-node or a 3-node. A 2-node contains one data item and has two children. A 3-node contains two data items and has 3 children. All leaves are at the same level (the bottom level) All data are kept in sorted order Every leaf node will contain 1 or 2 fields.
The number of nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a binary tree of height h is at least and at most where is the depth of the tree. The number of leaf nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a perfect binary tree can also be found using this formula: where is the number of leaf nodes in the tree. The number of null links (absent children of nodes) in a complete binary tree of nodes is . The number of internal nodes (non-leaf nodes) in a Complete Binary Tree of nodes is . For any non-empty binary tree with leaf nodes and nodes of degree 2, .[5]

Proof:
Let n = the total number of nodes B = number of branches n0, n1, n2 represent the number of nodes with no children, a single child, and two children respectively.


B = n - 1 (since all nodes except the root node come from a single branch) B = n1 + 2*n2 n = n1+ 2*n2 + 1 n = n0 + n1 + n2 n1+ 2*n2 + 1 = n0 + n1 + n2 ==> n0 = n2 + 1
The k-d tree is a binary tree in which every node is a k-dimensional point. Every non-leaf node can be thought of as implicitly generating a splitting hyperplane that divides the space into two parts, known as half-spaces. Points to the left of this hyperplane represent the left subtree of that node and points right of the hyperplane are represented by the right subtree. The hyperplane direction is chosen in the following way: every node in the tree is associated with one of the k-dimensions, with the hyperplane perpendicular to that dimension's axis. So, for example, if for a particular split the "x" axis is chosen, all points in the subtree with a smaller "x" value than the node will appear in the left subtree and all points with larger "x" value will be in the right subtree. In such a case, the hyperplane would be set by the x-value of the point, and its normal would be the unit x-axis.[1]
An example of a 1-dimensional range tree.
A range tree on a set of 1-dimensional points is a balanced binary search tree on those points. The points stored in the tree are stored in the leaves of the tree; each internal node stores the largest value contained in its left subtree. A range tree on a set of points in d-dimensions is a recursively defined multi-level binary search tree. Each level of the data structure is a binary search tree on one of the d-dimensions. The first level is a binary search tree on the first of the d-coordinates. Each vertex v of this tree contains an associated structure that is a (d�?)-dimensional range tree on the last (d�?)-coordinates of the points stored in the subtree of v.
An example of a red–black tree
In addition to the requirements imposed on a binary search trees, with red–black trees:
A node is either red or black. The root is black. (This rule is sometimes omitted. Since the root can always be changed from red to black, but not necessarily vice-versa, this rule has little effect on analysis.) All leaves (NIL) are black. (All leaves are same color as the root.) Both children of every red node are black. Every simple path from a given node to any of its descendant leaves contains the same number of black nodes.
These constraints enforce a critical property of red–black trees: that the path from the root to the furthest leaf is no more than twice as long as the path from the root to the nearest leaf. The result is that the tree is roughly height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height of the tree, this theoretical upper bound on the height allows red–black trees to be efficient in the worst case, unlike ordinary binary search trees. Red–black trees are in general not weight-balanced,[4] that is sibling nodes can have hugely differing numbers of descendants.
To see why this is guaranteed, it suffices to consider the effect of properties 4 and 5 together. For a red–black tree T, let B be the number of black nodes in property 5. Therefore the shortest possible path from the root of T to any leaf consists of B black nodes. Longer possible paths may be constructed by inserting red nodes. However, property 4 makes it impossible to insert more than one consecutive red node. Therefore the longest possible path consists of 2B nodes, alternating black and red.
The shortest possible path has all black nodes, and the longest possible path alternates between red and black nodes. Since all maximal paths have the same number of black nodes, by property 5, this shows that no path is more than twice as long as any other path.
In many of the presentations of tree data structures, it is possible for a node to have only one child, and leaf nodes contain data. It is possible to present red–black trees in this paradigm, but it changes several of the properties and complicates the algorithms. For this reason, this article uses "null leaves", which contain no data and merely serve to indicate where the tree ends, as shown above. These nodes are often omitted in drawings, resulting in a tree that seems to contradict the above principles, but in fact does not. A consequence of this is that all internal (non-leaf) nodes have two children, although one or both of those children may be null leaves. Property 5 ensures that a red node must have either two black null leaves or two black non-leaves as children. For a black node with one null leaf child and one non-null-leaf child, properties 3, 4 and 5 ensure that the non-null-leaf child must be a red node with two black null leaves as children.
Some explain a red–black tree as a binary search tree whose edges, instead of nodes, are colored in red or black, but this does not make any difference. The color of a node in this article's terminology corresponds to the color of the edge connecting the node to its parent, except that the root node is always black (property 2) whereas the corresponding edge does not exist.
This section describes the structure of a segment tree in a one-dimensional space.
Let S be a set of intervals, or segments. Let p1, p2, ..., pm be the list of distinct interval endpoints, sorted from left to right. Consider the partitioning of the real line induced by those points. The regions of this partitioning are called elementary intervals. Thus, the elementary intervals are, from left to right:

That is, the list of elementary intervals consists of open intervals between two consecutive endpoints pi and pi+1, alternated with closed intervals consisting of a single endpoint. Single points are treated themselves as intervals because the answer to a query is not necessarily the same at the interior of an elementary interval and its endpoints.[2]
Graphic example of the structure of the segment tree. This instance is built for the segments shown at the bottom.
Given a set I of intervals, or segments, a segment tree T for I is structured as follows:
T is a binary tree. Its leaves correspond to the elementary intervals induced by the endpoints in I, in an ordered way: the leftmost leaf corresponds to the leftmost interval, and so on. The elementary interval corresponding to a leaf v is denoted Int(v). The internal nodes of T correspond to intervals that are the union of elementary intervals: the interval Int(N) corresponding to node N is the union of the intervals corresponding to the leaves of the tree rooted at N. That implies that Int(N) is the union of the intervals of its two children. Each node or leaf v in T stores the interval Int(v) and a set of intervals, in some data structure. This canonical subset of node v contains the intervals [x, x�?/i>] from I such that [x, x�?/i>] contains Int(v) and does not contain Int(parent(v)). That is, each segment in I stores the segments that span through its interval, but do not span through the interval of its parent.[3]
Maximum number of nodes
�?Let be height of a ternary tree.
�?Let be the maximum number of nodes in a ternary tree of height h
h M(h) 0 1 1 4 2 13 3 40
�?
�?Every tree of height h has at most nodes.
If a node occupies TREE [k}, then its Left Child is stored in tree . Mid Child is stored in TREE . Right Child is stored in TREE .
Every non-leaf is a 2-node, 3-node or a 4-node. A 2-node contains one data item and has two children. A 3-node contains two data items and has 3 children. A 4-node contains 3 data items and has 4 children. All leaves are at the same level (the bottom level) All data are kept in sorted order Every non-leaf node will contain 1, 2 or 3 fields.
Every non-leaf is a 2-node or a 3-node. A 2-node contains one data item and has two children. A 3-node contains two data items and has 3 children. All leaves are at the same level (the bottom level) All data are kept in sorted order Every leaf node will contain 1 or 2 fields.
The number of nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a binary tree of height h is at least and at most where is the depth of the tree. The number of leaf nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a perfect binary tree can also be found using this formula: where is the number of leaf nodes in the tree. The number of null links (absent children of nodes) in a complete binary tree of nodes is . The number of internal nodes (non-leaf nodes) in a Complete Binary Tree of nodes is . For any non-empty binary tree with leaf nodes and nodes of degree 2, .[5]

Proof:
Let n = the total number of nodes B = number of branches n0, n1, n2 represent the number of nodes with no children, a single child, and two children respectively.


B = n - 1 (since all nodes except the root node come from a single branch) B = n1 + 2*n2 n = n1+ 2*n2 + 1 n = n0 + n1 + n2 n1+ 2*n2 + 1 = n0 + n1 + n2 ==> n0 = n2 + 1
The number of nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a binary tree of height h is at least and at most where is the depth of the tree. The number of leaf nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a perfect binary tree can also be found using this formula: where is the number of leaf nodes in the tree. The number of null links (absent children of nodes) in a complete binary tree of nodes is . The number of internal nodes (non-leaf nodes) in a Complete Binary Tree of nodes is . For any non-empty binary tree with leaf nodes and nodes of degree 2, .[5]

Proof:
Let n = the total number of nodes B = number of branches n0, n1, n2 represent the number of nodes with no children, a single child, and two children respectively.


B = n - 1 (since all nodes except the root node come from a single branch) B = n1 + 2*n2 n = n1+ 2*n2 + 1 n = n0 + n1 + n2 n1+ 2*n2 + 1 = n0 + n1 + n2 ==> n0 = n2 + 1
An example of a Bloom filter, representing the set {x, y, z}. The colored arrows show the positions in the bit array that each set element is mapped to. The element w is not in the set {x, y, z}, because it hashes to one bit-array position containing 0. For this figure, m=18 and k=3.
An empty Bloom filter is a bit array of m bits, all set to 0. There must also be k different hash functions defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.
To add an element, feed it to each of the k hash functions to get k array positions. Set the bits at all these positions to 1.
To query for an element (test whether it is in the set), feed it to each of the k hash functions to get k array positions. If any of the bits at these positions are 0, the element is definitely not in the set �?if it were, then all the bits would have been set to 1 when it was inserted. If all are 1, then either the element is in the set, or the bits have by chance been set to 1 during the insertion of other elements, resulting in a false positive. In a simple bloom filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.
The requirement of designing k different independent hash functions can be prohibitive for large k. For a good hash function with a wide output, there should be little if any correlation between different bit-fields of such a hash, so this type of hash can be used to generate multiple "different" hash functions by slicing its output into multiple bit fields. Alternatively, one can pass k different initial values (such as 0, 1, ..., k �?#160;1) to a hash function that takes an initial value; or add (or append) these values to the key. For larger m and/or k, independence among the hash functions can be relaxed with negligible increase in false positive rate (Dillinger & Manolios (2004a), Kirsch & Mitzenmacher (2006)). Specifically, Dillinger & Manolios (2004b) show the effectiveness of deriving the k indices using enhanced double hashing or triple hashing, variants of double hashing that are effectively simple random number generators seeded with the two or three hash values.
Removing an element from this simple Bloom filter is impossible because false negatives are not permitted. An element maps to k bits, and although setting any one of those k bits to zero suffices to remove the element, it also results in removing any other elements that happen to map onto that bit. Since there is no way of determining whether any other elements have been added that affect the bits for an element to be removed, clearing any of the bits would introduce the possibility for false negatives.
One-time removal of an element from a Bloom filter can be simulated by having a second Bloom filter that contains items that have been removed. However, false positives in the second filter become false negatives in the composite filter, which may be undesirable. In this approach re-adding a previously removed item is not possible, as one would have to remove it from the "removed" filter.
It is often the case that all the keys are available but are expensive to enumerate (for example, requiring many disk reads). When the false positive rate gets too high, the filter can be regenerated; this should be a relatively rare event.
Unlike sets based on hash tables, any Bloom filter can represent the entire universe of elements. In this case, all bits are 1. Another consequence of this property is that add never fails due to the data structure "filling up." However, the false positive rate increases steadily as elements are added until all bits in the filter are set to 1, so a negative value is never returned. At this point, the Bloom filter completely ceases to differentiate between differing inputs, and is functionally useless.
Union and intersection of Bloom filters with the same size and set of hash functions can be implemented with bitwise OR and AND operations, respectively. The union operation on Bloom filters is lossless in the sense that the resulting Bloom filter is the same as the Bloom filter created from scratch using the union of the two sets. The intersect operation satisfies a weaker property: the false positive probability in the resulting Bloom filter is at most the false-positive probability in one of the constituent Bloom filters, but may be larger than the false positive probability in the Bloom filter created from scratch using the intersection of the two sets.
Some kinds of superimposed code can be seen as a Bloom filter implemented with physical edge-notched cards.
An example of a Bloom filter, representing the set {x, y, z}. The colored arrows show the positions in the bit array that each set element is mapped to. The element w is not in the set {x, y, z}, because it hashes to one bit-array position containing 0. For this figure, m=18 and k=3.
An empty Bloom filter is a bit array of m bits, all set to 0. There must also be k different hash functions defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.
To add an element, feed it to each of the k hash functions to get k array positions. Set the bits at all these positions to 1.
To query for an element (test whether it is in the set), feed it to each of the k hash functions to get k array positions. If any of the bits at these positions are 0, the element is definitely not in the set �?if it were, then all the bits would have been set to 1 when it was inserted. If all are 1, then either the element is in the set, or the bits have by chance been set to 1 during the insertion of other elements, resulting in a false positive. In a simple bloom filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.
The requirement of designing k different independent hash functions can be prohibitive for large k. For a good hash function with a wide output, there should be little if any correlation between different bit-fields of such a hash, so this type of hash can be used to generate multiple "different" hash functions by slicing its output into multiple bit fields. Alternatively, one can pass k different initial values (such as 0, 1, ..., k �?#160;1) to a hash function that takes an initial value; or add (or append) these values to the key. For larger m and/or k, independence among the hash functions can be relaxed with negligible increase in false positive rate (Dillinger & Manolios (2004a), Kirsch & Mitzenmacher (2006)). Specifically, Dillinger & Manolios (2004b) show the effectiveness of deriving the k indices using enhanced double hashing or triple hashing, variants of double hashing that are effectively simple random number generators seeded with the two or three hash values.
Removing an element from this simple Bloom filter is impossible because false negatives are not permitted. An element maps to k bits, and although setting any one of those k bits to zero suffices to remove the element, it also results in removing any other elements that happen to map onto that bit. Since there is no way of determining whether any other elements have been added that affect the bits for an element to be removed, clearing any of the bits would introduce the possibility for false negatives.
One-time removal of an element from a Bloom filter can be simulated by having a second Bloom filter that contains items that have been removed. However, false positives in the second filter become false negatives in the composite filter, which may be undesirable. In this approach re-adding a previously removed item is not possible, as one would have to remove it from the "removed" filter.
It is often the case that all the keys are available but are expensive to enumerate (for example, requiring many disk reads). When the false positive rate gets too high, the filter can be regenerated; this should be a relatively rare event.
Unlike sets based on hash tables, any Bloom filter can represent the entire universe of elements. In this case, all bits are 1. Another consequence of this property is that add never fails due to the data structure "filling up." However, the false positive rate increases steadily as elements are added until all bits in the filter are set to 1, so a negative value is never returned. At this point, the Bloom filter completely ceases to differentiate between differing inputs, and is functionally useless.
Union and intersection of Bloom filters with the same size and set of hash functions can be implemented with bitwise OR and AND operations, respectively. The union operation on Bloom filters is lossless in the sense that the resulting Bloom filter is the same as the Bloom filter created from scratch using the union of the two sets. The intersect operation satisfies a weaker property: the false positive probability in the resulting Bloom filter is at most the false-positive probability in one of the constituent Bloom filters, but may be larger than the false positive probability in the Bloom filter created from scratch using the intersection of the two sets.
Some kinds of superimposed code can be seen as a Bloom filter implemented with physical edge-notched cards.
The full automorphism group of the butterfly graph is a group of order 8 isomorphic to the Dihedral group D4, the group of symmetries of a square, including both rotations and reflections.
The characteristic polynomial of the butterfly graph is .
If a member of the generating set is its own inverse, , then it is generally represented by an undirected edge.
The Cayley graph depends in an essential way on the choice of the set of generators. For example, if the generating set has elements then each vertex of the Cayley graph has incoming and outgoing directed edges. In the case of a symmetric generating set with elements, the Cayley graph is a regular graph of degree
Cycles (or closed walks) in the Cayley graph indicate relations between the elements of In the more elaborate construction of the Cayley complex of a group, closed paths corresponding to relations are "filled in" by polygons. This means that the problem of constructing the Cayley graph of a given presentation is equivalent to solving the Word Problem for .[1]
If is a surjective group homomorphism and the images of the elements of the generating set for are distinct, then it induces a covering of graphs

where
In particular, if a group has generators, all of order different from 2, and the set consists of these generators together with their inverses, then the Cayley graph is covered by the infinite regular tree of degree corresponding to the free group on the same set of generators.
A graph can be constructed even if the set does not generate the group However, it is disconnected and is not considered to be a Cayley graph. In this case, each connected component of the graph represents a coset of the subgroup generated by .
For any finite Cayley graph, considered as undirected, the vertex connectivity is at least equal to 2/3 of the degree of the graph. If the generating set is minimal (removal of any element and, if present, its inverse from the generating set leaves a set which is not generating), the vertex connectivity is equal to the degree. The edge connectivity is in all cases equal to the degree.[4]
Cographs may be recognized in linear time, and a cotree representation constructed, using modular decomposition (Corneil, Perl & Stewart 1985), partition refinement (Habib & Paul 2005), or split decomposition (Gioan & Paul 2008). Once a cotree representation has been constructed, many familiar graph problems may be solved via simple bottom-up calculations on the cotrees.
For instance, to find the maximum clique in a cograph, compute in bottom-up order the maximum clique in each subgraph represented by a subtree of the cotree. For a node labeled 0, the maximum clique is the maximum among the cliques computed for that node's children. For a node labeled 1, the maximum clique is the union of the cliques computed for that node's children, and has size equal to the sum of the children's clique sizes. Thus, by alternately maximizing and summing values stored at each node of the cotree, we may compute the maximum clique size, and by alternately maximizing and taking unions, we may construct the maximum clique itself. Similar bottom-up tree computations allow the maximum independent set, vertex coloring number, maximum clique cover, and Hamiltonicity (that is the existence of a Hamiltonian cycle) to be computed in linear time from a cotree representation of a cograph. One can also use cotrees to determine in linear time whether two cographs are isomorphic.
If H is an induced subgraph of a cograph G, then H is itself a cograph; the cotree for H may be formed by removing some of the leaves from the cotree for G and then suppressing nodes that have only one child. It follows from Kruskal's tree theorem that the relation of being an induced subgraph is a well-quasi-ordering on the cographs (Damaschke 1990). Thus, if a subfamily of the cographs (such as the planar cographs) is closed under induced subgraph operations then it has a finite number of forbidden induced subgraphs. Computationally, this means that testing membership in such a subfamily may be performed in linear time, by using a bottom-up computation on the cotree of a given graph to test whether it contains any of these forbidden subgraphs.
The number of nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a binary tree of height h is at least and at most where is the depth of the tree. The number of leaf nodes in a perfect binary tree can be found using this formula: where is the depth of the tree. The number of nodes in a perfect binary tree can also be found using this formula: where is the number of leaf nodes in the tree. The number of null links (absent children of nodes) in a complete binary tree of nodes is . The number of internal nodes (non-leaf nodes) in a Complete Binary Tree of nodes is . For any non-empty binary tree with leaf nodes and nodes of degree 2, .[5]

Proof:
Let n = the total number of nodes B = number of branches n0, n1, n2 represent the number of nodes with no children, a single child, and two children respectively.


B = n - 1 (since all nodes except the root node come from a single branch) B = n1 + 2*n2 n = n1+ 2*n2 + 1 n = n0 + n1 + n2 n1+ 2*n2 + 1 = n0 + n1 + n2 ==> n0 = n2 + 1
Given a bipartite graph, finding its complete bipartite subgraph Km,n with maximal number of edges mn is an NP-complete problem. A planar graph cannot contain K3,3 as a minor; an outerplanar graph cannot contain K3,2 as a minor (These are not sufficient conditions for planarity and outerplanarity, but necessary). A complete bipartite graph. Kn,n is a Moore graph and a (n,4)-cage. A complete bipartite graph Kn,n or Kn,n+1 is a Turán graph. A complete bipartite graph Km,n has a vertex covering number of min{m,n} and an edge covering number of max{m,n}. A complete bipartite graph Km,n has a maximum independent set of size max{m,n}. The adjacency matrix of a complete bipartite graph Km,n has eigenvalues �?nm), −√(nm) and 0; with multiplicity 1, 1 and n+m�? respectively. The laplacian matrix of a complete bipartite graph Km,n has eigenvalues n+m, n, m, and 0; with multiplicity 1, m�?, n�? and 1 respectively. A complete bipartite graph Km,n has mn�? nm�? spanning trees. A complete bipartite graph Km,n has a maximum matching of size min{m,n}. A complete bipartite graph Kn,n has a proper n-edge-coloring corresponding to a Latin square. The last two results are corollaries of the Marriage Theorem as applied to a k-regular bipartite graph.
The complete graph on n vertices is denoted by Kn.  Some sources claim that the letter K in this notation stands for the German word komplett,[2] but the German name for a complete graph, vollständiger Graph, does not contain the letter K, and other sources state that the notation honors the contributions of Kazimierz Kuratowski to graph theory.[3]
Kn has n(n �?1)/2 edges (a triangular number), and is a regular graph of degree n �?1.  All complete graphs are their own maximal cliques.  They are maximally connected as the only vertex cut which disconnects the graph is the complete set of vertices.  The complement graph of a complete graph is an empty graph.
If the edges of a complete graph are each given an orientation, the resulting directed graph is called a tournament.
The number of matchings of the complete graphs are given by the telephone numbers
1, 1, 2, 4, 10, 26, 76, 232, 764, 2620, 9496, ... (sequence A000085 in OEIS).
These numbers give the largest possible value of the Hosoya index for an n-vertex graph.[4]
Connectedness is preserved by graph homomorphisms.
If G is connected then its line graph L(G) is also connected.
A graph G is 2-edge-connected if and only if it has an orientation that is strongly connected.
Balinski's theorem states that the polytopal graph (1-skeleton) of a -dimensional convex polytope is a -vertex-connected graph.[7] As a partial converse, Steinitz showed that any 3-vertex-connected planar graph is a polytopal graph (Steinitz theorem).
According to a theorem of G. A. Dirac, if a graph is k-connected for k �?#160;2, then for every set of k vertices in the graph there is a cycle that passes through all the vertices in the set.[8][9] The converse is true when k = 2.
A cycle graph is:
Connected 2-regular Eulerian Hamiltonian 2-vertex colorable, if and only if it has an even number of vertices. More generally, a graph is bipartite if and only if it has no odd cycles (Kőnig, 1936). 2-edge colorable, if and only if it has an even number of vertices 3-vertex colorable and 3-edge colorable, for any number of vertices A unit distance graph
In addition:
As cycle graphs can be drawn as regular polygons, the symmetries of an n-cycle are the same as those of a regular polygon with n sides, the dihedral group of order 2n. In particular, there exist symmetries taking any vertex to any other vertex, and any edge to any other edge, so the n-cycle is a symmetric graph.
Two red graphs are duals for the blue one, but they are not isomorphic.
The dual of a plane graph is a plane multigraph - multiple edges.[1] If G is a connected plane graph and if G�?is the dual of G then G is the dual of G�? Since the dual graph depends on a particular embedding, the dual graph of a planar graph is not unique in the sense that the same planar graph can have non-isomorphic dual graphs. In the picture, the red graphs are not isomorphic because the upper one has a vertex with degree 6 (the outer face).
Because of the duality, any result involving counting faces and vertices can be dualized by exchanging them.
The Gray graph is edge-transitive and regular, but not vertex-transitive.
Edge-transitive graphs include any complete bipartite graph , and any symmetric graph, such as the vertices and edges of the cube.[1] Symmetric graphs are also vertex-transitive (if they are connected), but in general edge-transitive graphs need not be vertex-transitive. The Gray graph is an example of a graph which is edge-transitive but not vertex-transitive. All such graphs are bipartite,[1] and hence can be colored with only two colors.
An edge-transitive graph that is also regular, but not vertex-transitive, is called semi-symmetric. The Gray graph again provides an example. Every edge-transitive graph must be bipartite and either semi-symmetric or biregular.[2]
Non-uniform return parameter type Existentially quantified type parameters Local constraints
See also: Glossary of graph theory and Graph property
Two edges of a graph are called adjacent (sometimes coincident) if they share a common vertex. Two arrows of a directed graph are called consecutive if the head of the first one is at the nock (notch end) of the second one. Similarly, two vertices are called adjacent if they share a common edge (consecutive if they are at the notch and at the head of an arrow), in which case the common edge is said to join the two vertices. An edge and a vertex on that edge are called incident.
The graph with only one vertex and no edges is called the trivial graph. A graph with only vertices and no edges is known as an edgeless graph. The graph with no vertices and no edges is sometimes called the null graph or empty graph, but the terminology is not consistent and not all mathematicians allow this object.
In a weighted graph or digraph, each edge is associated with some value, variously called its cost, weight, length or other term depending on the application; such graphs arise in many contexts, for example in optimal routing problems such as the traveling salesman problem.
Normally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called vertex-labeled. However, for many questions it is better to treat vertices as indistinguishable; then the graph may be called unlabeled. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges). The same remarks apply to edges, so graphs with labeled edges are called edge-labeled graphs. Graphs with labels attached to edges or vertices are more generally designated as labeled. Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called unlabeled. (Note that in the literature the term labeled may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)
The automorphism group of the Gray graph is a group of order 1296. It acts transitively on the edges the graph but not on its vertices : there are symmetries taking every edge to any other edge, but not taking every vertex to any other vertex. The vertices that correspond to points of the underlying configuration can only be symmetric to other vertices that correspond to points, and the vertices that correspond to lines can only be symmetric to other vertices that correspond to lines. Therefore, the Gray graph is a semi-symmetric graph, the smallest possible cubic semi-symmetric graph.
The characteristic polynomial of the Gray graph is

The Heawood graph is a toroidal graph; that is, it can be embedded without crossings onto a torus. One embedding of this type places its vertices and edges into three-dimensional Euclidean space as the set of vertices and edges of a nonconvex polyhedron with the topology of a torus, the Szilassi polyhedron.
The graph is named after Percy John Heawood, who in 1890 proved that in every subdivision of the torus into polygons, the polygonal regions can be colored by at most seven colors.[5][6] The Heawood graph forms a subdivision of the torus with seven mutually adjacent regions, showing that this bound is tight.
The Heawood graph is also the Levi graph of the Fano plane, the graph representing incidences between points and lines in that geometry. With this interpretation, the 6-cycles in the Heawood graph correspond to triangles in the Fano plane.
The Heawood graph has crossing number 3, and is the smallest cubic graph with that crossing number (sequence A110507 in OEIS). Including the Heawood graph, there are 8 distinct graphs of order 14 with crossing number 3.
The Heawood graph is a unit distance graph: it can be embedded in the plane such that adjacent vertices are exactly at distance one apart, with no two vertices embedded to the same point and no vertex embedded into a point within an edge. However, the known embeddings of this type lack any of the symmetries that are inherent in the graph.[7]
For a k-ary tree with height h, the upper bound for the maximum number of leaves is . The total number of nodes is , while the height h is

Note : Formula applicable only for number_of_nodes = number of nodes in *complete* k-ary tree
The k-d tree is a binary tree in which every node is a k-dimensional point. Every non-leaf node can be thought of as implicitly generating a splitting hyperplane that divides the space into two parts, known as half-spaces. Points to the left of this hyperplane represent the left subtree of that node and points right of the hyperplane are represented by the right subtree. The hyperplane direction is chosen in the following way: every node in the tree is associated with one of the k-dimensions, with the hyperplane perpendicular to that dimension's axis. So, for example, if for a particular split the "x" axis is chosen, all points in the subtree with a smaller "x" value than the node will appear in the left subtree and all points with larger "x" value will be in the right subtree. In such a case, the hyperplane would be set by the x-value of the point, and its normal would be the unit x-axis.[1]
A square grid graph is a Cartesian product of graphs, namely, of two path graphs with n - 1 and m - 1 edges.[2] Since a path graph is a median graph, the latter fact implies that the square grid graph is also a median graph. All grid graphs are bipartite.
A path graph may also be considered to be a grid graph on the grid n times 1. A 2x2 grid graph is a 4-cycle.[2]
Properties of a graph G that depend only on adjacency between edges may be translated into equivalent properties in L(G) that depend on adjacency between vertices. For instance, a matching in G is a set of edges no two of which are adjacent, and corresponds to a set of vertices in L(G) no two of which are adjacent, that is, an independent set.
Thus,
The line graph of a connected graph is connected. If G is connected, it contains a path connecting any two of its edges, which translates into a path in L(G) containing any two of the vertices of L(G). However, a graph G that has some isolated vertices, and is therefore disconnected, may nevertheless have a connected line graph. A maximum independent set in a line graph corresponds to maximum matching in the original graph. Since maximum matchings may be found in polynomial time, so may the maximum independent sets of line graphs, despite the hardness of the maximum independent set problem for more general families of graphs. The edge chromatic number of a graph G is equal to the vertex chromatic number of its line graph L(G). The line graph of an edge-transitive graph is vertex-transitive. If a graph G has an Euler cycle, that is, if G is connected and has an even number of edges at each vertex, then the line graph of G is Hamiltonian. (However, not all Hamiltonian cycles in line graphs come from Euler cycles in this way.) Line graphs are claw-free graphs, graphs without an induced subgraph in the form of a three-leaf tree. The line graphs of trees are exactly the claw-free block graph.
The automorphism group of the Ljubljana graph is a group of order 168. It acts transitively on the edges the graph but not on its vertices : there are symmetries taking every edge to any other edge, but not taking every vertex to any other vertex. Therefore, the Ljubljana graph is a semi-symmetric graph, the third smallest possible cubic semi-symmetric graph after the Gray graph on 54 vertices and the Iofinova-Ivanov graph on 110 vertices.[3]
The characteristic polynomial of the Ljubljana graph is

The Cartesian product of graphs forms a median graph from two smaller median graphs.
The Cartesian product of any two median graphs is another median graph.  Medians in the product graph may be computed by independently finding the medians in the two factors, just as medians in grid graphs may be computed by independently finding the median in each linear dimension.
The windex of a graph measures the amount of lookahead needed to optimally solve a problem in which one is given a sequence of graph vertices si, and must find as output another sequence of vertices ti minimizing the sum of the distances d(si,ti) and d(ti �?#160;1,ti).  Median graphs are exactly the graphs that have windex 2.  In a median graph, the optimal choice is to set ti = m(ti �?#160;1,si,si + 1).[1]
The property of having a unique median is also called the unique Steiner point property.[1]  An optimal Steiner tree for any three vertices a, b, and c in a median graph may be found as the union of three shortest paths, from a, b, and c to m(a,b,c).  Bandelt & Barthélémy (1984) study more generally the problem of finding the vertex minimizing the sum of distances to each of a given set of vertices, and show that it has a unique solution for any odd number of vertices in a median graph.  They also show that this median of a set S of vertices in a median graph satisfies the Condorcet criterion for the winner of an election: compared to any other vertex, it is closer to a majority of the vertices in S.
As with partial cubes more generally, any median graph with n vertices has at most (n/2) log2 n edges.  However, the number of edges cannot be too small: Klavžar, Mulder & Škrekovski (1998) prove that in any median graph the inequality 2n �?#160;m �?#160;k �?#160;2 holds, where m is the number of edges and k is the dimension of the hypercube that the graph is a retract of.  This inequality is an equality if and only if the median graph contains no cubes.  This is a consequence of another identity for median graphs: the Euler characteristic Σ (-1)dim(Q) is always equal to one, where the sum is taken over all hypercube subgraphs Q of the given median graph.[24]
The only regular median graphs are the hypercubes.[25]
For any cycle C in the graph, if the weight of an edge e of C is larger than the weights of all other edges of C, then this edge cannot belong to an MST. Assuming the contrary, i.e. that e belongs to an MST T1, then deleting e will break T1 into two subtrees with the two ends of e in different subtrees. The remainder of C reconnects the subtrees, hence there is an edge f of C with ends in different subtrees, i.e., it reconnects the subtrees into a tree T2 with weight less than that of T1, because the weight of f is less than the weight of e.
For any cycle C in the graph, if the weight of an edge e of C is larger than the weights of all other edges of C, then this edge cannot belong to an MST. Assuming the contrary, i.e. that e belongs to an MST T1, then deleting e will break T1 into two subtrees with the two ends of e in different subtrees. The remainder of C reconnects the subtrees, hence there is an edge f of C with ends in different subtrees, i.e., it reconnects the subtrees into a tree T2 with weight less than that of T1, because the weight of f is less than the weight of e.
A symmetric embedding of the Nauru graph on a genus-4 surface, with six dodecagonal faces.
The Nauru graph has two different embeddings as generalized regular polyhedra: topological surfaces partitioned into edges, vertices, and faces in such a way that there is a symmetry taking any flag (an incident triple of a vertex, edge, and face) into any other flag.[7]
One of these two embeddings forms a torus, so the Nauru graph is a toroidal graph: it consists of 12 hexagonal faces together with the 24 vertices and 36 edges of the Nauru graph. The dual graph of this embedding is a symmetric 6-regular graph with 12 vertices and 36 edges.
The other symmetric embedding of the Nauru graph has six dodecagonal faces, and forms a surface of genus 4. Its dual is not a simple graph, since each face shares three edges with four other faces, but a multigraph. This dual can be formed from the graph of a regular octahedron by replacing each edge with a bundle of three parallel edges.
The set of faces of any one of these two embeddings is the set of Petrie polygons of the other embedding.
The automorphism group of the Pappus graph is a group of order 216. It acts transitively on the vertices, on the edges and on the arcs of the graph. Therefore the Pappus graph is a symmetric graph. It has automorphisms that take any vertex to any other vertex and any edge to any other edge. According to the Foster census, the Pappus graph, referenced as F018A, is the only cubic symmetric graph on 18 vertices.[4][5]
The characteristic polynomial of the Pappus graph is . It is the only graph with this characteristic polynomial, making it a graph determined by its spectrum.
The Petersen graph:
is 3-connected and hence 3-edge-connected and bridgeless. See the glossary. has independence number 4 and is 3-partite. See the glossary. is cubic, has domination number 3, and has a perfect matching and a 2-factor. is the smallest cubic graph of girth 5. (It is the unique -cage. In fact, since it has only 10 vertices, it is the unique -Moore graph.)[9] has radius 2 and diameter 2. It is the largest cubic graph with diameter 2.[10] has graph spectrum �?, �?, �?, �?, 1, 1, 1, 1, 1, 3. has 2000 spanning trees, the most of any 10-vertex cubic graph.[11] has chromatic polynomial [12] has characteristic polynomial , making it an integral graph—a graph whose spectrum consists entirely of integers.
The theory of random graphs studies typical properties of random graphs, those that hold with high probability for graphs drawn from a particular distribution. For example, we might ask for a given value of and what the probability is that is connected. In studying such questions, researchers often concentrate on the asymptotic behavior of random graphs—the values that various probabilities converge to as grows very large. Percolation theory characterizes the connectedness of random graphs, especially infinitely large ones.
Percolation is related to the robustness of the graph (called also network). Given a random graph of n nodes and an average degree . Next we remove randomly a fraction of nodes and leave only a fraction . There exists a critical percolation threshold below which the network becomes fragmented while above a giant connected component exists [1] [3] [4] [5] [6] .[7]
(threshold functions, evolution of G~)
Random graphs are widely used in the probabilistic method, where one tries to prove the existence of graphs with certain properties. The existence of a property on a random graph can often imply, via the famous Szemerédi regularity lemma, the existence of that property on almost all graphs.
As with more general random graphs, it is possible to prove that certain properties of random r-regular graphs hold almost surely. In particular, a random r-regular graph is almost surely r-connected.[2] In other words, although r-regular graphs with connectivity less than r exist, the probability of selecting such a graph tends to 0 as n increases.
If > 0 is a positive constant, and d is the least integer satisfying

then, almost surely, a random r-regular graph has diameter at most d. There is also a (more complex) lower bound on the diameter of r-regular graphs, so that almost all r-regular graphs (of the same size) have almost the same diameter.[3]
The distribution of the number of short cycles is also known: for fixed m �?3, let Y3,Y4,�?Ym be the number of cycles of lengths up to m. Then the Yi are asymptotically independent Poisson random variables with means[4]

An example of a 1-dimensional range tree.
A range tree on a set of 1-dimensional points is a balanced binary search tree on those points. The points stored in the tree are stored in the leaves of the tree; each internal node stores the largest value contained in its left subtree. A range tree on a set of points in d-dimensions is a recursively defined multi-level binary search tree. Each level of the data structure is a binary search tree on one of the d-dimensions. The first level is a binary search tree on the first of the d-coordinates. Each vertex v of this tree contains an associated structure that is a (d�?)-dimensional range tree on the last (d�?)-coordinates of the points stored in the subtree of v.
The number of size-n recursive trees is given by

Hence the exponential generating function T(z) of the sequence Tn is given by

Combinatorically a recursive tree can be interpreted as a root followed by an unordered sequence of recursive trees. Let F denote the family of recursive trees.

where denotes the node labeled by 1, × the Cartesian product and the partition product for labeled objects.
By translation of the formal description one obtains the differential equation for T(z)

with T(0) = 0.
An example of a red–black tree
In addition to the requirements imposed on a binary search trees, with red–black trees:
A node is either red or black. The root is black. (This rule is sometimes omitted. Since the root can always be changed from red to black, but not necessarily vice-versa, this rule has little effect on analysis.) All leaves (NIL) are black. (All leaves are same color as the root.) Both children of every red node are black. Every simple path from a given node to any of its descendant leaves contains the same number of black nodes.
These constraints enforce a critical property of red–black trees: that the path from the root to the furthest leaf is no more than twice as long as the path from the root to the nearest leaf. The result is that the tree is roughly height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height of the tree, this theoretical upper bound on the height allows red–black trees to be efficient in the worst case, unlike ordinary binary search trees. Red–black trees are in general not weight-balanced,[4] that is sibling nodes can have hugely differing numbers of descendants.
To see why this is guaranteed, it suffices to consider the effect of properties 4 and 5 together. For a red–black tree T, let B be the number of black nodes in property 5. Therefore the shortest possible path from the root of T to any leaf consists of B black nodes. Longer possible paths may be constructed by inserting red nodes. However, property 4 makes it impossible to insert more than one consecutive red node. Therefore the longest possible path consists of 2B nodes, alternating black and red.
The shortest possible path has all black nodes, and the longest possible path alternates between red and black nodes. Since all maximal paths have the same number of black nodes, by property 5, this shows that no path is more than twice as long as any other path.
In many of the presentations of tree data structures, it is possible for a node to have only one child, and leaf nodes contain data. It is possible to present red–black trees in this paradigm, but it changes several of the properties and complicates the algorithms. For this reason, this article uses "null leaves", which contain no data and merely serve to indicate where the tree ends, as shown above. These nodes are often omitted in drawings, resulting in a tree that seems to contradict the above principles, but in fact does not. A consequence of this is that all internal (non-leaf) nodes have two children, although one or both of those children may be null leaves. Property 5 ensures that a red node must have either two black null leaves or two black non-leaves as children. For a black node with one null leaf child and one non-null-leaf child, properties 3, 4 and 5 ensure that the non-null-leaf child must be a red node with two black null leaves as children.
Some explain a red–black tree as a binary search tree whose edges, instead of nodes, are colored in red or black, but this does not make any difference. The color of a node in this article's terminology corresponds to the color of the edge connecting the node to its parent, except that the root node is always black (property 2) whereas the corresponding edge does not exist.
Let A be the adjacency matrix of a graph. Then the graph is regular if and only if is an eigenvector of A.[2] Its eigenvalue will be the constant degree of the graph. Eigenvectors corresponding to other eigenvalues are orthogonal to , so for such eigenvectors , we have .
A regular graph of degree k is connected if and only if the eigenvalue k has multiplicity one.[2]
There is also a criterion for regular and connected graphs : a graph is connected and regular if and only if the matrix J, with , is in the adjacency algebra of the graph (meaning it is a linear combination of powers of A).[citation needed]
Let G be a k-regular graph with diameter D and eigenvalues of adjacency matrix . If G is not bipartite

where .[citation needed]
This section describes the structure of a segment tree in a one-dimensional space.
Let S be a set of intervals, or segments. Let p1, p2, ..., pm be the list of distinct interval endpoints, sorted from left to right. Consider the partitioning of the real line induced by those points. The regions of this partitioning are called elementary intervals. Thus, the elementary intervals are, from left to right:

That is, the list of elementary intervals consists of open intervals between two consecutive endpoints pi and pi+1, alternated with closed intervals consisting of a single endpoint. Single points are treated themselves as intervals because the answer to a query is not necessarily the same at the interior of an elementary interval and its endpoints.[2]
Graphic example of the structure of the segment tree. This instance is built for the segments shown at the bottom.
Given a set I of intervals, or segments, a segment tree T for I is structured as follows:
T is a binary tree. Its leaves correspond to the elementary intervals induced by the endpoints in I, in an ordered way: the leftmost leaf corresponds to the leftmost interval, and so on. The elementary interval corresponding to a leaf v is denoted Int(v). The internal nodes of T correspond to intervals that are the union of elementary intervals: the interval Int(N) corresponding to node N is the union of the intervals corresponding to the leaves of the tree rooted at N. That implies that Int(N) is the union of the intervals of its two children. Each node or leaf v in T stores the interval Int(v) and a set of intervals, in some data structure. This canonical subset of node v contains the intervals [x, x�?/i>] from I such that [x, x�?/i>] contains Int(v) and does not contain Int(parent(v)). That is, each segment in I stores the segments that span through its interval, but do not span through the interval of its parent.[3]
Every series-parallel graph has treewidth at most 2 and branchwidth at most 2. Indeed, a graph has treewidth at most 2 if and only if it has branchwidth at most 2, if and only if every biconnected component is a series-parallel graph.[3][4] The maximal series-parallel graphs, graphs to which no additional edges can be added without destroying their series-parallel structure, are exactly the 2-trees. Graphs of treewidth at most 2 have an explicit forbidden minor characterization, implying that a graph is series-parallel if and only if its biconnected components are linked in a path and it excludes the complete graph K4 as a minor.
Series parallel graphs may also be characterized by their ear decompositions.[1]
See also: Glossary of graph theory and Graph property
Two edges of a graph are called adjacent (sometimes coincident) if they share a common vertex. Two arrows of a directed graph are called consecutive if the head of the first one is at the nock (notch end) of the second one. Similarly, two vertices are called adjacent if they share a common edge (consecutive if they are at the notch and at the head of an arrow), in which case the common edge is said to join the two vertices. An edge and a vertex on that edge are called incident.
The graph with only one vertex and no edges is called the trivial graph. A graph with only vertices and no edges is known as an edgeless graph. The graph with no vertices and no edges is sometimes called the null graph or empty graph, but the terminology is not consistent and not all mathematicians allow this object.
In a weighted graph or digraph, each edge is associated with some value, variously called its cost, weight, length or other term depending on the application; such graphs arise in many contexts, for example in optimal routing problems such as the traveling salesman problem.
Normally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called vertex-labeled. However, for many questions it is better to treat vertices as indistinguishable; then the graph may be called unlabeled. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges). The same remarks apply to edges, so graphs with labeled edges are called edge-labeled graphs. Graphs with labels attached to edges or vertices are more generally designated as labeled. Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called unlabeled. (Note that in the literature the term labeled may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)
A skip list is built in layers. The bottom layer is an ordinary ordered linked list. Each higher layer acts as an "express lane" for the lists below, where an element in layer i appears in layer i+1 with some fixed probability p (two commonly used values for p are 1/2 or 1/4). On average, each element appears in 1/(1-p) lists, and the tallest element (usually a special head element at the front of the skip list) in lists.
A search for a target element begins at the head element in the top list, and proceeds horizontally until the current element is greater than or equal to the target. If the current element is equal to the target, it has been found. If the current element is greater than the target, or the search reaches the end of the linked list, the procedure is repeated after returning to the previous element and dropping down vertically to the next lower list. The expected number of steps in each linked list is at most 1/p, which can be seen by tracing the search path backwards from the target until reaching an element that appears in the next higher list or reaching the beginning of the current list. Therefore, the total expected cost of a search is which is when p is a constant. By choosing different values of p, it is possible to trade search costs against storage costs.
This is one useful application of stacks. Consider that a freight train has n railroad cars, each to be left at different station. They're numbered 1 through n and freight train visits these stations in order n through 1. Obviously, the railroad cars are labeled by their destination. To facilitate removal of the cars from the train, we must rearrange them in ascending order of their number (i.e. 1 through n). When cars are in this order, they can be detached at each station. We rearrange cars at a shunting yard that has input track, output track and k holding tracks between input & output tracks (i.e. holding track).
The four parameters in an srg(v,k,λ,μ) are not independent and must obey the following relation:

The above relation can be derived very easily through a counting argument as follows:
Imagine the nodes of the graph to lie in three levels. Pick any node as the root node, in Level 0. Then its k neighbor nodes lie in Level 1, and all other nodes lie in Level 2. Nodes in Level 1 are directly connected to the root, hence they must have other neighbors in common with the root, and these common neighbors must also be in Level 1. Since each node has degree k, there are edges remaining for each Level 1 node to connect to nodes in Level 2. Nodes in Level 2 are not directly connected to the root, hence they must have common neighbors with the root, and these common neighbors must all be in Level 1. Therefore nodes in Level 1 are connected to each node in Level 2 and each of the k nodes in Level 1 is connected to nodes in Level 2. Therefore the number of nodes in Level 2 is . The total number of nodes across all three levels is therefore and by rearranging we get . QED
Let I denote the identity matrix (of order v) and let J denote the matrix whose entries all equal 1. The adjacency matrix A of a strongly regular graph satisfies these properties : (This is a trivial restatement of the vertex degree requirement). (The first term gives the number of 2-step paths from each vertex to all vertices. For the vertex pairs directly connected by an edge, the equation reduces to the number of such 2-step paths being equal to . For the vertex pairs not directly connected by an edge, the equation reduces to the number of such 2-step paths being equal to . For the trivial self-pairs, the equation reduces to the degree being equal to k).
The graph has exactly three eigenvalues: whose multiplicity is 1 whose multiplicity is whose multiplicity is
Strongly regular graphs for which are called conference graphs because of their connection with symmetric conference matrices. Their parameters reduce to .
Strongly regular graphs for which have integer eigenvalues with unequal multiplicities.
The complement of an srg(v,k,λ,μ) is also strongly regular. It is an srg(v, v−k�?, v�?�?k+μ, v�?k+λ).
The vertex-connectivity of a symmetric graph is always equal to the degree d.[3] In contrast, for vertex-transitive graphs in general, the vertex-connectivity is bounded below by 2(d+1)/3.[2]
A t-transitive graph of degree 3 or more has girth at least 2(t�?). However, there are no finite t-transitive graphs of degree 3 or more for t �?8. In the case of the degree being exactly 3 (cubic symmetric graphs), there are none for t �?6.
Maximum number of nodes
�?Let be height of a ternary tree.
�?Let be the maximum number of nodes in a ternary tree of height h
h M(h) 0 1 1 4 2 13 3 40
�?
�?Every tree of height h has at most nodes.
If a node occupies TREE [k}, then its Left Child is stored in tree . Mid Child is stored in TREE . Right Child is stored in TREE .
See also: Glossary of graph theory and Graph property
Two edges of a graph are called adjacent (sometimes coincident) if they share a common vertex. Two arrows of a directed graph are called consecutive if the head of the first one is at the nock (notch end) of the second one. Similarly, two vertices are called adjacent if they share a common edge (consecutive if they are at the notch and at the head of an arrow), in which case the common edge is said to join the two vertices. An edge and a vertex on that edge are called incident.
The graph with only one vertex and no edges is called the trivial graph. A graph with only vertices and no edges is known as an edgeless graph. The graph with no vertices and no edges is sometimes called the null graph or empty graph, but the terminology is not consistent and not all mathematicians allow this object.
In a weighted graph or digraph, each edge is associated with some value, variously called its cost, weight, length or other term depending on the application; such graphs arise in many contexts, for example in optimal routing problems such as the traveling salesman problem.
Normally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called vertex-labeled. However, for many questions it is better to treat vertices as indistinguishable; then the graph may be called unlabeled. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges). The same remarks apply to edges, so graphs with labeled edges are called edge-labeled graphs. Graphs with labels attached to edges or vertices are more generally designated as labeled. Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called unlabeled. (Note that in the literature the term labeled may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)
Given only one list item, one cannot immediately obtain the addresses of the other elements of the list. Two XOR operations suffice to do the traversal from one item to the next, the same instructions sufficing in both cases. Consider a list with items {…B C D…} and with R1 and R2 being registers containing, respectively, the address of the current (say C) list item and a work register containing the XOR of the current address with the previous address (say C⊕D). Cast as System/360 instructions:
X  R2,Link    R2 <- C⊕D �?B⊕D (i.e. B⊕C, "Link" being the link field
                                  in the current record, containing B⊕D)
XR R1,R2      R1 <- C �?B⊕C    (i.e. B, voilà: the next record)
End of list is signified by imagining a list item at address zero placed adjacent to an end point, as in {0 A B C…}. The link field at A would be 0⊕B. An additional instruction is needed in the above sequence after the two XOR operations to detect a zero result in developing the address of the current item, A list end point can be made reflective by making the link pointer be zero. A zero pointer is a mirror. (The XOR of the left and right neighbor addresses, being the same, is zero.)
