A graph with six nodes.
The diagram at right is a graphic representation of the following graph:
V = {1, 2, 3, 4, 5, 6} E = {{1, 2}, {1, 5}, {2, 3}, {2, 5}, {3, 4}, {4, 5}, {4, 6}}.
In category theory a small category can be represented by a directed multigraph in which the objects of the category represented as vertices and the morphisms as directed edges. Then, the functors between categories induce some, but not necessarily all, of the digraph morphisms of the graph. In computer science, directed graphs are used to represent knowledge (e.g., Conceptual graph), finite state machines, and many other discrete structures. A binary relation R on a set X defines a directed graph. An element x of X is a direct predecessor of an element y of X iff xRy.
To insert the value "25" into this 2�?�? tree:

Begin at the root (10, 20) and descend towards the rightmost child (22, 24, 29). (Its interval (20, �? contains 25.) Node (22, 24, 29) is a 4-node, so its middle element 24 is pushed up into the parent node.

The remaining 3-node (22, 29) is split into a pair of 2-nodes (22) and (29). Ascend back into the new parent (10, 20, 24). Descend towards the rightmost child (29). (Its interval (24, �? contains 25.)

Node (29) has no leftmost child. (The child for interval (�? 29) is empty.) Stop here and insert value 25 into this node.

Some common ADTs, which have proved useful in a great variety of applications, are
Container Deque List Map Multimap Multiset Priority queue Queue Set Stack String Tree
Each of these ADTs may be defined in many ways and variants, not necessarily equivalent. For example, a stack ADT may or may not have a count operation that tells how many items have been pushed and not yet popped. This choice makes a difference not only for its clients but also for the implementation.
Suppose that the set of loans made by a library is to be represented in a data structure. Each book in a library may be checked out only by a single library patron at a time. However, a single patron may be able to check out multiple books. Therefore, the information about which books are checked out to which patrons may be represented by an associative array, in which the books are the keys and the patrons are the values. For instance (using the notation from Python in which a binding is represented by placing a colon between the key and the value), the current checkouts may be represented by an associative array
{
    "Great Expectations": "John",
    "Pride and Prejudice": "Alice",
    "Wuthering Heights": "Alice"
}
A lookup operation with the key "Great Expectations" in this array would return the name of the person who checked out that book, John. If John returns his book, that would cause a deletion operation in the associative array, and if Pat checks out another book, that would cause an insertion operation, leading to a different state:
{
    "Pride and Prejudice": "Alice",
    "The Brothers Karamazov": "Pat",
    "Wuthering Heights": "Alice"
}
In this new state, the same lookup as before, with the key "Great Expectations", would raise an exception, because this key is no longer present in the array.
If every circle belongs to B, and there are no half-edges, Ω is balanced. A balanced biased graph is (for most purposes) essentially the same as an ordinary graph.
If B is empty, Ω is called contrabalanced. Contrabalanced biased graphs are related to bicircular matroids.
If B consists of the circles of even length, Ω is called antibalanced and is the biased graph obtained from an all-negative signed graph.
The linear class B is additive, that is, closed under repeated symmetric difference (when the result is a circle), if and only if B is the class of positive circles of a signed graph.
Ω may have underlying graph that is a cycle of length n �?3 with all edges doubled. Call this a biased 2Cn . Such biased graphs in which no digon (circle of length 2) is balanced lead to spikes and swirls (see Matroids, below).
Some kinds of biased graph are obtained from gain graphs or are generalizations of special kinds of gain graph. The latter include biased expansion graphs, which generalize group expansion graphs.
[1]
The left figure below shows a binary decision tree (the reduction rules are not applied), and a truth table, each representing the function f (x1, x2, x3). In the tree on the left, the value of the function can be determined for a given variable assignment by following a path down the graph to a terminal. In the figures below, dotted lines represent edges to a low child, while solid lines represent edges to a high child. Therefore, to find (x1=0, x2=1, x3=1), begin at x1, traverse down the dotted line to x2 (since x1 has an assignment to 0), then down two solid lines (since x2 and x3 each have an assignment to one). This leads to the terminal 1, which is the value of f (x1=0, x2=1, x3=1).
The binary decision tree of the left figure can be transformed into a binary decision diagram by maximally reducing it according to the two reduction rules. The resulting BDD is shown in the right figure.
Binary decision tree and truth table for the function BDD for the function f
When modelling relations between two different classes of objects, bipartite graphs very often arise naturally. For instance, a graph of football players and clubs, with an edge between a player and a club if the player has played for that club, is a natural example of an affiliation network, a type of bipartite graph used in social network analysis.[6]
Another example where bipartite graphs appear naturally is in the (NP-complete) railway optimization problem, in which the input is a schedule of trains and their stops, and the goal is to find as small a set of train stations as possible such that every train visits at least one of the chosen stations. This problem can be modeled as a dominating set problem in a bipartite graph that has a vertex for each train and each station and an edge for each pair of a station and a train that stops at that station.[7]
More abstract examples include the following:
Every tree is bipartite.[4] Cycle graphs with an even number of vertices are bipartite.[4] Every planar graph whose faces all have even length is bipartite.[8] Special cases of this are grid graphs and squaregraphs, in which every inner face consists of 4 edges and every inner vertex has four or more neighbors.[9] The complete bipartite graph on n and m vertices, denoted by Kn,m is the bipartite graph G = (V, U, E), where V and U are disjoint sets of size n and m, respectively, and E connects every vertex in V with all vertices in U. It follows that Kn,m has nm edges.[10] Closely related to the complete bipartite graphs are the crown graphs, formed from complete bipartite graphs by removing the edges of a perfect matching.[11] Hypercube graphs, partial cubes, and median graphs are bipartite. In these graphs, the vertices may be labeled by bitvectors, in such a way that two vertices are adjacent if and only if the corresponding bitvectors differ in a single position. A bipartition may be formed by separating the vertices whose bitvectors have an even number of ones from the vertices with an odd number of ones. Trees and squaregraphs form examples of median graphs, and every median graph is a partial cube.[12]
Google BigTable uses Bloom filters to reduce the disk lookups for non-existent rows or columns. Avoiding costly disk lookups considerably increases the performance of a database query operation.[4]
The Squid Web Proxy Cache uses Bloom filters for cache digests.[5]
The Venti archival storage system uses Bloom filters to detect previously stored data.[6]
The SPIN model checker uses Bloom filters to track the reachable state space for large verification problems.[7]
Google BigTable uses Bloom filters to reduce the disk lookups for non-existent rows or columns. Avoiding costly disk lookups considerably increases the performance of a database query operation.[4]
The Squid Web Proxy Cache uses Bloom filters for cache digests.[5]
The Venti archival storage system uses Bloom filters to detect previously stored data.[6]
The SPIN model checker uses Bloom filters to track the reachable state space for large verification problems.[7]
Suppose that is the infinite cyclic group and the set S consists of the standard generator 1 and its inverse (�? in the additive notation) then the Cayley graph is an infinite chain.
Similarly, if is the finite cyclic group of order n and the set S consists of two elements, the standard generator of G and its inverse, then the Cayley graph is the cycle .
The Cayley graph of the direct product of groups is the cartesian product of the corresponding Cayley graphs[citation needed]. Thus the Cayley graph of the abelian group with the set of generators consisting of four elements is the infinite grid on the plane , while for the direct product with similar generators the Cayley graph is the finite grid on a torus.
Cayley graph of the dihedral group Dih4 on two generators a and b
On two generators of Dih4, which are both self-inverse
A Cayley graph of the dihedral group D4 on two generators a and b is depicted to the left. Red arrows represent left-multiplication by element a. Since element b is self-inverse, the blue lines which represent left-multiplication by element b are undirected. Therefore the graph is mixed: it has eight vertices, eight arrows, and four edges. The Cayley table of the group D4 can be derived from the group presentation


A different Cayley graph of Dih4 is shown on the right. b is still the horizontal reflection and represented by blue lines; c is a diagonal reflection and represented by green lines. As both reflections are self-inverse the Cayley graph on the right is completely undirected. This graph corresponds to the presentation


The Cayley graph of the free group on two generators a, b corresponding to the set S = {a, b, a�?, b�?} is depicted at the top of the article, and e represents the identity element. Travelling along an edge to the right represents right multiplication by a, while travelling along an edge upward corresponds to the multiplication by b. Since the free group has no relations, the Cayley graph has no cycles. This Cayley graph is a key ingredient in the proof of the Banach–Tarski paradox.
Part of a Cayley graph of the Heisenberg group. (The coloring is only for visual aid.)
A Cayley graph of the discrete Heisenberg group
is depicted to the right. The generators used in the picture are the three matrices X, Y, Z given by the three permutations of 1, 0, 0 for the entries x, y, z. They satisfy the relations , which can also be read off from the picture. This is a non-commutative infinite group, and despite being three-dimensional in some sense, the Cayley graph has four-dimensional volume growth.
Several graph-theoretic concepts are related to each other via complement graphs:
The complement of an edgeless graph is a complete graph and vice versa. An independent set in a graph is a clique in the complement graph and vice versa. The complement of any triangle-free graph is a claw-free graph. A self-complementary graph is a graph that is isomorphic to its own complement. Cographs are defined as the graphs that can be built up from disjoint union and complementation operations, and form a self-complementary family of graphs: the complement of any cograph is another (possibly different) cograph.
The star graphs S3, S4, S5 and S6.
The utility graph K3,3
For any k, K1,k is called a star. All complete bipartite graphs which are trees are stars. The graph K1,3 is called a claw, and is used to define the claw-free graphs. The graph K3,3 is called the utility graph. This usage comes from a standard mathematical puzzle in which three utilities must each be connected to three buildings; it is impossible to solve without crossings due to the nonplanarity of K3,3.
Complete graphs on n vertices, for n between 1 and 12, are shown below along with the numbers of edges:
K1: 0 K2: 1 K3: 3 K4: 6 K5: 10 K6: 15 K7: 21 K8: 28 K9: 36 K10: 45 K11: 55 K12: 66
The vertex- and edge-connectivities of a disconnected graph are both 0. 1-connectedness is synonymous with connectedness. The complete graph on n vertices has edge-connectivity equal to n �?1. Every other simple graph on n vertices has strictly smaller edge-connectivity. In a tree, the local edge-connectivity between every pair of vertices is 1.
Containers are divided in the Standard Template Library into associative containers and standard sequence containers. Besides this two types, so-called container adaptors exist. Data structures that implement containers include arrays, lists, maps, queues, sets, stacks, tables, trees, and vectors.
Complete graphs are distance regular with diameter 1 and degree v�?. Cycles C2d+1 of odd length are distance regular with k = 2 and diameter d. The intersection numbers ai = 0, bi = 1, and ci = 1, except for the usual special cases (see above) and cd = 2. All Moore graphs, in particular the Petersen graph and the Hoffman-Singleton graph, are distance regular. Strongly regular graphs are distance regular. The odd graphs are distance regular.
The Gray graph is edge-transitive and regular, but not vertex-transitive.
Edge-transitive graphs include any complete bipartite graph , and any symmetric graph, such as the vertices and edges of the cube.[1] Symmetric graphs are also vertex-transitive (if they are connected), but in general edge-transitive graphs need not be vertex-transitive. The Gray graph is an example of a graph which is edge-transitive but not vertex-transitive. All such graphs are bipartite,[1] and hence can be colored with only two colors.
An edge-transitive graph that is also regular, but not vertex-transitive, is called semi-symmetric. The Gray graph again provides an example. Every edge-transitive graph must be bipartite and either semi-symmetric or biregular.[2]
A graph with six nodes.
The diagram at right is a graphic representation of the following graph:
V = {1, 2, 3, 4, 5, 6} E = {{1, 2}, {1, 5}, {2, 3}, {2, 5}, {3, 4}, {4, 5}, {4, 6}}.
In category theory a small category can be represented by a directed multigraph in which the objects of the category represented as vertices and the morphisms as directed edges. Then, the functors between categories induce some, but not necessarily all, of the digraph morphisms of the graph. In computer science, directed graphs are used to represent knowledge (e.g., Conceptual graph), finite state machines, and many other discrete structures. A binary relation R on a set X defines a directed graph. An element x of X is a direct predecessor of an element y of X iff xRy.
The graph Q0 consists of a single vertex, while Q1 is the complete graph on two vertices and Q2 is a cycle of length 4.
The graph Q3 is the 1-skeleton of a cube, a planar graph with eight vertices and twelve edges.
The graph Q4 is the Levi graph of the Möbius configuration.
Consider the hypergraph with edges

and

Then clearly and are isomorphic (with , etc.), but they are not strongly isomorphic. So, for example, in , vertex meets edges 1, 4 and 6, so that,

In graph , there does not exist any vertex that meets edges 1, 4 and 6:

In this example, and are equivalent, , and the duals are strongly isomorphic: .
The complete graph Kn is integral for all n. The edgeless graph is integral for all n. Among the cubic symmetric graphs the utility graph, the Petersen graph, the Nauru graph and the Desargues graph are integral. The Higman–Sims graph, the Hall–Janko graph, the Clebsch graph, the Hoffman–Singleton graph, the Shrikhande graph and the Hoffman graph are integral.
To search for an interval, you walk the tree, omitting those branches which can't contain what you're looking for. The simple case is looking for a point:
// Search for all intervals which contain "p", starting with the
 // node "n" and adding matching intervals to the list "result"
 public void search(IntervalNode n, Point p, List<Interval> result) {
     // Don't search nodes that don't exist
     if (n == null)
         return;
 
     // If p is to the right of the rightmost point of any interval
     // in this node and all children, there won't be any matches.
     if (p.compareTo(n.getValue()) > 0)
         return;
 
     // Search left children
     if (n.getLeft() != null)
         search(IntervalNode (n.getLeft()), p, result);
 
     // Check this node
     if (n.getKey().contains(p))
         result.add(n.getKey());
 
     // If p is to the left of the start of this interval,
     // then it can't be in any child to the right.
     if (p.compareTo(n.getKey().getStart()) < 0)
         return;
 
     // Otherwise, search right children
     if (n.getRight() != null)
         search(IntervalNode (n.getRight()), p, result);
 }
The code to search for an interval is exactly the same except for the check in the middle:
// Check this node
 if (n.getKey().overlapsWith(i))
     result.add (n.getKey());
overlapsWith() is defined as:
public boolean overlapsWith(Interval other) {
     return start.compareTo(other.getEnd()) <= 0 &&
            end.compareTo(other.getStart()) >= 0;
 }
This is an example of the node class used to store integers in a Java implementation of a linked list.
public class IntNode {
     public int value;
     public IntNode link;
     public IntNode(int v) { value = v; }
}
Suppose you wanted to create a linked list of families and their members. Using internal storage, the structure might look like the following:
record member { // member of a family
     member next;
     string firstName;
     integer age;
 }
 record family { // the family itself
     family next;
     string lastName;
     string address;
     member members // head of list of members of this family
 }
To print a complete list of families and their members using internal storage, we could write:
aFamily := Families // start at head of families list
 while aFamily �?null // loop through list of families
     print information about family
     aMember := aFamily.members // get head of list of this family's members
     while aMember �?null // loop through list of members
         print information about member
         aMember := aMember.next
aFamily := aFamily.next
Using external storage, we would create the following structures:
record node { // generic link structure
     node next;
     pointer data // generic pointer for data at node
 }
 record member { // structure for family member
     string firstName;
     integer age
 }
 record family { // structure for family
     string lastName;
     string address;
     node members // head of list of members of this family
 }
To print a complete list of families and their members using external storage, we could write:
famNode := Families // start at head of families list
 while famNode �?null // loop through list of families
     aFamily := (family) famNode.data // extract family from node
     print information about family
     memNode := aFamily.members // get list of family members
     while memNode �?null // loop through list of members
         aMember := (member)memNode.data // extract member from node
         print information about member
         memNode := memNode.next
famNode := famNode.next
Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type. This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure (node), and this language does not have parametric types.
As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.
The median of three vertices in a tree, showing the subtree formed by the union of shortest paths between the vertices.
Any tree is a median graph.[4]  To see this, observe that in a tree, the union of the three shortest paths between any three vertices a, b, and c is either itself a path, or a subtree formed by three paths meeting at a single central node with degree three.  If the union of the three paths is itself a path, the median m(a,b,c) is equal to one of a, b, or c, whichever of these three vertices is between the other two in the path.  If the subtree formed by the union of the three paths is not a path, the median of the three vertices is the central degree-three node of the subtree.
Additional examples of median graphs are provided by the grid graphs.  In a grid graph, the coordinates of the median m(a,b,c) can be found as the median of the coordinates of a, b, and c.  Conversely, it turns out that, in any median graph, one may label the vertices by points in an integer lattice in such a way that medians can be calculated coordinatewise in this way.[5]
A squaregraph.
Squaregraphs, planar graphs in which all interior faces are quadrilaterals and all interior vertices have four or more incident edges, are another subclass of the median graphs.[6]  A polyomino is a special case of a squaregraph and therefore also forms a median graph.
The simplex graph κ(G) of any undirected graph G has a node for every clique (complete subgraph) of G; two nodes are linked by an edge if the corresponding cliques differ by one vertex.  The median of any three cliques may be formed by using the majority rule to determine which vertices of the cliques to include; the simplex graph is a median graph in which this rule determines the median of any three vertices.
No cycle graph of length other than four can be a median graph, because any such cycle has three vertices a, b, and c such that the three shortest paths wrap all the way around the cycle without having a common intersection.  For such a triple of vertices, there can be no median.
Perhaps the simplest persistent data structure is the singly linked list or cons-based list, a simple list of objects formed by each carrying a reference to the next in the list. This is persistent because we can take a tail of the list, meaning the last k items for some k, and add new nodes on to the front of it. The tail will not be duplicated, instead becoming shared between both the old list and the new list. So long as the contents of the tail are immutable, this sharing will be invisible to the program.
Many common reference-based data structures, such as red-black trees,[2] and stacks,[3] can easily be adapted to create a persistent version. Some others need slightly more effort, for example: Queue, Double-ended queues (dequeue), Min-Dequeue (which have additional operation min returning minimal element in constant time without incurring additional complexity on standard operations of queuing and dequeuing on both ends), Random access list (with constant cons/head as single linked list, but with additional operation of random access with sub-linear, most often logarithmic, complexity), Random access queue, Random access double-ended queue and Random access stack (as well Random access Min-List, Min-Queue, Min-Dequeue, Min-Stack).
There also exist persistent data structures which use destructible operations, making them impossible to implement efficiently in purely functional languages (like Haskell), but possible in languages like C or Java. These types of data structures can be avoided with proper design. One primary advantage to using purely persistent data structures is that they often behave better in multi-threaded environments.
The PQ tree representing [1 (2 3 4) 5]
If all the leaves of a PQ tree are connected directly to a root P node then all possible orderings are allowed. If all the leaves are connected directly to a root Q node then only one order (and its reverse) is allowed. If nodes a,b,c connect to a P node, which connects to a root P node, with all other leaf nodes connected directly to the root, then any ordering where a,b,c are contiguous is allowed.
Where graphical presentation is unavailable PQ trees are often noted using nested parenthesized lists. Square parentheses represents a Q node and regular ones represents P nodes. Leaves are non-parentheses elements of the lists. The image on the left is represented in this notation by [1 (2 3 4) 5]. This PQ tree represents the following twelve permutations on the set {1, 2, 3, 4, 5}:
12345, 12435, 13245, 13425, 14235, 14325, 52341, 52431, 53241, 53421, 54231, 54321.
Some examples of search-tree data structures are:
AVL trees, Red-black trees, splay trees and Tango Trees which are instances of self-balancing binary search trees; Ternary search trees, in which each internal node has exactly three children; B trees, commonly used in databases; B+ trees, like B trees but with all data values stored in the leaves; van Emde Boas trees, very efficient if the data values are fixed-size integers.
The complete signed graph on n vertices with loops, denoted by ±Kno, has every possible positive and negative edge including negative loops, but no positive loops. Its edges correspond to the roots of the root system Cn; the column of an edge in the incidence matrix (see below) is the vector representing the root.
The complete signed graph with half-edges, ±Kn', is ±Kn with a half-edge at every vertex. Its edges correspond to the roots of the root system Bn, half-edges corresponding to the unit basis vectors.
The complete signed link graph, ±Kn, is the same but without loops. Its edges correspond to the roots of the root system Dn.
An all-positive signed graph has only positive edges. If the underlying graph is G, the all-positive signing is written +G.
An all-negative signed graph has only negative edges. It is balanced if and only if it is bipartite because a circle is positive if and only if it has even length. An all-negative graph with underlying graph G is written �?i>G.
A signed complete graph has as underlying graph G the ordinary complete graph Kn. It may have any signs. Signed complete graphs are equivalent to two-graphs, which are of value in finite group theory. A two-graph can be defined as the class of vertex sets of negative triangles in a signed complete graph.
A graph with six nodes.
The diagram at right is a graphic representation of the following graph:
V = {1, 2, 3, 4, 5, 6} E = {{1, 2}, {1, 5}, {2, 3}, {2, 5}, {3, 4}, {4, 5}, {4, 6}}.
In category theory a small category can be represented by a directed multigraph in which the objects of the category represented as vertices and the morphisms as directed edges. Then, the functors between categories induce some, but not necessarily all, of the digraph morphisms of the graph. In computer science, directed graphs are used to represent knowledge (e.g., Conceptual graph), finite state machines, and many other discrete structures. A binary relation R on a set X defines a directed graph. An element x of X is a direct predecessor of an element y of X iff xRy.
Railroad cars example
Consider the input arrangement from figure, here we note that the car 3 is at the front, so it can't be output yet, as it to be preceded by cars 1 & 2. So car 3 is detached & moved to holding track H1. The next car 6 can't be output & it is moved to holding track H2. Because we have to output car 3 before car 6 & this will not possible if we move car 6 to holding track H1. Now it's obvious that we move car 9 to H3.
The requirement of rearrangement of cars on any holding track is that the cars should be preferred to arrange in ascending order from top to bottom.
So car 2 is now moved to holding track H1 so that it satisfies the previous statement. If we move car 2 to H2 or H3, then we've no place to move cars 4,5,7,8.The least restrictions on future car placement arise when the new car λ is moved to the holding track that has a car at its top with smallest label Ψ such that λ < Ψ. We may call it an assignment rule to decide whether a particular car belongs to a specific holding track. When car 4 is considered, there are three places to move the car H1,H2,H3. The top of these tracks are 2,6,9.So using above mentioned Assignment rule, we move car 4 to H2. The car 7 is moved to H3. The next car 1 has the least label, so it's moved to output track. Now it's time for car 2 & 3 to output which are from H1(in short all the cars from H1 are appended to car 1 on output track).
The car 4 is moved to output track. No other cars can be moved to output track at this time.
The next car 8 is moved to holding track H1. Car 5 is output from input track. Car 6 is moved to output track from H2, so is the 7 from H3,8 from H1 & 9 from H3.
[8]
The cycle of length 5 is an srg(5,2,0,1). The Petersen graph is an srg(10,3,0,1). The Clebsch graph is an srg(16,5,0,2). The Shrikhande graph is an srg(16,6,2,2) which is not a distance-transitive graph. The Line graph of Generalized quadrangle GQ(2,4) are srg(27,10,1,5). The Schläfli graph is an srg(27,16,10,8).[3] The Chang graphs are srg(28,12,6,4). The Hoffman–Singleton graph is an srg(50,7,0,1). The Sims-Gewirtz graph is an (56,10,0,2). The M22 graph is an srg(77,16,0,4). The Brouwer–Haemers graph is an srg(81,20,1,6). The Higman–Sims graph is an srg(100,22,0,6). The Local McLaughlin graph is an srg(162,56,10,24). The Paley graph of order q is an srg(q, (q �?#160;1)/2, (q �?#160;5)/4, (q �?#160;1)/4). The n × n square rook's graph is an srg(n2, 2n �?#160;2, n �?#160;2, 2).
A strongly regular graph is called primitive if both the graph and its complement are connected. All the above graphs are primitive, as otherwise μ=0 or μ=k.
Combining the symmetry condition with the restriction that graphs be cubic (i.e. all vertices have degree 3) yields quite a strong condition, and such graphs are rare enough to be listed. The Foster census and its extensions provide such lists.[7] The Foster census was begun in the 1930s by Ronald M. Foster while he was employed by Bell Labs,[8] and in 1988 (when Foster was 92[1]) the then current Foster census (listing all cubic symmetric graphs up to 512 vertices) was published in book form.[9] The first thirteen items in the list are cubic symmetric graphs with up to 30 vertices[10][11] (ten of these are also distance transitive; the exceptions are as indicated):
Vertices Diameter Girth Graph Notes 4 1 3 The complete graph K4 distance transitive, 2-transitive 6 2 4 The complete bipartite graph K3,3 distance transitive, 3-transitive 8 3 4 The vertices and edges of the cube distance transitive, 2-transitive 10 2 5 The Petersen graph distance transitive, 3-transitive 14 3 6 The Heawood graph distance transitive, 4-transitive 16 4 6 The Möbius–Kantor graph 2-transitive 18 4 6 The Pappus graph distance transitive, 3-transitive 20 5 5 The vertices and edges of the dodecahedron distance transitive, 2-transitive 20 5 6 The Desargues graph distance transitive, 3-transitive 24 4 6 The Nauru graph (the generalized Petersen graph G(12,5)) 2-transitive 26 5 6 The F26A graph[11] 1-transitive 28 4 7 The Coxeter graph distance transitive, 3-transitive 30 4 8 The Tutte–Coxeter graph distance transitive, 5-transitive
Other well known cubic symmetric graphs are the Dyck graph, the Foster graph and the Biggs–Smith graph. The ten distance-transitive graphs listed above, together with the Foster graph and the Biggs–Smith graph, are the only cubic distance-transitive graphs.
Non-cubic symmetric graphs include cycle graphs (of degree 2), complete graphs (of degree 4 or more when there are 5 or more vertices), hypercube graphs (of degree 4 or more when there are 16 or more vertices), and the graphs formed by the vertices and edges of the octahedron, icosahedron, cuboctahedron, and icosidodecahedron. The Rado graph forms an example of a symmetric graph with infinitely many vertices and infinite degree.
A graph with six nodes.
The diagram at right is a graphic representation of the following graph:
V = {1, 2, 3, 4, 5, 6} E = {{1, 2}, {1, 5}, {2, 3}, {2, 5}, {3, 4}, {4, 5}, {4, 6}}.
In category theory a small category can be represented by a directed multigraph in which the objects of the category represented as vertices and the morphisms as directed edges. Then, the functors between categories induce some, but not necessarily all, of the digraph morphisms of the graph. In computer science, directed graphs are used to represent knowledge (e.g., Conceptual graph), finite state machines, and many other discrete structures. A binary relation R on a set X defines a directed graph. An element x of X is a direct predecessor of an element y of X iff xRy.
