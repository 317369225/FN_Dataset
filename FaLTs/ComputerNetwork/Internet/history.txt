Professor Leonard Kleinrock with the first ARPANET Interface Message Processors at UCLA
Main articles: History of the Internet and History of the World Wide Web
Research into packet switching started in the early 1960s and packet switched networks such as Mark I at NPL in the UK,[6] ARPANET, CYCLADES,[7][8] Merit Network,[9] Tymnet, and Telenet, were developed in the late 1960s and early 1970s using a variety of protocols. The ARPANET in particular led to the development of protocols for internetworking, where multiple separate networks could be joined together into a network of networks thanks to the work of British scientist Donald Davies whose ground-breaking work on Packet Switching was essential to the system.[10]
The first two nodes of what would become the ARPANET were interconnected between Leonard Kleinrock's Network Measurement Center at the UCLA's School of Engineering and Applied Science and Douglas Engelbart's NLS system at SRI International (SRI) in Menlo Park, California, on 29 October 1969.[11] The third site on the ARPANET was the Culler-Fried Interactive Mathematics center at the University of California at Santa Barbara, and the fourth was the University of Utah Graphics Department. In an early sign of future growth, there were already fifteen sites connected to the young ARPANET by the end of 1971.[12][13] These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.
Early international collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks.[14] Notable exceptions were the Norwegian Seismic Array (NORSAR) in June 1973,[15] followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter T. Kirstein's research group in the UK, initially at the Institute of Computer Science, University of London and later at University College London.[citation needed]
T3 NSFNET Backbone, c. 1992
In December 1974, RFC 675 �?Specification of Internet Transmission Control Program, by Vinton Cerf, Yogen Dalal, and Carl Sunshine, used the term internet, as a shorthand for internetworking; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.[16] Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) developed the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardized and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced.
TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNET) provided access to supercomputer sites in the United States from research and education organizations, first at 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s.[17] Commercial internet service providers (ISPs) began to emerge in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. The Internet was commercialized in 1995 when NSFNET was decommissioned, removing the last restrictions on the use of the Internet to carry commercial traffic.[18] The Internet started a rapid expansion to Europe and Australia in the mid to late 1980s[19][20] and to Asia in the late 1980s and early 1990s.[21]
This NeXT Computer was used by Sir Tim Berners-Lee at CERN and became the world's first Web server.
Since the mid-1990s the Internet has had a tremendous impact on culture and commerce, including the rise of near instant communication by email, instant messaging, Voice over Internet Protocol (VoIP) "phone calls", two-way interactive video calls, and the World Wide Web[22] with its discussion forums, blogs, social networking, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more. The Internet continues to grow, driven by ever greater amounts of online information and knowledge, commerce, entertainment and social networking.[23]
During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%.[24] This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary open nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network.[25] As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population).[26] It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication, by 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet.[27]
Distributed processing
This section requires expansion. (December 2010)
Before the advent of computer networks that were based upon some type of telecommunications system, communication between calculation machines and early computers was performed by human users by carrying instructions between them. Many of the social behaviors seen in today's Internet were demonstrably present in the 19th century and arguably in even earlier networks using visual signals.
In September 1940, George Stibitz used a Teletype machine to send instructions for a problem set from his Model at Dartmouth College to his Complex Number Calculator in New York and received results back by the same means. Linking output systems like teletypewriters to computers was an interest at the Advanced Research Projects Agency (ARPA) when, in 1962, J.C.R. Licklider was hired and developed a working group he called the "Intergalactic Computer Network", a precursor to the ARPANET. Early networks of communicating computers included the military radar system Semi-Automatic Ground Environment (SAGE), started in the late 1950s. The commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes in 1960. In 1964, researchers at Dartmouth developed the Dartmouth Time Sharing System for distributed users of large computer systems. The same year, at Massachusetts Institute of Technology, a research group supported by General Electric and Bell Labs used a computer to route and manage telephone connections. Throughout the 1960s Leonard Kleinrock, Paul Baran and Donald Davies independently conceptualized and developed network systems which used packets that could be used in a network between computer systems. 1965 Thomas Marill and Lawrence G. Roberts created the first wide area network (WAN). This was an immediate precursor to the ARPANET, of which Roberts became program manager. The first widely used telephone switch that used true computer control was introduced by Western Electric in 1965. In 1969 the University of California at Los Angeles, the Stanford Research Institute, University of California at Santa Barbara, and the University of Utah were connected as the beginning of the ARPANET network using 50 kbit/s circuits.[2] Commercial services using X.25 were deployed in 1972, and later used as an underlying infrastructure for expanding TCP/IP networks.
Today, computer networks are the core of modern communication. All modern aspects of the public switched telephone network (PSTN) are computer-controlled, and telephony increasingly runs over the Internet Protocol, although not necessarily the public Internet. The scope of communication has increased significantly in the past decade, and this boom in communications would not have been possible without the progressively advancing computer network. Computer networks, and the technologies needed to connect and communicate through and between them, continue to drive computer hardware, software, and peripherals industries. This expansion is mirrored by growth in the numbers and types of users of networks, from the researcher to the home user.
Interconnected collection of autonomous computers(unique identity) is known as computer network.
Packet switching, today the dominant basis for data communications worldwide, was a new concept at the time of the conception of the ARPANET. Prior to the advent of packet switching, both voice and data communications had been based on the idea of circuit switching, as in the traditional telephone circuit, wherein each telephone call is allocated a dedicated, end to end, electronic connection between the two communicating stations. Such stations might be telephones or computers. The (temporarily) dedicated line is typically composed of many intermediary lines which are assembled into a chain that stretches all the way from the originating station to the destination station. With packet switching, a data system could use a single communications link to communicate with more than one machine by collecting data into datagrams and transmitting these as packets onto the attached network link, as soon as the link becomes idle. Thus, not only can the link be shared, much as a single post box can be used to post letters to different destinations, but each packet can be routed independently of other packets.[4]
The earliest ideas for a computer network intended to allow general communications among computer users were formulated by computer scientist J. C. R. Licklider, of Bolt, Beranek and Newman (BBN), in August 1962, in memoranda discussing his concept for an "Intergalactic Computer Network". Those ideas contained almost everything that composes the contemporary Internet. In October 1963, Licklider was appointed head of the Behavioral Sciences and Command and Control programs at the Defense Department's Advanced Research Projects Agency �?ARPA (the initial ARPANET acronym). He then convinced Ivan Sutherland and Bob Taylor that this computer network concept was very important and merited development, although Licklider left ARPA before any contracts were let that worked on this concept.[5]
Ivan Sutherland and Bob Taylor continued their interest in creating such a computer communications network, in part, to allow ARPA-sponsored researchers at various corporate and academic locales to put to use the computers ARPA was providing them, and, in part, to make new software and other computer science results quickly and widely available.[6] In his office, Taylor had three computer terminals, each connected to separate computers, which ARPA was funding: the first, for the System Development Corporation (SDC) Q-32, in Santa Monica; the second, for Project Genie, at the University of California, Berkeley; and the third, for Multics, at MIT. Taylor recalls the circumstance: "For each of these three terminals, I had three different sets of user commands. So, if I was talking online with someone at S.D.C., and I wanted to talk to someone I knew at Berkeley, or M.I.T., about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them. I said, "Oh Man!", it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go. That idea is the ARPANET".[7] Somewhat contemporaneously, several other people had (mostly independently) worked out the aspects of "packet switching", with the first public demonstration presented by the National Physical Laboratory (NPL), on 5 August 1968, in the United Kingdom.[8]
In 1975, the Defense Communication Agency (DCA) took over operation of the ARPANET as it became an operational tool instead of a research project. In 1983, plans for a new generation of the Automatic Digital Network (Autodin II) were canceled. Instead, a separate network to connect military installations called MILNET was split off the ARPANET. The ARPANET would be used as an Internet backbone for researchers, but be slowly phased out. Both networks carried unclassified information, and were connected at a small number of points which would allow total separation in the event of an emergency. The DCA used the Defense Data Network (DDN) as the program name for these network programs.[2]
As a large-scale, private internet, the DDN provided Internet Protocol connectivity across the United States and to US military bases abroad. Throughout the 1980s it expanded as a set of four parallel military networks, each at a different security level. These networks transitioned to become the NIPRNET, SIPRNET, and JWICS networks in the 1990s.
The four DDN subnetworks were:
Military Network (MILNET) for Unclassified traffic Defense Secure Network One (DSNET 1) for Secret traffic Defense Secure Network Two (DSNET 2) for Top Secret traffic Defense Secure Network Three (DSNET 3) for Top Secret/Sensitive Compartmented Information (TS/SCI)
MILNET and DSNET 1 were common user networks, much like the public Internet, but DSNET 2 was dedicated to supporting the Worldwide Military Command and Control System (WWMCCS) and DSNET 3 was dedicated to supporting the DOD Intelligence Information System (DODIIS).
Ethernet was developed at Xerox PARC between 1973 and 1974.[1][2] It was inspired by ALOHAnet, which Robert Metcalfe had studied as part of his PhD dissertation.[3] The idea was first documented in a memo that Metcalfe wrote on May 22, 1973.[1][4] In 1975, Xerox filed a patent application listing Metcalfe, David Boggs, Chuck Thacker and Butler Lampson as inventors.[5] In 1976, after the system was deployed at PARC, Metcalfe and Boggs published a seminal paper.[6][note 1]
Metcalfe left Xerox in June 1979 to form 3Com.[1][8] He convinced Digital Equipment Corporation (DEC), Intel, and Xerox to work together to promote Ethernet as a standard. The so-called "DIX" standard, for "Digital/Intel/Xerox" specified 10 Mbit/s Ethernet, with 48-bit destination and source addresses and a global 16-bit Ethertype-type field. It was published on September 30, 1980 as "The Ethernet, A Local Area Network. Data Link Layer and Physical Layer Specifications".[9] Version 2 was published in November, 1982[10] and defines what has become known as Ethernet II. Formal standardization efforts proceeded at the same time.
Ethernet initially competed with two largely proprietary systems, Token Ring and Token Bus. Because Ethernet was able to adapt to market realities and shift to inexpensive and ubiquitous twisted pair wiring, these proprietary protocols soon found themselves competing in a market inundated by Ethernet products and by the end of the 1980s, Ethernet was clearly the dominant network technology.[1] In the process, 3Com became a major company. 3Com shipped its first 10 Mbit/s Ethernet 3C100 transceiver in March 1981, and that year started selling adapters for PDP-11s and VAXes, as well as Multibus-based Intel and Sun Microsystems computers.[11]:9 This was followed quickly by DEC's Unibus to Ethernet adapter, which DEC sold and used internally to build its own corporate network, which reached over 10,000 nodes by 1986, making it one of the largest computer networks in the world at that time.[12] An Ethernet adapter card for the IBM PC was released in 1982 and by 1985, 3Com had sold 100,000.[8]
Since then Ethernet technology has evolved to meet new bandwidth and market requirements.[13] In addition to computers, Ethernet is now used to interconnect appliances and other personal devices.[1] It is used in industrial applications and is quickly replacing legacy data transmission systems in the world's telecommunications networks.[14] By 2010, the market for Ethernet equipment amounted to over $16 billion per year.[15]
The result of research done at Xerox PARC in the early 1970s, Ethernet evolved into a widely implemented physical and link layer protocol. Fast Ethernet increased speed from 10 to 100 megabits per second (Mbit/s). Gigabit Ethernet was the next iteration, increasing the speed to 1000 Mbit/s. The initial standard for gigabit Ethernet was produced by the IEEE in June 1998 as IEEE 802.3z, and required optical fiber. 802.3z is commonly referred to as 1000BASE-X, where -X refers to either -CX, -SX, -LX, or (non-standard) -ZX.
IEEE 802.3ab, ratified in 1999, defines gigabit Ethernet transmission over unshielded twisted pair (UTP) category 5, 5e, or 6 cabling and became known as 1000BASE-T. With the ratification of 802.3ab, gigabit Ethernet became a desktop technology as organizations could use their existing copper cabling infrastructure.
IEEE 802.3ah, ratified in 2004 added two more gigabit fiber standards, 1000BASE-LX10 (which was already widely implemented as vendor specific extension) and 1000BASE-BX10. This was part of a larger group of protocols known as Ethernet in the First Mile.
Initially, gigabit Ethernet was deployed in high-capacity backbone network links (for instance, on a high-capacity campus network). In 2000, Apple's Power Mac G4 and PowerBook G4 were the first mass produced personal computers featuring the 1000BASE-T connection.[2] It quickly became a built-in feature in many other computers.
Higher bandwidth 10 Gigabit Ethernet standards have since become available as the IEEE ratified a fiber-based standard in 2002, and a twisted pair standard in 2006. As of 2009[update] 10Gb Ethernet is replacing 1Gb as the backbone network and has begun to migrate down to high-end server systems.[citation needed]
The first high speed backbone was created by the National Science Foundation in 1987. It was called the NSFNET, and was a T1 line that connected 170 smaller networks together. The following year, IBM, MCI and Merit would create a T3 backbone.[1] In the early days of the Internet, backbone providers exchanged their traffic at government-sponsored network access points, until the government privatized the Internet, and then transferred the NAPs to commercial providers.[2]
Professor Leonard Kleinrock with the first ARPANET Interface Message Processors at UCLA
Main articles: History of the Internet and History of the World Wide Web
Research into packet switching started in the early 1960s and packet switched networks such as Mark I at NPL in the UK,[6] ARPANET, CYCLADES,[7][8] Merit Network,[9] Tymnet, and Telenet, were developed in the late 1960s and early 1970s using a variety of protocols. The ARPANET in particular led to the development of protocols for internetworking, where multiple separate networks could be joined together into a network of networks thanks to the work of British scientist Donald Davies whose ground-breaking work on Packet Switching was essential to the system.[10]
The first two nodes of what would become the ARPANET were interconnected between Leonard Kleinrock's Network Measurement Center at the UCLA's School of Engineering and Applied Science and Douglas Engelbart's NLS system at SRI International (SRI) in Menlo Park, California, on 29 October 1969.[11] The third site on the ARPANET was the Culler-Fried Interactive Mathematics center at the University of California at Santa Barbara, and the fourth was the University of Utah Graphics Department. In an early sign of future growth, there were already fifteen sites connected to the young ARPANET by the end of 1971.[12][13] These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.
Early international collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks.[14] Notable exceptions were the Norwegian Seismic Array (NORSAR) in June 1973,[15] followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter T. Kirstein's research group in the UK, initially at the Institute of Computer Science, University of London and later at University College London.[citation needed]
T3 NSFNET Backbone, c. 1992
In December 1974, RFC 675 �?Specification of Internet Transmission Control Program, by Vinton Cerf, Yogen Dalal, and Carl Sunshine, used the term internet, as a shorthand for internetworking; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.[16] Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) developed the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardized and the concept of a world-wide network of fully interconnected TCP/IP networks called the Internet was introduced.
TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNET) provided access to supercomputer sites in the United States from research and education organizations, first at 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s.[17] Commercial internet service providers (ISPs) began to emerge in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. The Internet was commercialized in 1995 when NSFNET was decommissioned, removing the last restrictions on the use of the Internet to carry commercial traffic.[18] The Internet started a rapid expansion to Europe and Australia in the mid to late 1980s[19][20] and to Asia in the late 1980s and early 1990s.[21]
This NeXT Computer was used by Sir Tim Berners-Lee at CERN and became the world's first Web server.
Since the mid-1990s the Internet has had a tremendous impact on culture and commerce, including the rise of near instant communication by email, instant messaging, Voice over Internet Protocol (VoIP) "phone calls", two-way interactive video calls, and the World Wide Web[22] with its discussion forums, blogs, social networking, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more. The Internet continues to grow, driven by ever greater amounts of online information and knowledge, commerce, entertainment and social networking.[23]
During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%.[24] This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary open nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network.[25] As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population).[26] It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication, by 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet.[27]
A conceptual diagram of a local area network using 10BASE5 Ethernet
The increasing demand and use of computers in universities and research labs in the late 1960s generated the need to provide high-speed interconnections between computer systems. A 1970 report from the Lawrence Radiation Laboratory detailing the growth of their "Octopus" network[2][3] gave a good indication of the situation.
Cambridge Ring was developed at Cambridge University in 1974[4] but was never developed into a successful commercial product.
Ethernet was developed at Xerox PARC in 1973�?975,[5] and filed as U.S. Patent 4,063,220. In 1976, after the system was deployed at PARC, Metcalfe and Boggs published a seminal paper, "Ethernet: Distributed Packet-Switching For Local Computer Networks."[6]
ARCNET was developed by Datapoint Corporation in 1976 and announced in 1977.[7] It had the first commercial installation in December 1977 at Chase Manhattan Bank in New York.[8]
Main articles: History of mobile phones and History of the prepaid mobile phone
According to internal memos, American Telephone & Telegraph discussed developing a wireless phone in 1915, but were afraid that deployment of the technology could undermine its monopoly on wired service in the U.S.[1]
Public mobile phone systems were first introduced in the years after the Second World War and made use of technology developed before and during the conflict. The first system opened in St Louis, Missouri, USA in 1946 whilst other countries followed in the succeeding decades. The UK introduced its 'System 1' manual radiotelephone service as the South Lancashire Radiophone Service in 1958.[2] Calls were made via an operator using handsets identical to ordinary phone handsets.[3] The phone itself was a large box located in the boot (trunk) of the vehicle containing valves and other early electronic components. Although an uprated manual service ('System 3') was extended to cover most of the UK, automation did not arrive until 1981 with 'System 4'. Although this non-cellular service, based on German B-Netz technology, was expanded rapidly throughout the UK between 1982 and 1985 and continued in operation for several years before finally closing in Scotland, it was overtaken by the introduction in January 1985 of two cellular systems - the British Telecom/Securicor 'Cellnet' service and the Racal/Millicom/Barclays 'Vodafone' (from voice + data + phone) service. These cellular systems were based on US Advanced Mobile Phone Service (AMPS) technology, the modified technology being named Total Access Communication System (TACS).
In 1947 Bell Labs was the first to propose a cellular radio telephone network. The primary innovation was the development of a network of small overlapping cell sites supported by a call switching infrastructure that tracks users as they move through a network and passes their calls from one site to another without dropping the connection. In 1956 the MTA system was launched in Sweden. The first call on a handheld mobile phone was made on April 3, 1973 by Martin Cooper, then of Motorola [4] to his opposite number in Bell Labs who were also racing to be first. Bell Labs went on to install the first trial cellular network in Chicago in 1978. This trial system was licensed by the FCC to ATT for commercial use in 1982 and, as part of the divestiture arrangements for the breakup of ATT, the AMPS technology was distributed to local telcos. The first commercial system opened in Chicago in October 1983.[5][6] A system designed by Motorola also operated in the Washington D.C./Baltimore area from summer 1982 and became a full public service later the following year.[7] Japan's first commercial radiotelephony service was launched by NTT in 1978.
The first fully automatic first generation cellular system was the Nordic Mobile Telephone (NMT) system, simultaneously launched in 1981 in Denmark, Finland, Norway and Sweden.[8] NMT was the first mobile phone network featuring international roaming. The Swedish electrical engineer Östen Mäkitalo started to work on this vision in 1966, and is considered as the father of the NMT system and some consider him also the father of the cellular phone.[9][10]
The advent of cellular technology encouraged European countries to co-operate in the development of a pan-European cellular technology to rival those of the US and Japan. This resulted in the GSM system, the initials originally from the Groupe Spécial Mobile that was charged with the specification and development tasks but latterly as the 'Global System for Mobile Communications'. The GSM standard eventually spread outside Europe and is now the most widely used cellular technology in the world and the de facto standard. The industry association, the GSMA, now represents 219 countries and nearly 800 mobile network operators.[11] There are now estimated to be over 5 billion phone subscriptions according to the "List of countries by number of mobile phones in use" (although some users have multiple subscriptions, or inactive subscriptions), which also makes the mobile phone the most widely spread technology and the most common electronic device in the world.[12]
The first mobile phone to enable internet connectivity and wireless email, the Nokia Communicator, was released in 1996, creating a new category of multi-use devices called smartphones. In 1999 the first mobile internet service was launched by NTT DoCoMo in Japan under the i-Mode service. By 2007 over 798 million people around the world accessed the internet or equivalent mobile internet services such as WAP and i-Mode at least occasionally using a mobile phone rather than a personal computer.
After successful experiments with Voice over Ethernet from 1981 to 1984, Dr. W. David Sincoskie joined Bellcore and began addressing the problem of scaling up Ethernet networks. At 10 Mbit/s, Ethernet was faster than most alternatives of the time; however, Ethernet was a broadcast network and there was no good way of connecting multiple Ethernet networks together. This limited the total bandwidth of an Ethernet network to 10 Mbit/s and the maximum distance between any two nodes to a few hundred feet.
By contrast, although the existing telephone network's peak speed for individual connections was limited to 56 kbit/s (less than one hundredth of Ethernet's speed), the total bandwidth of that network was estimated at 1 Tbit/s, capable of moving over a hundred thousand times more information in a given timescale.
Although it was possible to use IP routing to connect multiple Ethernet networks together, the VAX-11/780 computers commonly used as routers cost $400,000 each at that time, and their total throughput was significantly less than Ethernet speeds. Sincoskie started looking for alternatives that required less processing per packet. In the process he independently reinvented the self-learning ethernet switch.[2]
However, using switches to connect multiple Ethernet networks in a fault-tolerant fashion requires redundant paths through that network, which in turn requires a spanning tree configuration. This ensures that there is only one active path from any source node to any destination on the network. This causes centrally located switches to become bottlenecks, which limits scalability as more networks are interconnected.
To help alleviate this problem, Sincoskie invented VLANs by adding a tag to each Ethernet packet. These tags could be thought of as colors, say red, green, or blue. Then each switch could be assigned to handle packets of a single color, and ignore the rest. The networks could be interconnected with three different spanning trees: a red spanning tree, a green spanning tree, and a blue spanning tree. By sending a mix of different packet colors, the aggregate bandwidth could be improved. Sincoskie referred to this as a multitree bridge. He and Chase Cotton created and refined the algorithms (called the Extended Bridge Algorithms for Large Networks) necessary to make the system feasible.[3]
This "color" is what is now known in the Ethernet frame as the 802.1Q header, or the VLAN tag. While VLANs are commonly used in modern Ethernet networks, using them for the original purpose would be rather unusual.
Main article: History of IEEE 802.11
802.11 technology has its origins in a 1985 ruling by the US Federal Communications Commission that released the ISM band for unlicensed use.[3] In 1991, NCR Corporation with AT&T Corporation invented the precursor to 802.11 intended for use in cashier systems. The first wireless products were under the name WaveLAN.
Vic Hayes has been called the "father of Wi-Fi" by some, due to his involvement in negotiating the initial standards within the IEEE while chairing the workgroup.[4][5]
A large number of patents by many companies are used in 802.11 standard.[6] In 1992 and 1996, Australian organisation CSIRO obtained patents for a method later used in Wi-Fi to "unsmear" the signal.[7] In April 2009, 14 tech companies agreed to pay CSIRO $250 million for infringements on CSIRO patents.[8] This led to Wi-Fi being attributed as an Australian invention,[9] though this has been the subject of some controversy.[10][11] CSIRO won a further $220 million settlement for Wi-Fi patent infringements in 2012 with global firms in the United States required to pay the CSIRO licensing rights estimated to be worth an additional $1 billion in royalties.[8][12][13]
In 1999, the Wi-Fi Alliance was formed as a trade association to hold the Wi-Fi trademark under which most products are sold.[14]
The key technologies behind Wi-Fi were developed by the radioastronomer John O'Sullivan as a by-product in a research project, "a failed experiment to detect exploding mini black holes the size of an atomic particle".[15]
These projects are in many senses an evolution of amateur radio, and more specifically packet radio, as well as an outgrowth of the free software community (which in itself substantially overlaps with amateur radio)[citation needed]. The key to using standard wireless networking devices designed for short-range use for multi-kilometre Long Range Wi-Fi linkups is the use of high-gain directional antennas. Rather than purchasing commercially available units, such groups sometimes advocate homebuilt antenna construction. Examples include the cantenna, which is typically constructed from a Pringles potato chip can, and RONJA, an optical link that can be made from a smoke flue and LEDs, with circuitry and instructions released under the GFDL. As with other wireless mesh networks, three distinct generations of mesh networks are used in wireless community networks.[3][4] In particular, in the 2004 timeframe, some mesh projects suffered poor performance when scaled up.[5][6]
Norman Abramson, a professor at the University of Hawaii, developed the world’s first wireless computer communication network, ALOHAnet, using low-cost ham-like radios. The system included seven computers deployed over four islands to communicate with the central computer on the Oahu Island without using phone lines.[2]
"In 1979, F.R. Gfeller and U. Bapst published a paper in the IEEE Proceedings reporting an experimental wireless local area network using diffused infrared communications. Shortly thereafter, in 1980, P. Ferrert reported on an experimental application of a single code spread spectrum radio for wireless terminal communications in the IEEE National Telecommunications Conference. In 1984, a comparison between infrared and CDMA spread spectrum communications for wireless office information networks was published by Kaveh Pahlavan in IEEE Computer Networking Symposium which appeared later in the IEEE Communication Society Magazine. In May 1985, the efforts of Marcus led the FCC to announce experimental ISM bands for commercial application of spread spectrum technology. Later on, M. Kavehrad reported on an experimental wireless PBX system using code division multiple access. These efforts prompted significant industrial activities in the development of a new generation of wireless local area networks and it updated several old discussions in the portable and mobile radio industry.
The first generation of wireless data modems was developed in the early 1980's by amateur communication groups. They added a voice band data communication modem, with data rates below 9600 bps, to an existing short distance radio system such as a walkie talkie. The second generation of wireless modems was developed immediately after the FCC announcement in the experimental bands for non-military use of the spread spectrum technology. These modems provided data rates on the order of hundreds of Kbps. The third generation of wireless modem now aims at compatibility with the existing LANs with data rates on the order of Mbps. Currently, several companies are developing the third generation products with data rates above 1 Mbps and a couple of products have already been announced. "[3]
54 Mbit/s WLAN PCI Card (802.11g)
"The first of the IEEE Workshops on Wireless LAN was held in 1991. At that time early wireless LAN products had just appeared in the market and the IEEE 802.11 committee had just started its activities to develop a standard for wireless LANs. The focus of that first workshop was evaluation of the alternative technologies. By 1996, the technology was relatively mature, a variety of applications had been identified and addressed and technologies that enable these applications were well understood. Chip sets aimed at wireless LAN implementations and applications, a key enabling technology for rapid market growth, were emerging in the market. Wireless LANs were being used in hospitals, stock exchanges, and in building and campus settings for nomadic access, point-to-point LAN bridges, ad-hoc networking, and even larger applications through internetworking. The IEEE 802.11 standard and variants and alternatives, such as the wireless LAN interoperability forum and the European HiperLAN specification had made rapid progress, and the unlicensed PCS Unlicensed Personal Communications Services and the proposed SUPERNet, later on renamed as U-NII, bands also presented new opportunities."[4]
WLAN hardware was initially so expensive that it was only used as an alternative to cabled LAN in places where cabling was difficult or impossible. Early development included industry-specific solutions and proprietary protocols, but at the end of the 1990s these were replaced by standards, primarily the various versions of IEEE 802.11 (in products using the Wi-Fi brand name). An alternative ATM-like 5 GHz standardized technology, HiperLAN/2, has so far not succeeded in the market, and with the release of the faster 54 Mbit/s 802.11a (5 GHz) and 802.11g (2.4 GHz) standards, it is even more unlikely that it will ever succeed. Since 2002 there has been newer standard added to 802.11; 802.11n which operates on both the 5Ghz and 2.4Ghz bands at 300 Mbit/s, most newer routers can broadcast a wireless network on both wireless bands, this is called dualband. Because of the crowded 2.4Ghz band and the interference with other services like Bluetooth, more and more routers are available as dualband version. The extension of the 5Ghz to 5.8Ghz leaves space for a lot of more stations. A HomeRF group was formed in 1997 to promote a technology aimed for residential use, but disbanded at the end of 2002.[5]
