Some learning algorithms perform feature selection as part of their overall operation. These include:

{\displaystyle l_{1}} l_{1}-regularization techniques, such as sparse regression, LASSO, and {\displaystyle l_{1}} l_{1}-SVM
Regularized trees,[27] e.g. regularized random forest implemented in the RRF package[28]
Decision tree[52]
Memetic algorithm
Random multinomial logit (RMNL)
Auto-encoding networks with a bottleneck-layer
Submodular feature selection[53][54][55]
Local learning based feature selection.[56] Compared with traditional methods, it does not involve any heuristic search, can easily handle multi-class problems, and works for both linear and nonlinear problems. It is also supported by a strong theoretical foundation. Numeric experiments showed that the method can achieve a close-to-optimal solution even when data contains >1M irrelevant features.