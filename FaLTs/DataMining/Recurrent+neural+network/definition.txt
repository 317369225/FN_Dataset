The perceptron is a binary classifier which maps its input (a real-valued vector) to an output value (a single binary value):

where is a vector of real-valued weights, is the dot product (which here computes a weighted sum), and is the 'bias', a constant term that does not depend on any input value.
The value of (0 or 1) is used to classify as either a positive or a negative instance, in the case of a binary classification problem. If is negative, then the weighted combination of inputs must produce a positive value greater than in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the decision boundary. The perceptron learning algorithm does not terminate if the learning set is not linearly separable. If the vectors are not linearly separable learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron's inability to solve problems with linearly nonseparable vectors is the boolean exclusive-or problem.
In the context of artificial neural networks, a perceptron is similar to a linear neuron. However, where a perceptron tries to learn weights that are always getting closer to a better set of weights, a linear neuron learns a set of weights where the outputs are always getting closer to the target outputs. Put another way, a perceptron is more interested in learning the hyperplane that correctly separates two classes of training input, whereas a linear neuron is more interested in learning a set of weights which reduce a real valued prediction error. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from the case of a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the (single-layer) perceptron is the simplest kind of feedforward neural network.
