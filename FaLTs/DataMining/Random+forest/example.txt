The following tree was constructed using JBoost on the spambase dataset[3] (available from the UCI Machine Learning Repository).[4] In this example, spam is coded as and regular email is coded as .

The following table contains part of the information for a single instance.
An instance to be classified Feature Value char_freq_bang 0.08 word_freq_hp 0.4 capital_run_length_longest 4 char_freq_dollar 0 word_freq_remove 0.9 word_freq_george 0 Other features ...
The instance is scored by summing all of the prediction nodes through which it passes. In the case of the instance above, the score is calculate as
Score for the above instance Iteration 0 1 2 3 4 5 6 Instance values N/A .08 < .052 = f .4 < .195 = f 0 < .01 = t 0 < 0.005 = t N/A .9 < .225 = f Prediction -0.093 0.74 -1.446 -0.38 0.176 0 1.66
The final score of is positive, so the instance is classified as spam. The magnitude of the value is a measure of confidence in the prediction. The original authors list three potential levels of interpretation for the set of attributes identified by an ADTree:
Individual nodes can be evaluated for their own predictive ability. Sets of nodes on the same path may be interpreted as having a joint effect The tree can be interpreted as a whole.
Care must be taken when interpreting individual nodes as the scores reflect a re weighting of the data in each iteration.
As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested, 1000 of them are sick and 1000 of them are healthy. About 990 true positives 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result.
However, if of the 2000 people only 100 are really sick: the likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease - that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that he or she really has the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result.
Logistic regression is used extensively in numerous disciplines, including the medical and social science fields. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al using logistic regression.[4] It is also employed in marketing applications such as prediction of a customer's propensity to purchase a product or cease a subscription, etc.[citation needed] For example, logistic regression might be used to predict whether a patient has a given disease (e.g. diabetes), based on observed characteristics of the patient (age, gender, body mass index, results of various blood tests, etc.). Another example might be to predict whether a voter will vote Democratic or Republican, based on age, income, gender, race, state of residence, votes in previous elections, etc.[citation needed] The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product.[5][6] In each of these instances, a logistic regression model would compute the relevant odds for each predictor or interaction term, take the natural logarithm of the odds (compute the logit), conduct a linear regression analysis on the predicted values of the logit, and then take the exponential function of the logit to compute the odds ratio. Conditional random fields, an extension of logistic regression to sequential data, are employed in natural language processing.
The following example problem contrasts some of the unique properties of transduction against induction.

A collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.
The inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to build a model that captures the structure of this data. For example, if a nearest-neighbor algorithm is used, then the points near the middle will be labeled "A" or "C", even though it is apparent that they belong to the same cluster as the point labeled "B".
Transduction has the advantage of being able to consider all of the points, not just the labeled points, while performing the labeling task. In this case, transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong. The points in the middle, therefore, would most likely be labeled "B", because they are packed very close to that cluster.
An advantage of transduction is that it may be able to make better predictions with fewer labeled points, because it uses the natural breaks found in the unlabeled points. One disadvantage of transduction is that it builds no predictive model. If a previously unknown point is added to the set, the entire transductive algorithm would need to be repeated with all of the points in order to predict a label. This can be computationally expensive if the data is made available incrementally in a stream. Further, this might cause the predictions of some of the old points to change (which may be good or bad, depending on the application). A supervised learning algorithm, on the other hand, can label new points instantly, with very little computational cost.
