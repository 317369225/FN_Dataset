In machine learning, correlation clustering or cluster editing operates in a scenario where the relationship between the objects are known instead of the actual representation of the objects. For example, given a signed graph where the edge label indicates whether two nodes are similar (+) or different (�?, the task is to cluster the vertices so that similar objects are grouped together. Unlike other clustering algorithms this does not require choosing the number of clusters in advance because the objective, to minimize the disagreements, is independent of the number of clusters.
It may not be possible to find a perfect clustering, where all similar items are in a cluster while all dissimilar ones are in different clusters. If the graph indeed admits a perfect clustering, then simply deleting all the negative edges and finding the connected components in the remaining graph will return the required clusters.
But, in general a graph may not have a perfect clustering. For example, given nodes a,b,c such that a,b and a,c are similar while b,c are dissimilar, a perfect clustering is not possible. In such cases, the task is to find a clustering that maximizes the number of agreements (number of + edges inside clusters plus the number of - edges between clusters) or minimizes the number of disagreements (the number of - edges inside clusters plus the number of + edges between clusters). This problem of maximizing the agreements is NP-complete (multiway cut problem reduces to maximizing weighted agreements and the problem of partitioning into triangles[2] can be reduced to the unweighted version)
Given a set of observations (x1, x2, �? xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k sets (k �?n) S = {S1, S2, �? Sk} so as to minimize the within-cluster sum of squares (WCSS):

where μi is the mean of points in Si.
