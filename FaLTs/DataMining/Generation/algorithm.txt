Choose the initial population of individuals Evaluate the fitness of each individual in that population Repeat on this generation until termination (time limit, sufficient fitness achieved, etc.): Select the best-fit individuals for reproduction Breed new individuals through crossover and mutation operations to give birth to offspring Evaluate the individual fitness of new individuals Replace least-fit population with new individuals
Introduction to Genetic Algorithms with interactive Java applets. For experimenting with GAs online. Cross discipline example applications for GAs with references. An interactive applet featuring evolving vehicles.
v t e Metaheuristics for combinatorial problems Genetic algorithm Simulated annealing Ant colony optimization Local search Genetic programming
v t e Metaheuristics for real-valued problems Genetic algorithm Evolutionary programming Particle swarm optimization Differential evolution CMA-ES Local search
v t e Major subfields of optimization Convex programming Integer programming Quadratic programming Nonlinear programming Stochastic programming Robust optimization Combinatorial optimization Infinite-dimensional optimization Metaheuristics Constraint satisfaction Multiobjective optimization
Computer simulations of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.[12][13] His 1954 publication was not widely noticed. Starting in 1957,[14] the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)[15] and Crosby (1973).[16] Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.[17] Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).[18]
Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,[19] artificial evolution became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s �?Rechenberg's group was able to solve complex engineering problems through evolution strategies.[20][21][22][23] Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
As academic interest grew, the dramatic increase in desktop computational power allowed for practical application of the new technique. In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. In 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote[24] about Evolver in 1990.
The cross-entropy (CE) method generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.
Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and meta-heuristics.
Similar techniques differ in the implementation details and the nature of the particular applied problem.
Genetic algorithm - This is the most popular type of EA. One seeks the solution of a problem in the form of strings of numbers (traditionally binary, although the best representations are usually those that reflect something about the problem being solved), by applying operators such as recombination and mutation (sometimes one, sometimes both). This type of EA is often used in optimization problems. Genetic programming - Here the solutions are in the form of computer programs, and their fitness is determined by their ability to solve a computational problem. Evolutionary programming - Similar to genetic programming, but the structure of the program is fixed and its numerical parameters are allowed to evolve. Gene expression programming - Like genetic programming, GEP also evolves computer programs but it explores a genotype-phenotype system, where computer programs of different sizes are encoded in linear chromosomes of fixed length. Evolution strategy - Works with vectors of real numbers as representations of solutions, and typically uses self-adaptive mutation rates. Memetic algorithm - It is the hybrid form of population based methods. Inspired by the both Darwinian principles of natural evolution and Dawkins�?notion of a meme and viewed as a form of population-based algorithm coupled with individual learning procedures capable of performing local refinements. The focus of the research study is thus to balance been exploration and exploitation in the search. Differential evolution - Based on vector differences and is therefore primarily suited for numerical optimization problems. Neuroevolution - Similar to genetic programming but the genomes represent artificial neural networks by describing structure and connection weights. The genome encoding can be direct or indirect. Learning classifier system
Generate the initial population of individuals randomly - first Generation Evaluate the fitness of each individual in that population Repeat on this generation until termination (time limit, sufficient fitness achieved, etc.): Select the best-fit individuals for reproduction - parents Breed new individuals through crossover and mutation operations to give birth to offspring Evaluate the individual fitness of new individuals Replace least-fit population with new individuals
More advanced algorithms that are even faster while still being able to compute the exact minimax value are known, such as SCOUT,[11] Negascout and MTD-f.
Since the minimax algorithm and its variants are inherently depth-first, a strategy such as iterative deepening is usually used in conjunction with alpha–beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using iterative deepening is that searches at shallower depths give move-ordering hints that can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.
Algorithms like SSS*, on the other hand, use the best-first strategy. This can potentially make them more time-efficient, but typically at a heavy cost in space-efficiency.[citation needed]
There is in practice a large number of algorithms claiming to be "ant colonies", without always sharing the general framework of optimization by canonical ant colonies (COA). In practice, the use of an exchange of information between ants via the environment (a principle called "Stigmergy") is deemed enough for an algorithm to belong to the class of ant colony algorithms. This principle has led some authors to create the term "value" to organize methods and behavior based on search of food, sorting larvae, division of labour and cooperative transportation.[67]
The Bees Algorithm is an optimisation algorithm inspired by the natural foraging behaviour of honey bees to find the optimal solution[4]. The algorithm requires a number of parameters to be set, namely: number of scout bees (n), number of sites selected out of n visited sites (m), number of best sites out of m selected sites (e), number of bees recruited for best e sites (nep), number of bees recruited for the other (m-e) selected sites (nsp), initial size of patches (ngh) which includes site and its neighbourhood and stopping criterion.
The pseudo code for the bees algorithm in its simplest form[4]:
  
  1. Initialise population with random solutions.
  2. Evaluate fitness of the population.
  3. While (stopping criterion not met) //Forming new population.
  4.   Select sites for neighbourhood search.
  5.   Recruit bees for selected sites (more bees for best e sites) and evaluate fitnesses.
  6.   Select the fittest bee from each patch.
  7.   Assign remaining bees to search randomly and evaluate their fitnesses.
  8. End While.
In first step, the bees algorithm starts with the scout bees (n) being placed randomly in the search space. In step 2, the fitnesses of the sites visited by the scout bees are evaluated. In step 4, bees that have the highest fitnesses are chosen as “selected bees�?and sites visited by them are chosen for neighbourhood search. Then, in steps 5 and 6, the algorithm conducts searches in the neighbourhood of the selected sites, assigning more bees to search near to the best e sites. The bees can be chosen directly according to the fitnesses associated with the sites they are visiting. Alternatively, the fitness values are used to determine the probability of the bees being selected. Searches in the neighbourhood of the best e sites which represent more promising solutions are made more detailed by recruiting more bees to follow them than the other selected bees. Together with scouting, this differential recruitment is a key operation of the Bees Algorithm. However, in step 6, for each patch only the bee with the highest fitness will be selected to form the next bee population. In nature, there is no such a restriction. This restriction is introduced here to reduce the number of points to be explored. In step 7, the remaining bees in the population are assigned randomly around the search space scouting for new potential solutions. These steps are repeated until a stopping criterion is met. At the end of each iteration, the colony will have two parts to its new population �?those that were the fittest representatives from a patch and those that have been sent out randomly.[4]
An example map of Germany with some connections between cities
The breadth-first tree obtained when running BFS on the given map and starting in Frankfurt
Graph and tree search algorithms α–�?/a> A* B* Beam Bellman–Ford Best-first Bidirectional Borůvka Branch & bound BFS British Museum D* DFS Depth-limited Dijkstra Edmonds Floyd–Warshall Hill climbing Iterative deepening Kruskal Johnson Lexicographic BFS Prim Uniform-cost Listings Graph algorithms Search algorithms List of graph algorithms Related topics Dynamic programming Graph traversal Tree traversal Search games v t e
The algorithm uses a queue data structure to store intermediate results as it traverses the graph, as follows:
Enqueue the root node Dequeue a node and examine it If the element sought is found in this node, quit the search and return a result. Otherwise enqueue any successors (the direct child nodes) that have not yet been discovered. If the queue is empty, every node on the graph has been examined �?quit the search and return "not found". If the queue is not empty, repeat from Step 2.
Note: Using a stack instead of a queue would turn this algorithm into a depth-first search.
Choose the initial population of individuals Evaluate the fitness of each individual in that population Repeat on this generation until termination (time limit, sufficient fitness achieved, etc.): Select the best-fit individuals for reproduction Breed new individuals through crossover and mutation operations to give birth to offspring Evaluate the individual fitness of new individuals Replace least-fit population with new individuals
Introduction to Genetic Algorithms with interactive Java applets. For experimenting with GAs online. Cross discipline example applications for GAs with references. An interactive applet featuring evolving vehicles.
v t e Metaheuristics for combinatorial problems Genetic algorithm Simulated annealing Ant colony optimization Local search Genetic programming
v t e Metaheuristics for real-valued problems Genetic algorithm Evolutionary programming Particle swarm optimization Differential evolution CMA-ES Local search
v t e Major subfields of optimization Convex programming Integer programming Quadratic programming Nonlinear programming Stochastic programming Robust optimization Combinatorial optimization Infinite-dimensional optimization Metaheuristics Constraint satisfaction Multiobjective optimization
Computer simulations of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.[12][13] His 1954 publication was not widely noticed. Starting in 1957,[14] the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)[15] and Crosby (1973).[16] Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.[17] Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).[18]
Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,[19] artificial evolution became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s �?Rechenberg's group was able to solve complex engineering problems through evolution strategies.[20][21][22][23] Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
As academic interest grew, the dramatic increase in desktop computational power allowed for practical application of the new technique. In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. In 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote[24] about Evolver in 1990.
The cross-entropy (CE) method generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.
Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and meta-heuristics.
