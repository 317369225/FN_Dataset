In this article, the field of scalars denoted is either the field of real numbers or the field of complex numbers .
Formally, an inner product space is a vector space V over the field together with an inner product, i.e., with a map

that satisfies the following three axioms for all vectors and all scalars :[1][2]
Conjugate symmetry:

Note that in , it is symmetric.
Linearity in the first argument:

Positive-definiteness:
with equality only for
Notice that conjugate symmetry implies that is real for all , since we have
Moreover, sesquilinearity (see below) implies that
Conjugate symmetry and linearity in the first variable gives


so an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate symmetric sesquilinear form is called a Hermitian form. While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a positive-definite Hermitian form.
In the case of , conjugate-symmetry reduces to symmetry, and sesquilinear reduces to bilinear. So, an inner product on a real vector space is a positive-definite symmetric bilinear form.
From the linearity property it is derived that implies while from the positive-definiteness axiom we obtain the converse, implies Combining these two, we have the property that if and only if
Combining the linearity of the inner product in its first argument and the conjugate symmetry gives the following important generalization of the familiar square expansion:

Assuming that the underlying field is , the inner product becomes symmetric, and we obtain

or similarly,

The property of an inner product space that
and
is also known as additivity.
Remark: Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines we would write the product as (the bra-ket notation of quantum mechanics), respectively (dot product as a case of the convention of forming the matrix product AB as the dot products of rows of A with columns of B). Here the kets and columns are identified with the vectors of V and the bras and rows with the dual vectors or linear functionals of the dual space V*, with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature, e.g., Emch [1972], taking to be conjugate linear in x rather than y. A few instead find a middle ground by recognizing both and as distinct notations differing only in which argument is conjugate linear.
There are various technical reasons why it is necessary to restrict the basefield to and in the definition. Briefly, the basefield has to contain an ordered subfield[citation needed] (in order for non-negativity to make sense) and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally any quadratically closed subfield of or will suffice for this purpose, e.g., the algebraic numbers, but when it is a proper subfield (i.e., neither nor ) even finite-dimensional inner product spaces will fail to be metrically complete. In contrast all finite-dimensional inner product spaces over or , such as those used in quantum computation, are automatically metrically complete and hence Hilbert spaces.
In some cases we need to consider non-negative semi-definite sesquilinear forms. This means that is only required to be non-negative. We show how to treat these below.
Let K be a field (such as the field of real numbers), and let V be a vector space over K. As usual, we call elements of V vectors and call elements of K scalars. Suppose that W is a subset of V. If W is a vector space itself, with the same vector space operations as V has, then it is a subspace of V.
To use this definition, we don't have to prove that all the properties of a vector space hold for W. Instead, we can prove a theorem that gives us an easier way to show that a subset of a vector space is a subspace.
Theorem: Let V be a vector space over the field K, and let W be a subset of V. Then W is a subspace if and only if it satisfies the following three conditions:
The zero vector, 0, is in W. If u and v are elements of W, then the sum u + v is an element of W; If u is an element of W and c is a scalar from K, then the scalar product cu is an element of W;
Proof: Firstly, property 1 ensures W is nonempty. Looking at the definition of a vector space, we see that properties 2 and 3 above assure closure of W under addition and scalar multiplication, so the vector space operations are well defined. Since elements of W are necessarily elements of V, axioms 1, 2 and 5-8 of a vector space are satisfied. By the closure of W under scalar multiplication
Conversely, if W is subspace of V, then W is itself a vector space under the operations induced by V, so properties 2 and 3 are satisfied. By property 3, -w is in W whenever w is, and it follows that W is closed under subtraction as well. Since W is nonempty, there is an element x in W, and is in W, so property 1 is satisfied. One can also argue that since W is nonempty, there is an element x in W, and 0 is in the field K so and therefore property 1 is satisfied.
Example I: Let the field K be the set R of real numbers, and let the vector space V be the Euclidean space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.
Proof:
Given u and v in W, then they can be expressed as u = (u1,u2,0) and v = (v1,v2,0). Then u + v = (u1+v1,u2+v2,0+0) = (u1+v1,u2+v2,0). Thus, u + v is an element of W, too. Given u in W and a scalar c in R, if u = (u1,u2,0) again, then cu = (cu1, cu2, c0) = (cu1,cu2,0). Thus, cu is an element of W too.
Example II: Let the field be R again, but now let the vector space be the Euclidean geometry R2. Take W to be the set of points (x,y) of R2 such that x = y. Then W is a subspace of R2.
Proof:
Let p = (p1,p2) and q = (q1,q2) be elements of W, that is, points in the plane such that p1 = p2 and q1 = q2. Then p + q = (p1+q1,p2+q2); since p1 = p2 and q1 = q2, then p1 + q1 = p2 + q2, so p + q is an element of W. Let p = (p1,p2) be an element of W, that is, a point in the plane such that p1 = p2, and let c be a scalar in R. Then cp = (cp1,cp2); since p1 = p2, then cp1 = cp2, so cp is an element of W.
In general, any subset of a Euclidean space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z = 0, and the equation in example II was x = y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.
